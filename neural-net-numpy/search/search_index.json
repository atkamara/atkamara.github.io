{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This site contains the project documentation for the neural network with numpy project that provides functions to build neural network using only numpy components github link . Table Of Contents The documentation consists of three separate parts: Tutorials How-To Guides Reference Quickly find what you're looking for depending on your use case by looking at the different pages. This packages provides modules for computing neural networks Modules exported by this package: activation : provides classes for several types of activation functions layers : This modules provides Layer classes architecture : This modules provides neural network architectures init_funcs : This modules provides initialization functions cost : This modules provides classes for several types of cost functions metrics : This modules provides metrics classes db : This modules provides sqlalchemy orm tables and utility objects pipeline : This modules provides functions for data preparation","title":"Neural-Net-Numpy(NNN)"},{"location":"#table-of-contents","text":"The documentation consists of three separate parts: Tutorials How-To Guides Reference Quickly find what you're looking for depending on your use case by looking at the different pages. This packages provides modules for computing neural networks Modules exported by this package: activation : provides classes for several types of activation functions layers : This modules provides Layer classes architecture : This modules provides neural network architectures init_funcs : This modules provides initialization functions cost : This modules provides classes for several types of cost functions metrics : This modules provides metrics classes db : This modules provides sqlalchemy orm tables and utility objects pipeline : This modules provides functions for data preparation","title":"Table Of Contents"},{"location":"how-to-guides/","text":"Neural-Net-Numpy(NNN) Creating a Sequential Neural Network 0. Install neural-net-numpy $ pip install -i https://test.pypi.org/simple/ neural-net-numpy 1. Import modules from neural_net package from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros,XavierHe from neural_net.activation import \u03c3,Softmax,LeakyReLU,Tanh,ELU,ReLU from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.metrics import accuracy from neural_net.pipeline import onehot,scaler,shuffle,Batch from neural_net.utils import IrisDatasetDownloader 2. Define Your Model NNN = Sequential( [ Fullyconnected(2,50,XavierHe(\"Uniform\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(50,1,XavierHe(\"Uniform\",\"Sigmoid\").init_func), Activation(\u03c3) ], BinaryCrossEntropy ) 3. Import or create your training dataset import numpy n,k = 5000,2 X = numpy.random.uniform(-100,100,size=(n,k)) y =( (X[:, 0]**2 + X[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 4. Train your model NNN.train(scaler(X),y,\u03b1=\u03b1,epochs=n_epoch,metrics=accuracy) 5. Make predictions NNN.predict(X)","title":"How-To Guides"},{"location":"how-to-guides/#neural-net-numpynnn","text":"","title":"Neural-Net-Numpy(NNN)"},{"location":"how-to-guides/#creating-a-sequential-neural-network","text":"","title":"Creating a Sequential Neural Network"},{"location":"how-to-guides/#0-install-neural-net-numpy","text":"$ pip install -i https://test.pypi.org/simple/ neural-net-numpy","title":"0. Install neural-net-numpy"},{"location":"how-to-guides/#1-import-modules-from-neural_net-package","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros,XavierHe from neural_net.activation import \u03c3,Softmax,LeakyReLU,Tanh,ELU,ReLU from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.metrics import accuracy from neural_net.pipeline import onehot,scaler,shuffle,Batch from neural_net.utils import IrisDatasetDownloader","title":"1. Import modules from neural_net package"},{"location":"how-to-guides/#2-define-your-model","text":"NNN = Sequential( [ Fullyconnected(2,50,XavierHe(\"Uniform\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(50,1,XavierHe(\"Uniform\",\"Sigmoid\").init_func), Activation(\u03c3) ], BinaryCrossEntropy )","title":"2. Define Your Model"},{"location":"how-to-guides/#3-import-or-create-your-training-dataset","text":"import numpy n,k = 5000,2 X = numpy.random.uniform(-100,100,size=(n,k)) y =( (X[:, 0]**2 + X[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0","title":"3. Import or create your training dataset"},{"location":"how-to-guides/#4-train-your-model","text":"NNN.train(scaler(X),y,\u03b1=\u03b1,epochs=n_epoch,metrics=accuracy)","title":"4. Train your model"},{"location":"how-to-guides/#5-make-predictions","text":"NNN.predict(X)","title":"5. Make predictions"},{"location":"reference/","text":"Reference Page Table of Contents layers activation functions neural network architectures initialisation functions cost functions metrics database management data preparation functions Class Models used to build other classes Utility functions Introduction This reference page provides an overview of all functions, classes and methods available in the neural_net project Section 1. layers This module provides Layer classes Fullyconnected Activation Activation Bases: Layer Activation Layer. This layer handles activation for a given activation function Parameters: func ( callable ) \u2013 an activation function like :func: ~activation.\u03c3 Source code in neural_net\\layers.py 29 30 31 32 33 34 35 36 37 38 39 40 class Activation ( Layer ): \"\"\" Activation Layer. This layer handles activation for a given activation function Args: func (callable): an activation function like :func:`~activation.\u03c3` \"\"\" def __init__ ( self , func , * kargs ) -> None : self + locals () Fullyconnected Bases: Layer A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Parameters: n_in ( int ) \u2013 Number of input features. n_out ( int ) \u2013 Number of output features . init_method ( callable ) \u2013 function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func ( callable , default: \u03a3 ) \u2013 default is :func: ~activation.\u03a3 Source code in neural_net\\layers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Fullyconnected ( Layer ): \"\"\" A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Args: n_in (int): Number of input features. n_out (int): Number of output features . init_method (callable): function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func (callable): default is :func:`~activation.\u03a3` \"\"\" def __init__ ( self , n_in : int , n_out : int , init_method : callable , func : callable = \u03a3 ) -> None : self + locals () Section 2. activation functions This module provides classes for several types of activation functions \u03a3 - Linear combination of weights and biases \u03c3 - sigmoid activation Softmax - Softmax activation LeakyReLU - Leaky rectified linear unit activation ELU Bases: Neurons A class representing the Exponential Linear Unit (ELU) activation function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha (e^{z} - 1) & \\text{if } z < 0 \\end{cases} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the ELU activation for input matrix X. pr Computes the derivative of the ELU function. Parameters: \u03b1 ( float , default: 0.001 ) \u2013 The slope coefficient for negative values (default is 0.001). Source code in neural_net\\activation.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 class ELU ( Neurons ): r \"\"\" A class representing the Exponential Linear Unit (ELU) activation function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha (e^{z} - 1) & \\text{if } z < 0 \\end{cases} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the ELU activation for input matrix X. pr(): Computes the derivative of the ELU function. Args: \u03b1 (float): The slope coefficient for negative values (default is 0.001). \"\"\" def __init__ ( self , Layer : Layer = None , \u03b1 = 0.001 ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ELU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self . preds + neg * self [ '\u03b1' ] + ~ neg def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ELU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ELU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * ( numpy . exp ( self . X ) - 1 ) + ~ neg * self . X return self . preds compute ( X ) Computes the ELU activation for input matrix X. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: ELU activation result of shape (n, n_out). Source code in neural_net\\activation.py 281 282 283 284 285 286 287 288 289 290 291 292 293 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ELU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ELU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * ( numpy . exp ( self . X ) - 1 ) + ~ neg * self . X return self . preds pr () Computes the derivative of the ELU function. \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ELU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self . preds + neg * self [ '\u03b1' ] + ~ neg LeakyReLU Bases: Neurons A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha z & \\text{if } z < 0 \\end{cases} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the LeakyReLU activation for input matrix X. pr Computes the derivative of the LeakyReLU function. Parameters: \u03b1 ( float , default: 0.001 ) \u2013 The slope coefficient for negative values (default is 0.001). Source code in neural_net\\activation.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 class LeakyReLU ( Neurons ): r \"\"\" A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha z & \\text{if } z < 0 \\end{cases} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the LeakyReLU activation for input matrix X. pr(): Computes the derivative of the LeakyReLU function. Args: \u03b1 (float): The slope coefficient for negative values (default is 0.001). \"\"\" def __init__ ( self , Layer : Layer = None , \u03b1 : float = .001 ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the LeakyReLU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ '\u03b1' ] + ~ neg def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * self . X + ~ neg * self . X return self . preds compute ( X ) Computes the LeakyReLU activation for input matrix X. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: LeakyReLU activation result of shape (n, n_out). Source code in neural_net\\activation.py 390 391 392 393 394 395 396 397 398 399 400 401 402 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * self . X + ~ neg * self . X return self . preds pr () Computes the derivative of the LeakyReLU function. \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the LeakyReLU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ '\u03b1' ] + ~ neg ReLU Bases: Neurons A class representing the Rectified Linear Unit (ReLU) activation function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the ReLU activation for input matrix X. pr Computes the derivative of the ReLU function. Source code in neural_net\\activation.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 class ReLU ( Neurons ): r \"\"\" A class representing the Rectified Linear Unit (ReLU) activation function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the ReLU activation for input matrix X. pr(): Computes the derivative of the ReLU function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ReLU function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( self . X >= 0 ) + 0 #for casting bool to int def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = numpy . maximum ( 0 , self . X ) return self . preds compute ( X ) Computes the ReLU activation for input matrix X. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: ReLU activation result of shape (n, n_out). Source code in neural_net\\activation.py 335 336 337 338 339 340 341 342 343 344 345 346 347 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = numpy . maximum ( 0 , self . X ) return self . preds pr () Computes the derivative of the ReLU function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ReLU function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( self . X >= 0 ) + 0 #for casting bool to int Softmax Bases: Neurons A class representing the softmax activation function. \\sigma(\\mathbf {z_{i}} )=\\frac {e^{z_{i}}}{\\sum _{j=1}^{k}e^{z_{j}}}\\\\ {\\text{ for }}j=1,\\dotsc ,k{\\text{ features }}{\\text{ and }} z_{i}=(z_{i,1},...,z_{i,n}) \\text{ for n observations} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the Softmax activation for input matrix X. pr Computes the derivative of the Softmax function. Source code in neural_net\\activation.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 class Softmax ( Neurons ): r \"\"\" A class representing the softmax activation function. $$ \\sigma(\\mathbf {z_{i}} )=\\frac {e^{z_{i}}}{\\sum _{j=1}^{k}e^{z_{j}}}\\\\ {\\text{ for }}j=1,\\dotsc ,k{\\text{ features }}{\\text{ and }} z_{i}=(z_{i,1},...,z_{i,n}) \\text{ for n observations} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the Softmax activation for input matrix X. pr(): Computes the derivative of the Softmax function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the Softmax function. Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds ) def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the softmax activation for input matrix X using vectorization with numpy. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . preds compute ( X ) Computes the softmax activation for input matrix X using vectorization with numpy. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Softmax activation result of shape (n, n_out). Source code in neural_net\\activation.py 223 224 225 226 227 228 229 230 231 232 233 234 235 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the softmax activation for input matrix X using vectorization with numpy. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . preds pr () Computes the derivative of the Softmax function. Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 214 215 216 217 218 219 220 221 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the Softmax function. Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds ) Tanh Bases: Neurons A class representing the Hyperbolic Tangent activation function. \\tanh(z)={\\frac{{\\rm{e}}^{z}-{\\rm{e}}^{-z}}{{\\rm {e}}^{z}+{\\rm {e}}^{-z}}}={\\frac{{\\rm {e}}^{2z}-1}{{\\rm{e}}^{2z}+1}}={\\frac{1-{\\rm{e}}^{-2z}}{1+{\\rm {e}}^{-2z}}} Tanh=2\\sigma(2z) - 1 Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the Tanh activation for input matrix X. pr Computes the derivative of the Tanh function. Source code in neural_net\\activation.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Tanh ( Neurons ): r \"\"\" A class representing the Hyperbolic Tangent activation function. $$ \\tanh(z)={\\frac{{\\rm{e}}^{z}-{\\rm{e}}^{-z}}{{\\rm {e}}^{z}+{\\rm {e}}^{-z}}}={\\frac{{\\rm {e}}^{2z}-1}{{\\rm{e}}^{2z}+1}}={\\frac{1-{\\rm{e}}^{-2z}}{1+{\\rm {e}}^{-2z}}} $$ $$ Tanh=2\\sigma(2z) - 1 $$ Attributes: preds: predicted values. Methods: compute(X): Computes the Tanh activation for input matrix X. pr(): Computes the derivative of the Tanh function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the Tanh function. $$ \\tanh '={\\frac {1}{\\cosh ^{2}}}=1-\\tanh ^{2} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return 1 - self . preds ** 2 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the Tanh activation for input matrix X. $$ Tanh(X)=2\\sigma(2X) - 1 $$ where $\\sigma$ is defined as follows: $$ \\sigma (X)={\\frac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Tanh activation result of shape (n, n_out). \"\"\" self . X = X \u03c3 = lambda z : 1 / ( 1 + numpy . exp ( - z )) self . preds = 2 * ( 2 * \u03c3 ( 2 * self . X ) - 1 ) return self . preds compute ( X ) Computes the Tanh activation for input matrix X. Tanh(X)=2\\sigma(2X) - 1 where $\\sigma$ is defined as follows: \\sigma (X)={\\frac {1}{1+e^{-X}}} Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Tanh activation result of shape (n, n_out). Source code in neural_net\\activation.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the Tanh activation for input matrix X. $$ Tanh(X)=2\\sigma(2X) - 1 $$ where $\\sigma$ is defined as follows: $$ \\sigma (X)={\\frac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Tanh activation result of shape (n, n_out). \"\"\" self . X = X \u03c3 = lambda z : 1 / ( 1 + numpy . exp ( - z )) self . preds = 2 * ( 2 * \u03c3 ( 2 * self . X ) - 1 ) return self . preds pr () Computes the derivative of the Tanh function. \\tanh '={\\frac {1}{\\cosh ^{2}}}=1-\\tanh ^{2} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 104 105 106 107 108 109 110 111 112 113 114 115 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the Tanh function. $$ \\tanh '={\\frac {1}{\\cosh ^{2}}}=1-\\tanh ^{2} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return 1 - self . preds ** 2 \u03a3 Bases: Neurons A class representing a linear combination operation. \\mathrm{\\mathit{H}}(z) = z.w + b where w is weights vector and b is bias Attributes: W ( ndarray ) \u2013 Weight matrix of shape (k+1, n_out). +1 for bias Methods: Name Description compute Computes the linear combination of input matrix X and bias vector using weight matrix W. pr Computes the derivative of the linear equation with respect to W (matrix X itself). grad Updates weights self.W and computes the new gradient \u0394 for backpropagation. Xb Concatenates X matrix with a vector of ones Source code in neural_net\\activation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class \u03a3 ( Neurons ): r \"\"\" A class representing a linear combination operation. $$ \\mathrm{\\mathit{H}}(z) = z.w + b $$ where w is weights vector and b is bias Attributes: W (numpy.ndarray): Weight matrix of shape (k+1, n_out). +1 for bias Methods: compute(X): Computes the linear combination of input matrix X and bias vector using weight matrix W. pr(): Computes the derivative of the linear equation with respect to W (matrix X itself). grad(\u0394): Updates weights self.W and computes the new gradient \u0394 for backpropagation. Xb(): Concatenates X matrix with a vector of ones \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () self . W = self . init_method ( self [ 'Layer_n_in' ], self [ 'Layer_n_out' ]) self . Xb = lambda : numpy . c_ [ self . X , numpy . ones (( self . n (), 1 ))] self . instantiateW () self . storeW () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . Xb () def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W ) def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.ndarray): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394 compute ( X ) Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Linear combination result of shape (n, n_out). Source code in neural_net\\activation.py 55 56 57 58 59 60 61 62 63 64 65 66 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W ) grad ( \u0394 ) Updates weights self.W and computes the gradient for backpropagation. Parameters: \u0394 ( ndarray ) \u2013 Gradient from next activation. Source code in neural_net\\activation.py 68 69 70 71 72 73 74 75 76 77 def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.ndarray): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394 pr () Computes the derivative of the linear equation (matrix itself). Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 46 47 48 49 50 51 52 53 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . Xb () \u03c3 Bases: Neurons A class representing the sigmoid activation function. \\sigma(z)={\\frac{1}{1+e^{-z}}}={\\frac{e^{z}}{1+e^{z}}}=1-\\sigma(-z) Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the sigmoid activation for input matrix X. pr Computes the derivative of the sigmoid function. Source code in neural_net\\activation.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class \u03c3 ( Neurons ): r \"\"\" A class representing the sigmoid activation function. $$ \\sigma(z)={\\frac{1}{1+e^{-z}}}={\\frac{e^{z}}{1+e^{z}}}=1-\\sigma(-z) $$ Attributes: preds: predicted values. Methods: compute(X): Computes the sigmoid activation for input matrix X. pr(): Computes the derivative of the sigmoid function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the sigmoid function. $$ {\\begin{aligned}\\sigma'(z)&={\\frac {e^{z}\\cdot (1+e^{z})-e^{z}\\cdot e^{z}}{(1+e^{z})^{2}}}\\\\&={\\frac {e^{z}}{(1+e^{z})^{2}}}\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left({\\frac {1}{1+e^{z}}}\\right)\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left(1-{\\frac {e^{z}}{1+e^{z}}}\\right)\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\\end{aligned}} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds ) def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the sigmoid activation for input matrix X using vectorization with numpy. $$ \\sigma (X)={\\dfrac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . preds = 1 / ( 1 + numpy . exp ( - self . X )) return self . preds compute ( X ) Computes the sigmoid activation for input matrix X using vectorization with numpy. \\sigma (X)={\\dfrac {1}{1+e^{-X}}} Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Sigmoid activation result of shape (n, n_out). Source code in neural_net\\activation.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the sigmoid activation for input matrix X using vectorization with numpy. $$ \\sigma (X)={\\dfrac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . preds = 1 / ( 1 + numpy . exp ( - self . X )) return self . preds pr () Computes the derivative of the sigmoid function. {\\begin{aligned}\\sigma'(z)&={\\frac {e^{z}\\cdot (1+e^{z})-e^{z}\\cdot e^{z}}{(1+e^{z})^{2}}}\\\\&={\\frac {e^{z}}{(1+e^{z})^{2}}}\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left({\\frac {1}{1+e^{z}}}\\right)\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left(1-{\\frac {e^{z}}{1+e^{z}}}\\right)\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\\end{aligned}} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 162 163 164 165 166 167 168 169 170 171 172 173 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the sigmoid function. $$ {\\begin{aligned}\\sigma'(z)&={\\frac {e^{z}\\cdot (1+e^{z})-e^{z}\\cdot e^{z}}{(1+e^{z})^{2}}}\\\\&={\\frac {e^{z}}{(1+e^{z})^{2}}}\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left({\\frac {1}{1+e^{z}}}\\right)\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left(1-{\\frac {e^{z}}{1+e^{z}}}\\right)\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\\end{aligned}} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds ) Section 3. neural network architectures This module provides neural network architectures Currently available are Sequential - Sequential linear net architecture Sequential Bases: Architecture Source code in neural_net\\architecture.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class Sequential ( Architecture ): def __init__ ( self , steps : list [ Layer ], cost : Cost , store : bool = False ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. store (bool): If True disables identification and storage Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" Define . _Define__store = store self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit () def train ( self , X : numpy . ndarray = None , y : numpy . ndarray = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.ndarray): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.ndarray): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm ( range ( epochs ), ascii = ' =' ) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit () __init__ ( steps , cost , store = False ) Initialize a Sequential class. Parameters: steps ( List [ Layer ] ) \u2013 A list of Layer objects representing the steps. cost ( Cost ) \u2013 A Cost object for computing cost information. store ( bool , default: False ) \u2013 If True disables identification and storage Example: layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) Source code in neural_net\\architecture.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , steps : list [ Layer ], cost : Cost , store : bool = False ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. store (bool): If True disables identification and storage Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" Define . _Define__store = store self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit () train ( X = None , y = None , batch = None , epochs = 100 , \u03b1 = 0.001 , metrics = Empty ) Trains a neural network model using sequential architecture Parameters: X ( ndarray , default: None ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( ndarray , default: None ) \u2013 Target variable with shape (n, 1). batch ( Optional [ Batch ] , default: None ) \u2013 Optional Batch object that generates batches from the training data. epochs ( int , default: 100 ) \u2013 Maximum number of training epochs. \u03b1 ( float , default: 0.001 ) \u2013 Learning rate (step size for weight updates). metrics ( Metrics , default: Empty ) \u2013 Metrics object that computes evaluation metrics (e.g., accuracy). Example: from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) Source code in neural_net\\architecture.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def train ( self , X : numpy . ndarray = None , y : numpy . ndarray = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.ndarray): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.ndarray): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm ( range ( epochs ), ascii = ' =' ) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit () Section 4. initialisation functions This module provides initialization functions zeros(n_in: int, n_out: int) - Initializes a weight matrix with zeros XHsigmoiduniform - AA function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. XHReluuniform - A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. XavierHe This class implements Xavier Glorot and He initializations.(source Hands on ML) Attributes: random ( dict ) \u2013 contains generators of random values by distribution. activation ( str ) \u2013 Name of activation. Parameters: distribution ( str [ Uniform , Normal ] ) \u2013 Name of distribution. activation ( str [ Sigmoid , Tanh , ReLU ] ) \u2013 Name of activation. Attributes: init_func \u2013 func(n_in,n_out,biais=True) for generating random values Source code in neural_net\\init_funcs.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class XavierHe : \"\"\" This class implements Xavier Glorot and He initializations.(source Hands on ML) ![png](static/xahe.png) Attributes: random (dict): contains generators of random values by distribution. activation (str): Name of activation. Args: distribution (str[\"Uniform\",\"Normal\"]): Name of distribution. activation (str[\"Sigmoid\",\"Tanh\",\"ReLU\"]): Name of activation. Attributes: init_func : func(n_in,n_out,biais=True) for generating random values \"\"\" random = { \"Uniform\" : lambda r , n_in , n_out , biais : numpy . random . uniform ( - r , r , size = ( n_in + biais , n_out )), \"Normal\" : lambda \u03c3 , n_in , n_out , biais : numpy . random . normal ( scale = \u03c3 , size = ( n_in + biais , n_out )) } _weight = { \"Sigmoid\" : 1 , \"Tanh\" : 4 , \"ReLU\" : 2 ** .5 } default_values = { \"Uniform\" : lambda n_in , n_out : ( 6 / ( n_in + n_out )) ** .5 , \"Normal\" : lambda n_in , n_out : ( 2 / ( n_in + n_out )) ** .5 , } @property def weight ( self ): return XavierHe . _weight . get ( self . activation ) @property def param ( self ): return XavierHe . default_values . get ( self . distribution ) @property def init_func ( self ): gen = XavierHe . random . get ( self . distribution ) eq = lambda n_in , n_out , biais = True : gen ( self . weight * self . param ( n_in , n_out ), n_in , n_out , biais ) return eq def __init__ ( self , distribution : Literal [ \"Uniform\" , \"Normal\" ], activation : Literal [ \"Sigmoid\" , \"Tanh\" , \"ReLU\" ]) -> None : self . activation = activation self . distribution = distribution XHReluuniform ( n_in , n_out , biais = True ) A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). biais ( bool , default: True ) \u2013 if True adds biais weights Returns: ndarray \u2013 numpy.ndarray : array of random values Source code in neural_net\\init_funcs.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def XHReluuniform ( n_in : int , n_out : int , biais : bool = True ) -> numpy . ndarray : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Args: n_in (int): Number of input units. n_out (int): Number of output units (neurons). biais (bool): if True adds biais weights returns: numpy.ndarray : array of random values \"\"\" r = 2 ** .5 * ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + biais , n_out )) XHsigmoiduniform ( n_in , n_out , biais = True ) A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). biais ( bool ) \u2013 if True adds biais weights Returns: ndarray \u2013 numpy.ndarray : array of random values Source code in neural_net\\init_funcs.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def XHsigmoiduniform ( n_in : int , n_out : int , biais : bool = True ) -> numpy . ndarray : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in (int): Number of input units. n_out (int): Number of output units (neurons). biais (bool): if True adds biais weights returns: numpy.ndarray : array of random values \"\"\" r = ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + biais , n_out )) zeros ( n_in , n_out , biais = True ) Initializes a weight matrix with zeros. Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units. biais ( bool , default: True ) \u2013 if True adds biais weights Returns: ndarray \u2013 numpy.ndarray: Weight matrix of shape (n_in + 1, n_out). Source code in neural_net\\init_funcs.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def zeros ( n_in : int , n_out : int , biais : bool = True ) -> numpy . ndarray : \"\"\" Initializes a weight matrix with zeros. Args: n_in (int): Number of input units. n_out (int): Number of output units. biais (bool): if True adds biais weights Returns: numpy.ndarray: Weight matrix of shape (n_in + 1, n_out). \"\"\" return numpy . zeros (( n_in + biais , n_out )) Section 5. cost functions This module provides classes for several types of cost functions binaryCrossEntropy CrossEntropy MSE BinaryCrossEntropy Bases: Cost Binary Cross-Entropy Loss. \\mathrm{\\mathit{Binary\\ Cross\\ Entropy}}(p, y) = \\begin{cases} -\\log(p) & \\text{if } y = 1, \\\\ -\\log(1-p) & \\text{otherwise.} \\end{cases} This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the binary cross-entropy loss. - pr numpy.ndarray, p: numpy.ndarray) -> numpy.ndarray: Computes the derivative function values. Example >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] Source code in neural_net\\cost.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class BinaryCrossEntropy ( Cost ): r \"\"\" Binary Cross-Entropy Loss. $$ \\mathrm{\\mathit{Binary\\ Cross\\ Entropy}}(p, y) = \\begin{cases} -\\log(p) & \\text{if } y = 1, \\\\ -\\log(1-p) & \\text{otherwise.} \\end{cases} $$ This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the binary cross-entropy loss. - pr(y: numpy.ndarray, p: numpy.ndarray) -> numpy.ndarray: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.ndarray: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean () compute ( y , p , clip = True ) Computes the binary cross-entropy loss. Parameters: y ( ndarray ) \u2013 True labels (0 or 1). p ( ndarray ) \u2013 Predicted probabilities (between 0 and 1). clip ( bool , default: True ) \u2013 Whether or not to clip predicted values see method clip Returns: float ( float ) \u2013 Binary cross-entropy loss value. Source code in neural_net\\cost.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean () pr () Computes the derivative function values with respet to p. Returns: ndarray \u2013 numpy.ndarray: Derivative function values. Source code in neural_net\\cost.py 46 47 48 49 50 51 52 53 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.ndarray: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) CrossEntropy Bases: Cost Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). Cross\\ Entropy(p,y) = -\\sum _{i}\\sum _{j}y_{ij}\\log p_{ij}\\ Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the cross-entropy loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) Source code in neural_net\\cost.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class CrossEntropy ( Cost ): r \"\"\" Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). $$ Cross\\ Entropy(p,y) = -\\sum _{i}\\sum _{j}y_{ij}\\log p_{ij}\\ $$ Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the cross-entropy loss. - pr() -> numpy.ndarray: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" left = ( self . y / self . p ) #right = left.sum(axis=1,keepdims=True)*(f:=((1-self.y)/(1-self.p)))/f.sum(axis=1,keepdims=True) right = ( 1 - self . y ) / ( 1 - self . p ) return - ( left - right ) def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip. Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return ( - self . y * numpy . log ( self . p )) . sum ( axis = 1 ) . mean () compute ( y , p , clip = True ) Computes the Cross-entropy loss. Parameters: y ( ndarray ) \u2013 True labels (0 or 1). p ( ndarray ) \u2013 Predicted probabilities (between 0 and 1). clip ( bool , default: True ) \u2013 Whether or not to clip predicted values see method clip. Returns: float ( float ) \u2013 Cross-entropy loss value. Source code in neural_net\\cost.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip. Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return ( - self . y * numpy . log ( self . p )) . sum ( axis = 1 ) . mean () pr () Computes the derivative function values with respet to p . Returns: ndarray \u2013 numpy.ndarray: Derivative function values. Source code in neural_net\\cost.py 113 114 115 116 117 118 119 120 121 122 123 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" left = ( self . y / self . p ) #right = left.sum(axis=1,keepdims=True)*(f:=((1-self.y)/(1-self.p)))/f.sum(axis=1,keepdims=True) right = ( 1 - self . y ) / ( 1 - self . p ) return - ( left - right ) MSE Bases: Cost Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). \\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left(y_{i}-{\\hat {y_{i}}}\\right)^{2} Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean squared error loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] Source code in neural_net\\cost.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class MSE ( Cost ): r \"\"\" Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). $$ \\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left(y_{i}-{\\hat {y_{i}}}\\right)^{2} $$ Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean squared error loss. - pr() -> numpy.ndarray: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" return - 2 * ( self . y - self . p ) def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean () compute ( y , p ) Computes the mean squared error loss. Parameters: y ( ndarray ) \u2013 True labels (ground truth). p ( ndarray ) \u2013 Predicted values. Returns: float ( float ) \u2013 Mean squared error loss value. Source code in neural_net\\cost.py 184 185 186 187 188 189 190 191 192 193 194 195 196 def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean () pr () Computes the derivative function values with respet to p . Returns: ndarray \u2013 numpy.ndarray: Derivative function values. Source code in neural_net\\cost.py 176 177 178 179 180 181 182 183 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" return - 2 * ( self . y - self . p ) Section 6. metrics This module provides metrics classes accuracy MAE MAE Bases: Metrics Mean Absolute Error (MAE) . This class computes the mean absolute error loss between true labels (y) and predicted values (p). \\displaystyle \\operatorname {MAE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left|y_{i}-{\\hat {y_{i}}}\\right| Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean absolute error. Example >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mae_loss = MAE() >>> loss_value = mae_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] Source code in neural_net\\metrics.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class MAE ( Metrics ): r \"\"\" Mean Absolute Error (MAE) . This class computes the mean absolute error loss between true labels (y) and predicted values (p). $$ \\displaystyle \\operatorname {MAE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left|y_{i}-{\\hat {y_{i}}}\\right| $$ Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean absolute error. Example: ```python >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mae_loss = MAE() >>> loss_value = mae_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean absolute error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean absolute error value. \"\"\" self . y , self . p = y , p return numpy . abs ( self . y - self . p ) . mean () compute ( y , p ) Computes the mean absolute error loss. Parameters: y ( ndarray ) \u2013 True labels (ground truth). p ( ndarray ) \u2013 Predicted values. Returns: float ( float ) \u2013 Mean absolute error value. Source code in neural_net\\metrics.py 118 119 120 121 122 123 124 125 126 127 128 129 130 def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean absolute error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean absolute error value. \"\"\" self . y , self . p = y , p return numpy . abs ( self . y - self . p ) . mean () accuracy Bases: Metrics Calculates the accuracy metric for binary or multiclass classification tasks. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Attributes: threshold ( float ) \u2013 The threshold value used for binary classification. Methods: Name Description compute Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 Source code in neural_net\\metrics.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class accuracy ( Metrics ): \"\"\" Calculates the accuracy metric for binary or multiclass classification tasks. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. Attributes: threshold (float): The threshold value used for binary classification. Methods: compute(y, p): Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: ```python >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 ``` \"\"\" def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 ) __init__ ( threshold = 0.5 ) Initializes the accuracy metric. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Source code in neural_net\\metrics.py 59 60 61 62 63 64 65 66 def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold compute ( y , p ) Computes the accuracy of predictions. Parameters: y ( ndarray ) \u2013 True labels (ground truth). p ( ndarray ) \u2013 Predicted values. Returns: float ( float ) \u2013 accuracy value. Source code in neural_net\\metrics.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 ) Section 7. database management This module provides sqlalchemy orm tables and utility objects DefaultTable - generic table template Architecture - Architecture table Layer - layer table Neurons - neurons table Cost - cost table Weight - weight table DBmanager Manages database connections and sessions using SQLAlchemy. Attributes: session ( Session ) \u2013 SQLAlchemy session for database operations. Methods: Name Description __start str=None)-> None: Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. Source code in neural_net\\db.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class DBmanager : \"\"\" Manages database connections and sessions using SQLAlchemy. Attributes: session (Session): SQLAlchemy session for database operations. Methods: __start(db : str=None)-> None: Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. \"\"\" engines = {} status = False def __start ( db : str = None ) -> None : \"\"\" Starts a session Args: db (str, optional): Path to database server or SQLite database file. Defaults to None. \"\"\" db_path = db or f 'sqlite:/// { get_module_path ([ \"run\" , f \"model { now () } .db\" ]) } ' DBmanager . db_path = db_path DBmanager . engines [ DBmanager . db_path ] = create_engine ( DBmanager . db_path ) Base . metadata . create_all ( DBmanager . engines [ DBmanager . db_path ]) Session = sessionmaker ( bind = DBmanager . engines [ DBmanager . db_path ]) DBmanager . session = Session () def add_table ( self , table : DefaultTable ) -> None : \"\"\" Adds a table instance to the current session. Args: table (DefaultTable): An instance of table object \"\"\" if not DBmanager . status : DBmanager . _DBmanager__start () DBmanager . status = True DBmanager . session . add ( table ) __start ( db = None ) Starts a session Parameters: db ( str , default: None ) \u2013 Path to database server or SQLite database file. Defaults to None. Source code in neural_net\\db.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __start ( db : str = None ) -> None : \"\"\" Starts a session Args: db (str, optional): Path to database server or SQLite database file. Defaults to None. \"\"\" db_path = db or f 'sqlite:/// { get_module_path ([ \"run\" , f \"model { now () } .db\" ]) } ' DBmanager . db_path = db_path DBmanager . engines [ DBmanager . db_path ] = create_engine ( DBmanager . db_path ) Base . metadata . create_all ( DBmanager . engines [ DBmanager . db_path ]) Session = sessionmaker ( bind = DBmanager . engines [ DBmanager . db_path ]) DBmanager . session = Session () add_table ( table ) Adds a table instance to the current session. Parameters: table ( DefaultTable ) \u2013 An instance of table object Source code in neural_net\\db.py 124 125 126 127 128 129 130 131 132 133 134 135 def add_table ( self , table : DefaultTable ) -> None : \"\"\" Adds a table instance to the current session. Args: table (DefaultTable): An instance of table object \"\"\" if not DBmanager . status : DBmanager . _DBmanager__start () DBmanager . status = True DBmanager . session . add ( table ) get_instance ( self ) Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table ( DefaultTable ) \u2013 SQLAlchemy Table object corresponding to the specified table name. Source code in neural_net\\db.py 70 71 72 73 74 75 76 77 78 79 def get_instance ( self ) -> DefaultTable : \"\"\" Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table: SQLAlchemy Table object corresponding to the specified table name. \"\"\" table , cols = tables [ str ( self )] values = { k : v for k , v in self . id . items () if k in cols } return table ( ** values ) update_instance ( self ) Updates the given instance with the provided keyword arguments. Source code in neural_net\\db.py 81 82 83 84 85 86 87 88 89 def update_instance ( self ) -> None : \"\"\" Updates the given instance with the provided keyword arguments. \"\"\" _ , cols = tables [ str ( self )] for k , v in self . id . items (): if k in cols : setattr ( self . table , k , v ) Section 8. data preparation functions This module provides functions for data preparation Batch - feed data in chunks shuffle - shuffles train sets onehot - onehot encodes target variables scaler - scales input features Batch Source code in neural_net\\pipeline.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Batch : def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.ndarray): function providing access to Numpy array containing features. y (numpy.ndarray): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0 def __iter__ ( self ): return self def __next__ ( self ): if self . c < len ( self . ix ): self . c += 1 return self . getters ( self . ix [ self . c - 1 ]) self . c = 0 raise StopIteration __init__ ( size , obs , X , y ) Initialize a Batch object. Parameters: size ( int ) \u2013 Size of each batch. obs ( int ) \u2013 Total sample size. X ( ndarray ) \u2013 function providing access to Numpy array containing features. y ( ndarray ) \u2013 function providing access to Numpy array containing target variable. Returns: None \u2013 None Example: >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] Source code in neural_net\\pipeline.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.ndarray): function providing access to Numpy array containing features. y (numpy.ndarray): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0 get_ix ( size , obs ) Create batch slices for a given sample size and batch size. Parameters: obs ( int ) \u2013 Total number of samples in the dataset. size ( int ) \u2013 Size of each batch. Returns: list [ slice ] \u2013 list[slice]: A list of slice objects representing batch indices. Example: >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] Source code in neural_net\\pipeline.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def get_ix ( size : int , obs : int ) -> list [ slice ]: \"\"\" Create batch slices for a given sample size and batch size. Args: obs (int): Total number of samples in the dataset. size (int): Size of each batch. Returns: list[slice]: A list of slice objects representing batch indices. Example: ```python >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] ``` \"\"\" batchix = list ( range ( 0 , obs , size )) if batchix [ - 1 ] < obs : batchix . append ( obs ) batchix = [ slice ( low , high ) for low , high in zip ( batchix , batchix [ 1 :])] return batchix onehot ( y ) One-hot encodes a categorical target variable. Parameters: y ( ndarray ) \u2013 Numpy array containing the categorical target variable. Returns: ndarray \u2013 numpy.ndarray: One-hot encoded representation of the target variable. Example: >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] Source code in neural_net\\pipeline.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def onehot ( y : numpy . ndarray ) -> numpy . ndarray : \"\"\" One-hot encodes a categorical target variable. Args: y (numpy.ndarray): Numpy array containing the categorical target variable. Returns: numpy.ndarray: One-hot encoded representation of the target variable. Example: ```python >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] ``` \"\"\" return ( y == numpy . unique ( y )) + 0 scaler ( X ) Custom scaler function for centering and standardizing features. Parameters: X ( ndarray ) \u2013 Input numpy array containing features. Returns: ndarray \u2013 numpy.ndarray: Scaled version of the input array. Example: >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] Source code in neural_net\\pipeline.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def scaler ( X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Custom scaler function for centering and standardizing features. Args: X (numpy.ndarray): Input numpy array containing features. Returns: numpy.ndarray: Scaled version of the input array. Example: ```python >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] ``` \"\"\" return ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) shuffle ( X , y ) shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Parameters: X ( ndarray ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( ndarray ) \u2013 Target variable with shape (n, 1). Returns: tuple [ ndarray , ndarray ] \u2013 Tuple[numpy.ndarray, numpy.ndarray]: Shuffled X and y arrays. Example: >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. Source code in neural_net\\pipeline.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def shuffle ( X : numpy . ndarray , y : numpy . ndarray ) -> tuple [ numpy . ndarray , numpy . ndarray ]: \"\"\" shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Args: X (numpy.ndarray): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.ndarray): Target variable with shape (n, 1). Returns: Tuple[numpy.ndarray, numpy.ndarray]: Shuffled X and y arrays. Example: ```python >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. ``` \"\"\" X = pandas . DataFrame ( X ) . sample ( frac = 1 ) y = pandas . DataFrame ( y ) . loc [ X . index ] return X . values , y . values 9. Class Models used to build other classes This modules provides genric templates for other modules Define - generic object Architecture - Architecture super object Layer - layer super object Neurons - neurons super object Cost - cost super object Metrics - weight super object Architecture Bases: Define Model for Architecture functions see :func: ~architecture.Sequential Source code in neural_net\\model.py 326 327 328 329 330 331 332 333 class Architecture ( Define ): \"\"\" Model for Architecture functions see :func:`~architecture.Sequential` \"\"\" def __str__ ( self ) -> str : return 'Architecture' Cost Bases: Define Model for Cost functions see :func: ~cost.binaryCrossEntropy Source code in neural_net\\model.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class Cost ( Define ): \"\"\" Model for Cost functions see :func:`~cost.binaryCrossEntropy` \"\"\" def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 ) def __str__ ( self ) -> str : return 'Cost' clip () Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than np.minimum(a_max, np.maximum(a, a_min)) . No check is performed to ensure a_min < a_max . Parameters a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If None , clipping is not performed on the corresponding edge. Only one of a_min and a_max may be None . Both are broadcast against a . out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. out must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref: ufunc docs <ufuncs.kwargs> . .. versionadded:: 1.17.0 Returns clipped_array : ndarray An array with the elements of a , but where values < a_min are replaced with a_min , and those > a_max with a_max . See Also :ref: ufuncs-output-type Notes When a_min is greater than a_max , clip returns an array in which all values are equal to a_max , as shown in the second example. Examples >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) Source code in neural_net\\model.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 ) Define Bases: DBmanager Source code in neural_net\\model.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class Define ( DBmanager ): __store = True def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__ @property def id ( self ) -> dict : return self . _id @id . setter def id ( self , loc ) -> None : \"\"\" Sets id property for the class Instantiate and stores sqlalchemy table of self \"\"\" loc = unfold ( loc ) self . _id = { 'id' : id ( self ), f ' { str ( self ) } _id' : id ( self ), 'name' : repr ( self ), ** loc } if Define . _Define__store : if not hasattr ( self , 'table' ): self . table = get_instance ( self ) self . add_table ( self . table ) else : update_instance ( self ) self . commit () def __getitem__ ( self , ix ) -> any : return self . id [ ix ] def __setitem__ ( self , ix , val ) -> None : self . id [ ix ] = val @property def get ( self ) -> dict . get : return self . id . get def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent } def __iter__ ( self ) -> object : return self def __len__ ( self ) -> int : return len ( self [ 'steps' ]) def __next__ ( self ) -> any : if self . c < len ( self ): self . c += 1 return self [ 'steps' ][ self . c - 1 ] self . c = 0 raise StopIteration def commit ( self ) -> None : if Define . _Define__store : DBmanager . session . commit () def predict ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.ndarray) : input features matrix Returns: numpy.ndarray of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out def update ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.ndarray) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) if Define . _Define__store : self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ]) __add__ ( loc ) Triggers the id property Parameters: loc ( dict) ) \u2013 dictionary of properties Source code in neural_net\\model.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent } __repr__ () Returns the name of the class. Returns: str ( str ) \u2013 The name of the class. Source code in neural_net\\model.py 19 20 21 22 23 24 25 26 def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__ compute_store () Generic method that computes item and stores it to sqlalchemy session Source code in neural_net\\model.py 131 132 133 134 135 136 137 138 139 140 141 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) if Define . _Define__store : self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value predict ( X ) Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Parameters: X ( numpy.ndarray) ) \u2013 input features matrix Returns: ndarray \u2013 numpy.ndarray of output layer predictions Source code in neural_net\\model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def predict ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.ndarray) : input features matrix Returns: numpy.ndarray of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out update ( \u0394 ) Implement backpropagation of cost gradient to all layers Passes gradients backward Parameters: \u0394 ( numpy.ndarray) ) \u2013 array of gradient from next step Returns: ndarray \u2013 numpy array of input layer gradient Source code in neural_net\\model.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def update ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.ndarray) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394 updateW () Updates sqlalchemy tables containing weights Source code in neural_net\\model.py 143 144 145 146 147 148 149 150 151 def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ]) Layer Bases: Define Model for layer functions see :func: ~layer.fullyconnected Source code in neural_net\\model.py 153 154 155 156 157 158 159 160 class Layer ( Define ): \"\"\" Model for layer functions see :func:`~layer.fullyconnected` \"\"\" def __str__ ( self ) -> str : return 'Layer' Metrics Bases: Define Model for Metrics functions see :func: ~metrics.accuracy Source code in neural_net\\model.py 317 318 319 320 321 322 323 324 class Metrics ( Define ): \"\"\" Model for Metrics functions see :func:`~metrics.accuracy` \"\"\" def __str__ ( self ) -> str : return 'Metrics' Neurons Bases: Define Model for activation functions see :func: ~activation.Softmax Source code in neural_net\\model.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class Neurons ( Define ): \"\"\" Model for activation functions see :func:`~activation.Softmax` \"\"\" with_weights = [] def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" if Define . _Define__store : table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ] def storeW ( self ): \"\"\" Stores weights tables \"\"\" if Define . _Define__store : for row in self . Wtables : for table in row : self . add_table ( table ) def __str__ ( self ) -> str : return 'Neurons' def __sub__ ( self , \u0394 : numpy . ndarray ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ] def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.ndarray) : gradient from next step Returns: numpy.ndarray of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394 __sub__ ( \u0394 ) Substracts Gradient to Weights Source code in neural_net\\model.py 204 205 206 207 208 209 def __sub__ ( self , \u0394 : numpy . ndarray ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394 grad ( \u0394 ) Computes gradient for previous step Parameters: \u0394 ( numpy.ndarray) ) \u2013 gradient from next step Returns: ndarray \u2013 numpy.ndarray of gradient for previous step Source code in neural_net\\model.py 218 219 220 221 222 223 224 225 226 227 228 229 230 def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.ndarray) : gradient from next step Returns: numpy.ndarray of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394 instantiateW () Instantiate weight tables Source code in neural_net\\model.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" if Define . _Define__store : table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ] n () Returns sample size for current features matrix Source code in neural_net\\model.py 211 212 213 214 215 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ] storeW () Stores weights tables Source code in neural_net\\model.py 192 193 194 195 196 197 198 199 def storeW ( self ): \"\"\" Stores weights tables \"\"\" if Define . _Define__store : for row in self . Wtables : for table in row : self . add_table ( table ) 10. Utility functions Provides utility functions get_module_path - Returns the path to a subdirectory named 'dir' relative to the currently executed script. now - current timestamp unfold - Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. HouseDatasetDownloader Downloads boston housing dataset. Parameters: src \u2013 str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: columns \u2013 list of str ordered list of columns in the dataset. data \u2013 array data array of features and target variable csv \u2013 str Raw CSV textfile description \u2013 str Full description of housing database Example >>> houseloader = HouseDatasetDownloader() >>> houseloader.load_dataset() >>> print(houseloader.columns) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] >>> print(houseloader.data) [[6.3200e-03 1.8000e+01 2.3100e+00 ... 3.9690e+02 4.9800e+00 2.4000e+01] [2.7310e-02 0.0000e+00 7.0700e+00 ... 3.9690e+02 9.1400e+00 2.1600e+01] [2.7290e-02 0.0000e+00 7.0700e+00 ... 3.9283e+02 4.0300e+00 3.4700e+01] ... [6.0760e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 5.6400e+00 2.3900e+01] [1.0959e-01 0.0000e+00 1.1930e+01 ... 3.9345e+02 6.4800e+00 2.2000e+01] [4.7410e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 7.8800e+00 1.1900e+01]] Source code in neural_net\\utils.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 class HouseDatasetDownloader : \"\"\" Downloads boston housing dataset. Parameters: src : str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: columns : list of str ordered list of columns in the dataset. data : array data array of features and target variable csv : str Raw CSV textfile description : str Full description of housing database Example: ```python >>> houseloader = HouseDatasetDownloader() >>> houseloader.load_dataset() >>> print(houseloader.columns) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] >>> print(houseloader.data) [[6.3200e-03 1.8000e+01 2.3100e+00 ... 3.9690e+02 4.9800e+00 2.4000e+01] [2.7310e-02 0.0000e+00 7.0700e+00 ... 3.9690e+02 9.1400e+00 2.1600e+01] [2.7290e-02 0.0000e+00 7.0700e+00 ... 3.9283e+02 4.0300e+00 3.4700e+01] ... [6.0760e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 5.6400e+00 2.3900e+01] [1.0959e-01 0.0000e+00 1.1930e+01 ... 3.9345e+02 6.4800e+00 2.2000e+01] [4.7410e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 7.8800e+00 1.1900e+01]] ``` \"\"\" def __init__ ( self , src = \"http://lib.stat.cmu.edu/datasets/boston\" ): self . src = src self . csv = None def load_dataset ( self ): \"\"\" Load the House dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text self . description , data = self . csv [: self . csv . index ( \"0.00632\" )], self . csv [ self . csv . index ( \"0.00632\" ):] data = data . replace ( ' \\n ' , ' ' ) . split ( ' \\n ' )[: - 1 ] data = [ re . findall ( r '(\\d+\\.*\\d*)' , r ) for r in data ] self . data = numpy . array ( data ) . astype ( float ) self . columns = columns = [ c . split ()[ 0 ] for c in self . description . split ( ' \\n ' )[ - 16 : - 2 ]] load_dataset () Load the House dataset from the specified online CSV source. Source code in neural_net\\utils.py 408 409 410 411 412 413 414 415 416 417 418 def load_dataset ( self ): \"\"\" Load the House dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text self . description , data = self . csv [: self . csv . index ( \"0.00632\" )], self . csv [ self . csv . index ( \"0.00632\" ):] data = data . replace ( ' \\n ' , ' ' ) . split ( ' \\n ' )[: - 1 ] data = [ re . findall ( r '(\\d+\\.*\\d*)' , r ) for r in data ] self . data = numpy . array ( data ) . astype ( float ) self . columns = columns = [ c . split ()[ 0 ] for c in self . description . split ( ' \\n ' )[ - 16 : - 2 ]] IrisDatasetDownloader Downloads the Iris dataset from an online CSV source. Parameters: src \u2013 str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: target_names \u2013 list of str Names of each target label (species) in the dataset. feature_names \u2013 list of str Names of all features (attributes) in the dataset. csv \u2013 str raw CSV textfile description \u2013 str Full description of iris database data \u2013 numpy.ndarray array of all features target \u2013 numpy.ndarray array of target variable Example >>> iris = IrisDatasetDownloader() >>> iris.load_dataset() >>> print(iris.data.shape,iris.target.shape) (150, 4) (150, 1) >>> print(iris.target_names) ['setosa', 'versicolor', 'virginica'] >>> print(iris.feature_names) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] >>> print(iris.data[:5,:]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] >>> print(iris.target[:5,:]) [[0] [0] [0] [0] [0]] Source code in neural_net\\utils.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 class IrisDatasetDownloader : \"\"\" Downloads the Iris dataset from an online CSV source. Parameters: src : str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: target_names : list of str Names of each target label (species) in the dataset. feature_names : list of str Names of all features (attributes) in the dataset. csv : str raw CSV textfile description : str Full description of iris database data : numpy.ndarray array of all features target : numpy.ndarray array of target variable Example: ```python >>> iris = IrisDatasetDownloader() >>> iris.load_dataset() >>> print(iris.data.shape,iris.target.shape) (150, 4) (150, 1) >>> print(iris.target_names) ['setosa', 'versicolor', 'virginica'] >>> print(iris.feature_names) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] >>> print(iris.data[:5,:]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] >>> print(iris.target[:5,:]) [[0] [0] [0] [0] [0]] ``` \"\"\" description = r \"\"\" 1. Title: Iris Plants Database Updated Sept 21 by C.Blake - Added discrepency information 2. Sources: (a) Creator: R.A. Fisher (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) (c) Date: July, 1988 3. Past Usage: - Publications: too many to mention!!! Here are a few. 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. -- Results: -- very low misclassification rates (0 % f or the setosa class) 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. -- Results: -- very low misclassification rates again 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. 4. Relevant Information: --- This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. --- Predicted attribute: class of iris plant. --- This is an exceedingly simple domain. --- This data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick@espeedaz.net ) The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features. 5. Number of Instances: 150 (50 in each of three classes) 6. Number of Attributes: 4 numeric, predictive attributes and the class 7. Attribute Information: 1. sepal length in cm 2. sepal width in cm 3. petal length in cm 4. petal width in cm 5. class: -- Iris Setosa -- Iris Versicolour -- Iris Virginica 8. Missing Attribute Values: None Summary Statistics: Min Max Mean SD Class Correlation sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) 9. Class Distribution: 33.3 % f or each of 3 classes. \"\"\" def __init__ ( self , src = \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\" ): self . src = src self . feature_names = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ] self . target_names = [] self . csv = None def load_dataset ( self ): \"\"\" Load the Iris dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text columns , * data , _ = self . csv . split ( ' \\n ' ) data , target = [ r . split ( ',' )[: - 1 ] for r in data ],[ r . split ( ',' )[ - 1 ] for r in data ] self . target_names = [] for l in target : if l not in self . target_names : self . target_names += [ l ] self . target = numpy . array ([[ self . target_names . index ( l )] for l in target ]) self . feature_names = columns . split ( ',' )[: - 1 ] for i in range ( len ( self . feature_names )): self . feature_names [ i ] = self . feature_names [ i ] . replace ( '_' , ' ' ) self . feature_names [ i ] += ' (cm)' self . data = numpy . array ( data ) . astype ( float ) load_dataset () Load the Iris dataset from the specified online CSV source. Source code in neural_net\\utils.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 def load_dataset ( self ): \"\"\" Load the Iris dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text columns , * data , _ = self . csv . split ( ' \\n ' ) data , target = [ r . split ( ',' )[: - 1 ] for r in data ],[ r . split ( ',' )[ - 1 ] for r in data ] self . target_names = [] for l in target : if l not in self . target_names : self . target_names += [ l ] self . target = numpy . array ([[ self . target_names . index ( l )] for l in target ]) self . feature_names = columns . split ( ',' )[: - 1 ] for i in range ( len ( self . feature_names )): self . feature_names [ i ] = self . feature_names [ i ] . replace ( '_' , ' ' ) self . feature_names [ i ] += ' (cm)' self . data = numpy . array ( data ) . astype ( float ) Pearson Computes the Pearson correlation matrix and generates a heatmap. Attributes: X \u2013 numpy.ndarray The input data containing features. n \u2013 int number of observations k \u2013 int number of features cols \u2013 list list of fed columns cov \u2013 numpy.ndarray covariance matrix var \u2013 numpy.ndarray variance matrix corr ( ndarray ) \u2013 numpy.ndarray correlation matrix Methods: Name Description __init__ numpy.ndarray,cols:list=None)->None: Initialize Pearson object corr computes Pearson correlation matrix heatmap int=6,digits:int=1, xrotation:Union[int,str]=45,yrotation:Union[int,str]='horizontal') -> None: plots correlation heatmap Example >>> import pandas as pd >>> from matplotlib import pyplot as plt >>> >>> # Create a sample dataset >>> data = pd.DataFrame({ ... 'feature1': [1, 2, 3, 4, 5], ... 'feature2': [5, 4, 3, 2, 1], ... 'feature3': [3, 3, 3, 3, 3] ... }) >>> >>> # Initialize the Pearson correlation analyzer >>> pearson_analyzer = Pearson(X=data.values,cols=data.columns) >>> # Compute the Pearson correlation matrix >>> correlation_matrix = pearson_analyzer.corr() >>> # Generate the heatmap >>> pearson_analyzer.heatmap() Source code in neural_net\\utils.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class Pearson : \"\"\" Computes the Pearson correlation matrix and generates a heatmap. Attributes: X : numpy.ndarray The input data containing features. n : int number of observations k: int number of features cols: list list of fed columns cov: numpy.ndarray covariance matrix var: numpy.ndarray variance matrix corr: numpy.ndarray correlation matrix Methods: __init__(X:numpy.ndarray,cols:list=None)->None: Initialize Pearson object corr()->numpy.ndarray: computes Pearson correlation matrix heatmap(ax=None,fontsize:int=6,digits:int=1, xrotation:Union[int,str]=45,yrotation:Union[int,str]='horizontal') -> None: plots correlation heatmap Example: ```python >>> import pandas as pd >>> from matplotlib import pyplot as plt >>> >>> # Create a sample dataset >>> data = pd.DataFrame({ ... 'feature1': [1, 2, 3, 4, 5], ... 'feature2': [5, 4, 3, 2, 1], ... 'feature3': [3, 3, 3, 3, 3] ... }) >>> >>> # Initialize the Pearson correlation analyzer >>> pearson_analyzer = Pearson(X=data.values,cols=data.columns) >>> # Compute the Pearson correlation matrix >>> correlation_matrix = pearson_analyzer.corr() >>> # Generate the heatmap >>> pearson_analyzer.heatmap() ``` \"\"\" def __init__ ( self , X : numpy . ndarray , cols : list = None ) -> None : \"\"\" Initialize Pearson object Args: X:numpy.ndarray array for which you'd like to get correlations from cols: list list of labels for columns \"\"\" self . X = X self . n , self . k = X . shape self . cols = cols or tuple ( range ( self . k )) def corr ( self ) -> numpy . ndarray : self . cov = ( v := ( self . X - self . X . mean ( axis = 0 ))) . T . dot ( v ) / self . n self . var = ( std := self . X . std ( axis = 0 )) . reshape ( - 1 , 1 ) * std self . corr = self . cov / self . var return self . corr def heatmap ( self , ax = None , fontsize : int = 6 , digits : int = 1 , xrotation : Union [ int , str ] = 45 , yrotation : Union [ int , str ] = 'horizontal' ): ax = ax or plt . subplots ()[ 1 ] im = ax . imshow ( self . corr ) im . set_clim ( - 1 , 1 ) ax . grid ( False ) ax . xaxis . set_ticks ( ticks = tuple ( range ( self . k )), labels = self . cols , rotation = xrotation ) ax . yaxis . set_ticks ( ticks = tuple ( range ( self . k )), labels = self . cols , rotation = yrotation ) for i in range ( self . k ): for j in range ( self . k ): ax . text ( j , i , self . corr . round ( digits )[ i , j ], ha = 'center' , va = 'center' , fontsize = fontsize , color = 'r' ) cbar = ax . figure . colorbar ( im , ax = ax , format = ' % .2f ' ) plt . show () __init__ ( X , cols = None ) Initialize Pearson object Parameters: X ( ndarray ) \u2013 numpy.ndarray array for which you'd like to get correlations from cols ( list , default: None ) \u2013 list list of labels for columns Source code in neural_net\\utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , X : numpy . ndarray , cols : list = None ) -> None : \"\"\" Initialize Pearson object Args: X:numpy.ndarray array for which you'd like to get correlations from cols: list list of labels for columns \"\"\" self . X = X self . n , self . k = X . shape self . cols = cols or tuple ( range ( self . k )) get_module_path ( dir ) Returns the path to a subdirectory named 'dir' relative to the currently executed script. Parameters: dir ( str ) \u2013 path to the subdirectory. Returns: str ( str ) \u2013 Absolute path to the specified subdirectory. Source code in neural_net\\utils.py 323 324 325 326 327 328 329 330 331 332 333 def get_module_path ( dir : list [ str ]) -> str : \"\"\" Returns the path to a subdirectory named 'dir' relative to the currently executed script. Args: dir (str): path to the subdirectory. Returns: str: Absolute path to the specified subdirectory. \"\"\" return os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), * dir ) make_circle_data ( centers , radii , p = 0.2 , n_grid = 100 , xmin =- 100 , xmax = 100 , ymin =- 100 , ymax = 100 ) Generates random data points distributed within circles. Parameters: centers \u2013 list of tuples List of (x, y) coordinates representing the centers of circles. radii \u2013 list of floats List of radii for each circle. p \u2013 float, optional (default=0.2) Percentage of randomly picked data points outside the circles. n_grid \u2013 int, optional (default=100) Meshgrid parameter for creating the grid of points. xmin \u2013 int, optional (default=-100) Minimum x limit for the data points. xmax \u2013 int, optional (default=100) Maximum x limit for the data points. ymin \u2013 int, optional (default=-100) Minimum y limit for the data points. ymax \u2013 int, optional (default=100) Maximum y limit for the data points. Returns: X ( tuple ) \u2013 numpy.ndarray, shape (n_samples, 2) 2D matrix of features (coordinates of data points). y ( tuple ) \u2013 numpy.ndarray, shape (n_samples,1) Labels corresponding to the data points (1 if inside a circle, 0 otherwise). Example >>> centers = [(0, 0), (20, 30)] >>> radii = [10, 15] >>> X, y = make_circle(centers, radii, p=0.1) >>> print(X.shape, y.shape) (200, 2) (200,1) Source code in neural_net\\utils.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def make_circle_data ( centers : list , radii : list , p : float = .2 , n_grid : int = 100 , xmin : int =- 100 , xmax : int = 100 , ymin : int =- 100 , ymax : int = 100 ) -> tuple : \"\"\" Generates random data points distributed within circles. Parameters: centers : list of tuples List of (x, y) coordinates representing the centers of circles. radii : list of floats List of radii for each circle. p : float, optional (default=0.2) Percentage of randomly picked data points outside the circles. n_grid : int, optional (default=100) Meshgrid parameter for creating the grid of points. xmin : int, optional (default=-100) Minimum x limit for the data points. xmax : int, optional (default=100) Maximum x limit for the data points. ymin : int, optional (default=-100) Minimum y limit for the data points. ymax : int, optional (default=100) Maximum y limit for the data points. Returns: X : numpy.ndarray, shape (n_samples, 2) 2D matrix of features (coordinates of data points). y : numpy.ndarray, shape (n_samples,1) Labels corresponding to the data points (1 if inside a circle, 0 otherwise). Example: ```python >>> centers = [(0, 0), (20, 30)] >>> radii = [10, 15] >>> X, y = make_circle(centers, radii, p=0.1) >>> print(X.shape, y.shape) (200, 2) (200,1) ``` \"\"\" x , y = numpy . linspace ( xmin , xmax , n_grid ), numpy . linspace ( ymin , ymax , n_grid ) xm , ym = numpy . meshgrid ( x , y ) x_news , y_news = [],[] labels = [] n_centers = len ( centers ) j = 0 for i in range ( n_centers ): c = ( xm - centers [ i ][ 0 ]) ** 2 + ( ym - centers [ i ][ 1 ]) ** 2 <= radii [ i ] ** 2 x_new , y_new = numpy . where ( c ) n = len ( x_new ) n_sample = int ( numpy . ceil ( n * p )) ix = numpy . random . randint ( 0 , n , n_sample ) x_news += [ x [ x_new ][ ix ]] y_news += [ y [ y_new ][ ix ]] labels += [ j ] * n_sample j += 1 x_news = numpy . concatenate ( x_news ) y_news = numpy . concatenate ( y_news ) labels = numpy . array ( labels ) . reshape ( - 1 , 1 ) return numpy . c_ [ x_news , y_news ], labels now () Returns the current timestamp as an integer. Returns: int ( int ) \u2013 Current timestamp (number of seconds since the epoch). Source code in neural_net\\utils.py 335 336 337 338 339 340 341 342 def now () -> int : \"\"\" Returns the current timestamp as an integer. Returns: int: Current timestamp (number of seconds since the epoch). \"\"\" return int ( datetime . datetime . now () . timestamp ()) unfold ( d ) Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Parameters: d ( dict ) \u2013 Input dictionary with nested dictionaries. Returns: dict ( dict ) \u2013 Unfolded dictionary with concatenated keys. Example: >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} Source code in neural_net\\utils.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def unfold ( d : dict ) -> dict : \"\"\" Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Args: d (dict): Input dictionary with nested dictionaries. Returns: dict: Unfolded dictionary with concatenated keys. Example: ```python >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} ``` \"\"\" new_d = {} for k in d : if hasattr ( d [ k ], 'keys' ): for j in d [ k ]: new_d [ f ' { k } _ { j } ' ] = d [ k ][ j ] else : new_d [ k ] = d [ k ] return new_d","title":"Reference"},{"location":"reference/#reference-page","text":"","title":"Reference Page"},{"location":"reference/#table-of-contents","text":"layers activation functions neural network architectures initialisation functions cost functions metrics database management data preparation functions Class Models used to build other classes Utility functions","title":"Table of Contents"},{"location":"reference/#introduction","text":"This reference page provides an overview of all functions, classes and methods available in the neural_net project","title":"Introduction"},{"location":"reference/#section-1-layers","text":"This module provides Layer classes Fullyconnected Activation","title":"Section 1. layers"},{"location":"reference/#neural_net.layers.Activation","text":"Bases: Layer Activation Layer. This layer handles activation for a given activation function Parameters: func ( callable ) \u2013 an activation function like :func: ~activation.\u03c3 Source code in neural_net\\layers.py 29 30 31 32 33 34 35 36 37 38 39 40 class Activation ( Layer ): \"\"\" Activation Layer. This layer handles activation for a given activation function Args: func (callable): an activation function like :func:`~activation.\u03c3` \"\"\" def __init__ ( self , func , * kargs ) -> None : self + locals ()","title":"Activation"},{"location":"reference/#neural_net.layers.Fullyconnected","text":"Bases: Layer A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Parameters: n_in ( int ) \u2013 Number of input features. n_out ( int ) \u2013 Number of output features . init_method ( callable ) \u2013 function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func ( callable , default: \u03a3 ) \u2013 default is :func: ~activation.\u03a3 Source code in neural_net\\layers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Fullyconnected ( Layer ): \"\"\" A fully connected neural network layer. This layer takes an input vector and transforms it linearly using a weights matrix. The product is then subjected to a non-linear activation function. Args: n_in (int): Number of input features. n_out (int): Number of output features . init_method (callable): function that initializes weights and takes in as parameters func(n_in,n_out) -> array.shape = (n_in +1, n_out) func (callable): default is :func:`~activation.\u03a3` \"\"\" def __init__ ( self , n_in : int , n_out : int , init_method : callable , func : callable = \u03a3 ) -> None : self + locals ()","title":"Fullyconnected"},{"location":"reference/#section-2-activation-functions","text":"This module provides classes for several types of activation functions \u03a3 - Linear combination of weights and biases \u03c3 - sigmoid activation Softmax - Softmax activation LeakyReLU - Leaky rectified linear unit activation","title":"Section 2. activation functions"},{"location":"reference/#neural_net.activation.ELU","text":"Bases: Neurons A class representing the Exponential Linear Unit (ELU) activation function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha (e^{z} - 1) & \\text{if } z < 0 \\end{cases} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the ELU activation for input matrix X. pr Computes the derivative of the ELU function. Parameters: \u03b1 ( float , default: 0.001 ) \u2013 The slope coefficient for negative values (default is 0.001). Source code in neural_net\\activation.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 class ELU ( Neurons ): r \"\"\" A class representing the Exponential Linear Unit (ELU) activation function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha (e^{z} - 1) & \\text{if } z < 0 \\end{cases} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the ELU activation for input matrix X. pr(): Computes the derivative of the ELU function. Args: \u03b1 (float): The slope coefficient for negative values (default is 0.001). \"\"\" def __init__ ( self , Layer : Layer = None , \u03b1 = 0.001 ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ELU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self . preds + neg * self [ '\u03b1' ] + ~ neg def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ELU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ELU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * ( numpy . exp ( self . X ) - 1 ) + ~ neg * self . X return self . preds","title":"ELU"},{"location":"reference/#neural_net.activation.ELU.compute","text":"Computes the ELU activation for input matrix X. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: ELU activation result of shape (n, n_out). Source code in neural_net\\activation.py 281 282 283 284 285 286 287 288 289 290 291 292 293 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ELU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ELU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * ( numpy . exp ( self . X ) - 1 ) + ~ neg * self . X return self . preds","title":"compute"},{"location":"reference/#neural_net.activation.ELU.pr","text":"Computes the derivative of the ELU function. \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ELU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self . preds + neg * self [ '\u03b1' ] + ~ neg","title":"pr"},{"location":"reference/#neural_net.activation.LeakyReLU","text":"Bases: Neurons A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha z & \\text{if } z < 0 \\end{cases} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the LeakyReLU activation for input matrix X. pr Computes the derivative of the LeakyReLU function. Parameters: \u03b1 ( float , default: 0.001 ) \u2013 The slope coefficient for negative values (default is 0.001). Source code in neural_net\\activation.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 class LeakyReLU ( Neurons ): r \"\"\" A class representing the Leaky Rectified Linear Unit (LeakyReLU) activation function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" \\alpha z & \\text{if } z < 0 \\end{cases} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the LeakyReLU activation for input matrix X. pr(): Computes the derivative of the LeakyReLU function. Args: \u03b1 (float): The slope coefficient for negative values (default is 0.001). \"\"\" def __init__ ( self , Layer : Layer = None , \u03b1 : float = .001 ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the LeakyReLU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ '\u03b1' ] + ~ neg def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * self . X + ~ neg * self . X return self . preds","title":"LeakyReLU"},{"location":"reference/#neural_net.activation.LeakyReLU.compute","text":"Computes the LeakyReLU activation for input matrix X. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: LeakyReLU activation result of shape (n, n_out). Source code in neural_net\\activation.py 390 391 392 393 394 395 396 397 398 399 400 401 402 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the LeakyReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: LeakyReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( neg := self . X < 0 ) * self [ '\u03b1' ] * self . X + ~ neg * self . X return self . preds","title":"compute"},{"location":"reference/#neural_net.activation.LeakyReLU.pr","text":"Computes the derivative of the LeakyReLU function. \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the LeakyReLU function. $$ \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( neg := self . X < 0 ) * self [ '\u03b1' ] + ~ neg","title":"pr"},{"location":"reference/#neural_net.activation.ReLU","text":"Bases: Neurons A class representing the Rectified Linear Unit (ReLU) activation function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the ReLU activation for input matrix X. pr Computes the derivative of the ReLU function. Source code in neural_net\\activation.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 class ReLU ( Neurons ): r \"\"\" A class representing the Rectified Linear Unit (ReLU) activation function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the ReLU activation for input matrix X. pr(): Computes the derivative of the ReLU function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ReLU function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( self . X >= 0 ) + 0 #for casting bool to int def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = numpy . maximum ( 0 , self . X ) return self . preds","title":"ReLU"},{"location":"reference/#neural_net.activation.ReLU.compute","text":"Computes the ReLU activation for input matrix X. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: ReLU activation result of shape (n, n_out). Source code in neural_net\\activation.py 335 336 337 338 339 340 341 342 343 344 345 346 347 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the ReLU activation for input matrix X. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: ReLU activation result of shape (n, n_out). \"\"\" self . X = X self . preds = numpy . maximum ( 0 , self . X ) return self . preds","title":"compute"},{"location":"reference/#neural_net.activation.ReLU.pr","text":"Computes the derivative of the ReLU function. \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the ReLU function. $$ \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return ( self . X >= 0 ) + 0 #for casting bool to int","title":"pr"},{"location":"reference/#neural_net.activation.Softmax","text":"Bases: Neurons A class representing the softmax activation function. \\sigma(\\mathbf {z_{i}} )=\\frac {e^{z_{i}}}{\\sum _{j=1}^{k}e^{z_{j}}}\\\\ {\\text{ for }}j=1,\\dotsc ,k{\\text{ features }}{\\text{ and }} z_{i}=(z_{i,1},...,z_{i,n}) \\text{ for n observations} Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the Softmax activation for input matrix X. pr Computes the derivative of the Softmax function. Source code in neural_net\\activation.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 class Softmax ( Neurons ): r \"\"\" A class representing the softmax activation function. $$ \\sigma(\\mathbf {z_{i}} )=\\frac {e^{z_{i}}}{\\sum _{j=1}^{k}e^{z_{j}}}\\\\ {\\text{ for }}j=1,\\dotsc ,k{\\text{ features }}{\\text{ and }} z_{i}=(z_{i,1},...,z_{i,n}) \\text{ for n observations} $$ Attributes: preds: predicted values. Methods: compute(X): Computes the Softmax activation for input matrix X. pr(): Computes the derivative of the Softmax function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the Softmax function. Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds ) def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the softmax activation for input matrix X using vectorization with numpy. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . preds","title":"Softmax"},{"location":"reference/#neural_net.activation.Softmax.compute","text":"Computes the softmax activation for input matrix X using vectorization with numpy. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Softmax activation result of shape (n, n_out). Source code in neural_net\\activation.py 223 224 225 226 227 228 229 230 231 232 233 234 235 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the softmax activation for input matrix X using vectorization with numpy. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Softmax activation result of shape (n, n_out). \"\"\" self . X = X self . preds = ( ex := numpy . exp ( self . X )) / ex . sum ( axis = 1 ) . reshape ( - 1 , 1 ) return self . preds","title":"compute"},{"location":"reference/#neural_net.activation.Softmax.pr","text":"Computes the derivative of the Softmax function. Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 214 215 216 217 218 219 220 221 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the Softmax function. Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds )","title":"pr"},{"location":"reference/#neural_net.activation.Tanh","text":"Bases: Neurons A class representing the Hyperbolic Tangent activation function. \\tanh(z)={\\frac{{\\rm{e}}^{z}-{\\rm{e}}^{-z}}{{\\rm {e}}^{z}+{\\rm {e}}^{-z}}}={\\frac{{\\rm {e}}^{2z}-1}{{\\rm{e}}^{2z}+1}}={\\frac{1-{\\rm{e}}^{-2z}}{1+{\\rm {e}}^{-2z}}} Tanh=2\\sigma(2z) - 1 Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the Tanh activation for input matrix X. pr Computes the derivative of the Tanh function. Source code in neural_net\\activation.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Tanh ( Neurons ): r \"\"\" A class representing the Hyperbolic Tangent activation function. $$ \\tanh(z)={\\frac{{\\rm{e}}^{z}-{\\rm{e}}^{-z}}{{\\rm {e}}^{z}+{\\rm {e}}^{-z}}}={\\frac{{\\rm {e}}^{2z}-1}{{\\rm{e}}^{2z}+1}}={\\frac{1-{\\rm{e}}^{-2z}}{1+{\\rm {e}}^{-2z}}} $$ $$ Tanh=2\\sigma(2z) - 1 $$ Attributes: preds: predicted values. Methods: compute(X): Computes the Tanh activation for input matrix X. pr(): Computes the derivative of the Tanh function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the Tanh function. $$ \\tanh '={\\frac {1}{\\cosh ^{2}}}=1-\\tanh ^{2} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return 1 - self . preds ** 2 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the Tanh activation for input matrix X. $$ Tanh(X)=2\\sigma(2X) - 1 $$ where $\\sigma$ is defined as follows: $$ \\sigma (X)={\\frac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Tanh activation result of shape (n, n_out). \"\"\" self . X = X \u03c3 = lambda z : 1 / ( 1 + numpy . exp ( - z )) self . preds = 2 * ( 2 * \u03c3 ( 2 * self . X ) - 1 ) return self . preds","title":"Tanh"},{"location":"reference/#neural_net.activation.Tanh.compute","text":"Computes the Tanh activation for input matrix X. Tanh(X)=2\\sigma(2X) - 1 where $\\sigma$ is defined as follows: \\sigma (X)={\\frac {1}{1+e^{-X}}} Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Tanh activation result of shape (n, n_out). Source code in neural_net\\activation.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the Tanh activation for input matrix X. $$ Tanh(X)=2\\sigma(2X) - 1 $$ where $\\sigma$ is defined as follows: $$ \\sigma (X)={\\frac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Tanh activation result of shape (n, n_out). \"\"\" self . X = X \u03c3 = lambda z : 1 / ( 1 + numpy . exp ( - z )) self . preds = 2 * ( 2 * \u03c3 ( 2 * self . X ) - 1 ) return self . preds","title":"compute"},{"location":"reference/#neural_net.activation.Tanh.pr","text":"Computes the derivative of the Tanh function. \\tanh '={\\frac {1}{\\cosh ^{2}}}=1-\\tanh ^{2} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 104 105 106 107 108 109 110 111 112 113 114 115 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the Tanh function. $$ \\tanh '={\\frac {1}{\\cosh ^{2}}}=1-\\tanh ^{2} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return 1 - self . preds ** 2","title":"pr"},{"location":"reference/#neural_net.activation.\u03a3","text":"Bases: Neurons A class representing a linear combination operation. \\mathrm{\\mathit{H}}(z) = z.w + b where w is weights vector and b is bias Attributes: W ( ndarray ) \u2013 Weight matrix of shape (k+1, n_out). +1 for bias Methods: Name Description compute Computes the linear combination of input matrix X and bias vector using weight matrix W. pr Computes the derivative of the linear equation with respect to W (matrix X itself). grad Updates weights self.W and computes the new gradient \u0394 for backpropagation. Xb Concatenates X matrix with a vector of ones Source code in neural_net\\activation.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class \u03a3 ( Neurons ): r \"\"\" A class representing a linear combination operation. $$ \\mathrm{\\mathit{H}}(z) = z.w + b $$ where w is weights vector and b is bias Attributes: W (numpy.ndarray): Weight matrix of shape (k+1, n_out). +1 for bias Methods: compute(X): Computes the linear combination of input matrix X and bias vector using weight matrix W. pr(): Computes the derivative of the linear equation with respect to W (matrix X itself). grad(\u0394): Updates weights self.W and computes the new gradient \u0394 for backpropagation. Xb(): Concatenates X matrix with a vector of ones \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () self . W = self . init_method ( self [ 'Layer_n_in' ], self [ 'Layer_n_out' ]) self . Xb = lambda : numpy . c_ [ self . X , numpy . ones (( self . n (), 1 ))] self . instantiateW () self . storeW () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . Xb () def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W ) def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.ndarray): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394","title":"\u03a3"},{"location":"reference/#neural_net.activation.\u03a3.compute","text":"Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Linear combination result of shape (n, n_out). Source code in neural_net\\activation.py 55 56 57 58 59 60 61 62 63 64 65 66 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes the linear combination of input matrix X and bias vector using weight matrix self.W. Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Linear combination result of shape (n, n_out). \"\"\" self . X = X return self . Xb () . dot ( self . W )","title":"compute"},{"location":"reference/#neural_net.activation.\u03a3.grad","text":"Updates weights self.W and computes the gradient for backpropagation. Parameters: \u0394 ( ndarray ) \u2013 Gradient from next activation. Source code in neural_net\\activation.py 68 69 70 71 72 73 74 75 76 77 def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Updates weights self.W and computes the gradient for backpropagation. Args: \u0394 (numpy.ndarray): Gradient from next activation. \"\"\" self - ( self . pr () . T . dot ( \u0394 )) / self . n () self . \u0394 = \u0394 . dot ( self . W [: - 1 ,:] . T ) #-1 to remove biais return self . \u0394","title":"grad"},{"location":"reference/#neural_net.activation.\u03a3.pr","text":"Computes the derivative of the linear equation (matrix itself). Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 46 47 48 49 50 51 52 53 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative of the linear equation (matrix itself). Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . Xb ()","title":"pr"},{"location":"reference/#neural_net.activation.\u03c3","text":"Bases: Neurons A class representing the sigmoid activation function. \\sigma(z)={\\frac{1}{1+e^{-z}}}={\\frac{e^{z}}{1+e^{z}}}=1-\\sigma(-z) Attributes: preds \u2013 predicted values. Methods: Name Description compute Computes the sigmoid activation for input matrix X. pr Computes the derivative of the sigmoid function. Source code in neural_net\\activation.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class \u03c3 ( Neurons ): r \"\"\" A class representing the sigmoid activation function. $$ \\sigma(z)={\\frac{1}{1+e^{-z}}}={\\frac{e^{z}}{1+e^{z}}}=1-\\sigma(-z) $$ Attributes: preds: predicted values. Methods: compute(X): Computes the sigmoid activation for input matrix X. pr(): Computes the derivative of the sigmoid function. \"\"\" def __init__ ( self , Layer : Layer = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the sigmoid function. $$ {\\begin{aligned}\\sigma'(z)&={\\frac {e^{z}\\cdot (1+e^{z})-e^{z}\\cdot e^{z}}{(1+e^{z})^{2}}}\\\\&={\\frac {e^{z}}{(1+e^{z})^{2}}}\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left({\\frac {1}{1+e^{z}}}\\right)\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left(1-{\\frac {e^{z}}{1+e^{z}}}\\right)\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\\end{aligned}} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds ) def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the sigmoid activation for input matrix X using vectorization with numpy. $$ \\sigma (X)={\\dfrac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . preds = 1 / ( 1 + numpy . exp ( - self . X )) return self . preds","title":"\u03c3"},{"location":"reference/#neural_net.activation.\u03c3.compute","text":"Computes the sigmoid activation for input matrix X using vectorization with numpy. \\sigma (X)={\\dfrac {1}{1+e^{-X}}} Parameters: X ( ndarray ) \u2013 Input matrix of shape (n, k). Returns: ndarray \u2013 numpy.ndarray: Sigmoid activation result of shape (n, n_out). Source code in neural_net\\activation.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def compute ( self , X : numpy . ndarray ) -> numpy . ndarray : r \"\"\" Computes the sigmoid activation for input matrix X using vectorization with numpy. $$ \\sigma (X)={\\dfrac {1}{1+e^{-X}}} $$ Args: X (numpy.ndarray): Input matrix of shape (n, k). Returns: numpy.ndarray: Sigmoid activation result of shape (n, n_out). \"\"\" self . X = X self . preds = 1 / ( 1 + numpy . exp ( - self . X )) return self . preds","title":"compute"},{"location":"reference/#neural_net.activation.\u03c3.pr","text":"Computes the derivative of the sigmoid function. {\\begin{aligned}\\sigma'(z)&={\\frac {e^{z}\\cdot (1+e^{z})-e^{z}\\cdot e^{z}}{(1+e^{z})^{2}}}\\\\&={\\frac {e^{z}}{(1+e^{z})^{2}}}\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left({\\frac {1}{1+e^{z}}}\\right)\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left(1-{\\frac {e^{z}}{1+e^{z}}}\\right)\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\\end{aligned}} Returns: ndarray \u2013 numpy.ndarray: Derivative matrix. Source code in neural_net\\activation.py 162 163 164 165 166 167 168 169 170 171 172 173 def pr ( self ) -> numpy . ndarray : r \"\"\" Computes the derivative of the sigmoid function. $$ {\\begin{aligned}\\sigma'(z)&={\\frac {e^{z}\\cdot (1+e^{z})-e^{z}\\cdot e^{z}}{(1+e^{z})^{2}}}\\\\&={\\frac {e^{z}}{(1+e^{z})^{2}}}\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left({\\frac {1}{1+e^{z}}}\\right)\\\\&=\\left({\\frac {e^{z}}{1+e^{z}}}\\right)\\left(1-{\\frac {e^{z}}{1+e^{z}}}\\right)\\\\&=\\sigma(z)\\left(1-\\sigma(z)\\right)\\end{aligned}} $$ Returns: numpy.ndarray: Derivative matrix. \"\"\" return self . preds * ( 1 - self . preds )","title":"pr"},{"location":"reference/#section-3-neural-network-architectures","text":"This module provides neural network architectures Currently available are Sequential - Sequential linear net architecture","title":"Section 3. neural network architectures"},{"location":"reference/#neural_net.architecture.Sequential","text":"Bases: Architecture Source code in neural_net\\architecture.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class Sequential ( Architecture ): def __init__ ( self , steps : list [ Layer ], cost : Cost , store : bool = False ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. store (bool): If True disables identification and storage Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" Define . _Define__store = store self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit () def train ( self , X : numpy . ndarray = None , y : numpy . ndarray = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.ndarray): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.ndarray): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm ( range ( epochs ), ascii = ' =' ) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit ()","title":"Sequential"},{"location":"reference/#neural_net.architecture.Sequential.__init__","text":"Initialize a Sequential class. Parameters: steps ( List [ Layer ] ) \u2013 A list of Layer objects representing the steps. cost ( Cost ) \u2013 A Cost object for computing cost information. store ( bool , default: False ) \u2013 If True disables identification and storage Example: layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) Source code in neural_net\\architecture.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , steps : list [ Layer ], cost : Cost , store : bool = False ) -> None : \"\"\" Initialize a Sequential class. Args: steps (List[Layer]): A list of Layer objects representing the steps. cost (Cost): A Cost object for computing cost information. store (bool): If True disables identification and storage Example: ```python layer1 = Fullyconnected(2,50,init_funcs.zeros) layer2 = Activation(activation.LeakyReLU) my_cost = binaryCrossEntropy my_instance = Sequential(steps=[layer1, layer2], cost=my_cost) ``` \"\"\" Define . _Define__store = store self + locals () self [ 'cost' ] = self [ 'cost' ]( self [ 'id' ]) self . commit ()","title":"__init__"},{"location":"reference/#neural_net.architecture.Sequential.train","text":"Trains a neural network model using sequential architecture Parameters: X ( ndarray , default: None ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( ndarray , default: None ) \u2013 Target variable with shape (n, 1). batch ( Optional [ Batch ] , default: None ) \u2013 Optional Batch object that generates batches from the training data. epochs ( int , default: 100 ) \u2013 Maximum number of training epochs. \u03b1 ( float , default: 0.001 ) \u2013 Learning rate (step size for weight updates). metrics ( Metrics , default: Empty ) \u2013 Metrics object that computes evaluation metrics (e.g., accuracy). Example: from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) Source code in neural_net\\architecture.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def train ( self , X : numpy . ndarray = None , y : numpy . ndarray = None , batch : Batch = None , epochs : int = 100 , \u03b1 : float = 0.001 , metrics : Metrics = Empty ) -> None : \"\"\" Trains a neural network model using sequential architecture Args: X (numpy.ndarray): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.ndarray): Target variable with shape (n, 1). batch (Optional[Batch]): Optional Batch object that generates batches from the training data. epochs (int): Maximum number of training epochs. \u03b1 (float): Learning rate (step size for weight updates). metrics (Metrics): Metrics object that computes evaluation metrics (e.g., accuracy). Example: ```python from neural_net import * # generate your training data >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> NN = architecture.Sequential( [ layers.Fullyconnected(2,50,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), layers.Fullyconnected(50,1,init_funcs.XHsigmoiduniform) , layers.Activation(activation.\u03c3), ], cost = cost.binaryCrossEntropy ) >>> NN.train(X_train, y_train,metrics=metrics.accuracy)) ``` \"\"\" Xys = batch or [( X , y )] epochs = tqdm ( range ( epochs ), ascii = ' =' ) m = metrics () for _ in epochs : for X , y in Xys : self . out = self . predict ( X ) self [ 'cost' ] . compute ( y , self . out ) self . update ( \u03b1 * self [ 'cost' ] . pr ()) epochs . set_description ( ' ' . join ( map ( repr ,[ self [ 'cost' ], self [ 'cost' ] . compute_store () . round ( 4 ), m , m . compute ( y , self . out )]))) self . updateW () self . commit ()","title":"train"},{"location":"reference/#section-4-initialisation-functions","text":"This module provides initialization functions zeros(n_in: int, n_out: int) - Initializes a weight matrix with zeros XHsigmoiduniform - AA function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. XHReluuniform - A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions.","title":"Section 4. initialisation functions"},{"location":"reference/#neural_net.init_funcs.XavierHe","text":"This class implements Xavier Glorot and He initializations.(source Hands on ML) Attributes: random ( dict ) \u2013 contains generators of random values by distribution. activation ( str ) \u2013 Name of activation. Parameters: distribution ( str [ Uniform , Normal ] ) \u2013 Name of distribution. activation ( str [ Sigmoid , Tanh , ReLU ] ) \u2013 Name of activation. Attributes: init_func \u2013 func(n_in,n_out,biais=True) for generating random values Source code in neural_net\\init_funcs.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class XavierHe : \"\"\" This class implements Xavier Glorot and He initializations.(source Hands on ML) ![png](static/xahe.png) Attributes: random (dict): contains generators of random values by distribution. activation (str): Name of activation. Args: distribution (str[\"Uniform\",\"Normal\"]): Name of distribution. activation (str[\"Sigmoid\",\"Tanh\",\"ReLU\"]): Name of activation. Attributes: init_func : func(n_in,n_out,biais=True) for generating random values \"\"\" random = { \"Uniform\" : lambda r , n_in , n_out , biais : numpy . random . uniform ( - r , r , size = ( n_in + biais , n_out )), \"Normal\" : lambda \u03c3 , n_in , n_out , biais : numpy . random . normal ( scale = \u03c3 , size = ( n_in + biais , n_out )) } _weight = { \"Sigmoid\" : 1 , \"Tanh\" : 4 , \"ReLU\" : 2 ** .5 } default_values = { \"Uniform\" : lambda n_in , n_out : ( 6 / ( n_in + n_out )) ** .5 , \"Normal\" : lambda n_in , n_out : ( 2 / ( n_in + n_out )) ** .5 , } @property def weight ( self ): return XavierHe . _weight . get ( self . activation ) @property def param ( self ): return XavierHe . default_values . get ( self . distribution ) @property def init_func ( self ): gen = XavierHe . random . get ( self . distribution ) eq = lambda n_in , n_out , biais = True : gen ( self . weight * self . param ( n_in , n_out ), n_in , n_out , biais ) return eq def __init__ ( self , distribution : Literal [ \"Uniform\" , \"Normal\" ], activation : Literal [ \"Sigmoid\" , \"Tanh\" , \"ReLU\" ]) -> None : self . activation = activation self . distribution = distribution","title":"XavierHe"},{"location":"reference/#neural_net.init_funcs.XHReluuniform","text":"A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). biais ( bool , default: True ) \u2013 if True adds biais weights Returns: ndarray \u2013 numpy.ndarray : array of random values Source code in neural_net\\init_funcs.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def XHReluuniform ( n_in : int , n_out : int , biais : bool = True ) -> numpy . ndarray : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for Rectified linear unit(RELU) activation functions. Args: n_in (int): Number of input units. n_out (int): Number of output units (neurons). biais (bool): if True adds biais weights returns: numpy.ndarray : array of random values \"\"\" r = 2 ** .5 * ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + biais , n_out ))","title":"XHReluuniform"},{"location":"reference/#neural_net.init_funcs.XHsigmoiduniform","text":"A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units (neurons). biais ( bool ) \u2013 if True adds biais weights Returns: ndarray \u2013 numpy.ndarray : array of random values Source code in neural_net\\init_funcs.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def XHsigmoiduniform ( n_in : int , n_out : int , biais : bool = True ) -> numpy . ndarray : \"\"\" A function representing weight initialization using Xavier (Glorot) initialization for sigmoid activation functions. Attributes: n_in (int): Number of input units. n_out (int): Number of output units (neurons). biais (bool): if True adds biais weights returns: numpy.ndarray : array of random values \"\"\" r = ( 6 / ( n_in + n_out )) ** .5 return numpy . random . uniform ( low =- r , high = r , size = ( n_in + biais , n_out ))","title":"XHsigmoiduniform"},{"location":"reference/#neural_net.init_funcs.zeros","text":"Initializes a weight matrix with zeros. Parameters: n_in ( int ) \u2013 Number of input units. n_out ( int ) \u2013 Number of output units. biais ( bool , default: True ) \u2013 if True adds biais weights Returns: ndarray \u2013 numpy.ndarray: Weight matrix of shape (n_in + 1, n_out). Source code in neural_net\\init_funcs.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def zeros ( n_in : int , n_out : int , biais : bool = True ) -> numpy . ndarray : \"\"\" Initializes a weight matrix with zeros. Args: n_in (int): Number of input units. n_out (int): Number of output units. biais (bool): if True adds biais weights Returns: numpy.ndarray: Weight matrix of shape (n_in + 1, n_out). \"\"\" return numpy . zeros (( n_in + biais , n_out ))","title":"zeros"},{"location":"reference/#section-5-cost-functions","text":"This module provides classes for several types of cost functions binaryCrossEntropy CrossEntropy MSE","title":"Section 5. cost functions"},{"location":"reference/#neural_net.cost.BinaryCrossEntropy","text":"Bases: Cost Binary Cross-Entropy Loss. \\mathrm{\\mathit{Binary\\ Cross\\ Entropy}}(p, y) = \\begin{cases} -\\log(p) & \\text{if } y = 1, \\\\ -\\log(1-p) & \\text{otherwise.} \\end{cases} This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the binary cross-entropy loss. - pr numpy.ndarray, p: numpy.ndarray) -> numpy.ndarray: Computes the derivative function values. Example >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] Source code in neural_net\\cost.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class BinaryCrossEntropy ( Cost ): r \"\"\" Binary Cross-Entropy Loss. $$ \\mathrm{\\mathit{Binary\\ Cross\\ Entropy}}(p, y) = \\begin{cases} -\\log(p) & \\text{if } y = 1, \\\\ -\\log(1-p) & \\text{otherwise.} \\end{cases} $$ This class computes the binary cross-entropy loss between true labels (y) and predicted probabilities (p). Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the binary cross-entropy loss. - pr(y: numpy.ndarray, p: numpy.ndarray) -> numpy.ndarray: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[0], [1], [1], [0]]) >>> predicted_probs = numpy.array([[0.2], [0.8], [0.6], [0.3]]) >>> bce_loss = binaryCrossEntropy() >>> loss_value = bce_loss.compute(y_true, predicted_probs) >>> print(f\"Binary Cross-Entropy Loss: {loss_value:.4f}\") Binary Cross-Entropy Loss: 0.3284 >>> derivative_values = bce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [ 1.25 -1.25 -1.66666667 1.42857143] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.ndarray: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p )) def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean ()","title":"BinaryCrossEntropy"},{"location":"reference/#neural_net.cost.BinaryCrossEntropy.compute","text":"Computes the binary cross-entropy loss. Parameters: y ( ndarray ) \u2013 True labels (0 or 1). p ( ndarray ) \u2013 Predicted probabilities (between 0 and 1). clip ( bool , default: True ) \u2013 Whether or not to clip predicted values see method clip Returns: float ( float ) \u2013 Binary cross-entropy loss value. Source code in neural_net\\cost.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the binary cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip Returns: float: Binary cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return - ( self . y * numpy . log ( self . p ) + ( 1 - self . y ) * numpy . log ( 1 - self . p )) . mean ()","title":"compute"},{"location":"reference/#neural_net.cost.BinaryCrossEntropy.pr","text":"Computes the derivative function values with respet to p. Returns: ndarray \u2013 numpy.ndarray: Derivative function values. Source code in neural_net\\cost.py 46 47 48 49 50 51 52 53 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p. Returns: numpy.ndarray: Derivative function values. \"\"\" return - ( self . y / self . p - ( 1 - self . y ) / ( 1 - self . p ))","title":"pr"},{"location":"reference/#neural_net.cost.CrossEntropy","text":"Bases: Cost Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). Cross\\ Entropy(p,y) = -\\sum _{i}\\sum _{j}y_{ij}\\log p_{ij}\\ Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the cross-entropy loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) Source code in neural_net\\cost.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class CrossEntropy ( Cost ): r \"\"\" Cross-Entropy Loss. This class computes the cross-entropy loss between true labels (y) and predicted probabilities (p). $$ Cross\\ Entropy(p,y) = -\\sum _{i}\\sum _{j}y_{ij}\\log p_{ij}\\ $$ Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the cross-entropy loss. - pr() -> numpy.ndarray: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[1, 0, 0], ... [0, 1, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0]]) >>> predicted_probs = numpy.array([[0, 0.6, 0.3], ... [0.4, 0.2, 0.4], ... [0.2, 0.3, 0.5], ... [0.5, 0.1, 0.4], ... [0.3, 0.4, 0.3]]) >>> ce_loss = CrossEntropy() >>> loss_value = ce_loss.compute(y_true, predicted_probs) >>> print(f\"Cross-Entropy Loss: {loss_value:.4f}\") Cross-Entropy Loss: 1.7915 >>> derivative_values = ce_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: array([[-1.00000000e+07, 2.50000000e+00, 1.42857143e+00], [ 1.66666667e+00, -5.00000000e+00, 1.66666667e+00], [ 1.25000000e+00, 1.42857143e+00, -2.00000000e+00], [ 2.00000000e+00, -1.00000000e+01, 1.66666667e+00], [-3.33333333e+00, 1.66666667e+00, 1.42857143e+00]]) ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" left = ( self . y / self . p ) #right = left.sum(axis=1,keepdims=True)*(f:=((1-self.y)/(1-self.p)))/f.sum(axis=1,keepdims=True) right = ( 1 - self . y ) / ( 1 - self . p ) return - ( left - right ) def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip. Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return ( - self . y * numpy . log ( self . p )) . sum ( axis = 1 ) . mean ()","title":"CrossEntropy"},{"location":"reference/#neural_net.cost.CrossEntropy.compute","text":"Computes the Cross-entropy loss. Parameters: y ( ndarray ) \u2013 True labels (0 or 1). p ( ndarray ) \u2013 Predicted probabilities (between 0 and 1). clip ( bool , default: True ) \u2013 Whether or not to clip predicted values see method clip. Returns: float ( float ) \u2013 Cross-entropy loss value. Source code in neural_net\\cost.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def compute ( self , y : numpy . ndarray , p : numpy . ndarray , clip : bool = True ) -> float : \"\"\" Computes the Cross-entropy loss. Args: y (numpy.ndarray): True labels (0 or 1). p (numpy.ndarray): Predicted probabilities (between 0 and 1). clip (bool): Whether or not to clip predicted values see method clip. Returns: float: Cross-entropy loss value. \"\"\" self . y , self . p = y , p if clip : self . clip () return ( - self . y * numpy . log ( self . p )) . sum ( axis = 1 ) . mean ()","title":"compute"},{"location":"reference/#neural_net.cost.CrossEntropy.pr","text":"Computes the derivative function values with respet to p . Returns: ndarray \u2013 numpy.ndarray: Derivative function values. Source code in neural_net\\cost.py 113 114 115 116 117 118 119 120 121 122 123 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" left = ( self . y / self . p ) #right = left.sum(axis=1,keepdims=True)*(f:=((1-self.y)/(1-self.p)))/f.sum(axis=1,keepdims=True) right = ( 1 - self . y ) / ( 1 - self . p ) return - ( left - right )","title":"pr"},{"location":"reference/#neural_net.cost.MSE","text":"Bases: Cost Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). \\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left(y_{i}-{\\hat {y_{i}}}\\right)^{2} Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean squared error loss. - pr Computes the derivative function values. Example >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] Source code in neural_net\\cost.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class MSE ( Cost ): r \"\"\" Mean Squared Error (MSE) Loss. This class computes the mean squared error loss between true labels (y) and predicted values (p). $$ \\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left(y_{i}-{\\hat {y_{i}}}\\right)^{2} $$ Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean squared error loss. - pr() -> numpy.ndarray: Computes the derivative function values. Example: ```python >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mse_loss = MSE() >>> loss_value = mse_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" return - 2 * ( self . y - self . p ) def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean ()","title":"MSE"},{"location":"reference/#neural_net.cost.MSE.compute","text":"Computes the mean squared error loss. Parameters: y ( ndarray ) \u2013 True labels (ground truth). p ( ndarray ) \u2013 Predicted values. Returns: float ( float ) \u2013 Mean squared error loss value. Source code in neural_net\\cost.py 184 185 186 187 188 189 190 191 192 193 194 195 196 def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean squared error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean squared error loss value. \"\"\" self . y , self . p = y , p return (( self . y - self . p ) ** 2 ) . mean ()","title":"compute"},{"location":"reference/#neural_net.cost.MSE.pr","text":"Computes the derivative function values with respet to p . Returns: ndarray \u2013 numpy.ndarray: Derivative function values. Source code in neural_net\\cost.py 176 177 178 179 180 181 182 183 def pr ( self ) -> numpy . ndarray : \"\"\" Computes the derivative function values with respet to p . Returns: numpy.ndarray: Derivative function values. \"\"\" return - 2 * ( self . y - self . p )","title":"pr"},{"location":"reference/#section-6-metrics","text":"This module provides metrics classes accuracy MAE","title":"Section 6. metrics"},{"location":"reference/#neural_net.metrics.MAE","text":"Bases: Metrics Mean Absolute Error (MAE) . This class computes the mean absolute error loss between true labels (y) and predicted values (p). \\displaystyle \\operatorname {MAE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left|y_{i}-{\\hat {y_{i}}}\\right| Methods: Name Description - compute numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean absolute error. Example >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mae_loss = MAE() >>> loss_value = mae_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] Source code in neural_net\\metrics.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class MAE ( Metrics ): r \"\"\" Mean Absolute Error (MAE) . This class computes the mean absolute error loss between true labels (y) and predicted values (p). $$ \\displaystyle \\operatorname {MAE} ={\\frac {1}{n}}\\sum _{i=1}^{n}\\left|y_{i}-{\\hat {y_{i}}}\\right| $$ Methods: - compute(y: numpy.ndarray, p: numpy.ndarray) -> float: Computes the mean absolute error. Example: ```python >>> y_true = numpy.array([[2.0], [3.5], [5.0], [4.2]]) >>> predicted_values = numpy.array([[1.8], [3.2], [4.8], [4.0]]) >>> mae_loss = MAE() >>> loss_value = mae_loss.compute(y_true, predicted_values) >>> print(f\"Mean Squared Error Loss: {loss_value:.4f}\") Mean Squared Error Loss: 0.0525 >>> derivative_values = mse_loss.pr() >>> print(f\"Derivative Function Values: {derivative_values}\") Derivative Function Values: [[-0.4] [-0.6] [-0.4] [-0.4]] ``` \"\"\" def __init__ ( self , Architecture_id = None ) -> None : self + locals () def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean absolute error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean absolute error value. \"\"\" self . y , self . p = y , p return numpy . abs ( self . y - self . p ) . mean ()","title":"MAE"},{"location":"reference/#neural_net.metrics.MAE.compute","text":"Computes the mean absolute error loss. Parameters: y ( ndarray ) \u2013 True labels (ground truth). p ( ndarray ) \u2013 Predicted values. Returns: float ( float ) \u2013 Mean absolute error value. Source code in neural_net\\metrics.py 118 119 120 121 122 123 124 125 126 127 128 129 130 def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the mean absolute error loss. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: Mean absolute error value. \"\"\" self . y , self . p = y , p return numpy . abs ( self . y - self . p ) . mean ()","title":"compute"},{"location":"reference/#neural_net.metrics.accuracy","text":"Bases: Metrics Calculates the accuracy metric for binary or multiclass classification tasks. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Attributes: threshold ( float ) \u2013 The threshold value used for binary classification. Methods: Name Description compute Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 Source code in neural_net\\metrics.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class accuracy ( Metrics ): \"\"\" Calculates the accuracy metric for binary or multiclass classification tasks. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. Attributes: threshold (float): The threshold value used for binary classification. Methods: compute(y, p): Computes the accuracy score based on true labels (y) and predicted probabilities (p). Example: ```python >>> acc = accuracy(threshold=0.6) >>> y_true = numpy.array([[1], [0], [1], [0]]) >>> y_pred = numpy.array([[0.8], [0.3], [0.9], [0.5]]) >>> val = acc.compute(y_true, y_pred) >>> print(f\"Accuracy: {val:.4f}\") Accuracy: 1.0000 >>> y_true_multiclass = numpy.array([[0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 0, 1], ... [0, 1, 0], ... [1, 0, 0], ... [0, 1, 0], ... [0, 0, 1]]) >>> y_pred_multiclass = numpy.array([ ... [0.1, 0.2, 0.7], # Predicted probabilities for class 0 ... [0.6, 0.3, 0.1], # Predicted probabilities for class 1 ... [0.8, 0.1, 0.1], # Predicted probabilities for class 2 ... [0.2, 0.3, 0.5], ... [0.4, 0.4, 0.2], ... [0.7, 0.2, 0.1], ... [0.3, 0.4, 0.3], ... [0.1, 0.2, 0.7] ... ]) >>> model_multiclass = accuracy(threshold=0.5) >>> acc_multiclass = model_multiclass.compute(y_true_multiclass, y_pred_multiclass) >>> print(f\"Accuracy (multiclass): {acc_multiclass:.4f}\") Accuracy (multiclass): 0.7500 ``` \"\"\" def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 )","title":"accuracy"},{"location":"reference/#neural_net.metrics.accuracy.__init__","text":"Initializes the accuracy metric. Parameters: threshold ( float , default: 0.5 ) \u2013 Threshold value for binary classification. Defaults to 0.5. Source code in neural_net\\metrics.py 59 60 61 62 63 64 65 66 def __init__ ( self , threshold = .5 ) -> None : \"\"\" Initializes the accuracy metric. Args: threshold (float, optional): Threshold value for binary classification. Defaults to 0.5. \"\"\" self . threshold = threshold","title":"__init__"},{"location":"reference/#neural_net.metrics.accuracy.compute","text":"Computes the accuracy of predictions. Parameters: y ( ndarray ) \u2013 True labels (ground truth). p ( ndarray ) \u2013 Predicted values. Returns: float ( float ) \u2013 accuracy value. Source code in neural_net\\metrics.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def compute ( self , y : numpy . ndarray , p : numpy . ndarray ) -> float : \"\"\" Computes the accuracy of predictions. Args: y (numpy.ndarray): True labels (ground truth). p (numpy.ndarray): Predicted values. Returns: float: accuracy value. \"\"\" if y . shape [ 1 ] > 1 : p = p . argmax ( axis = 1 ) y = y . argmax ( axis = 1 ) else : p = ( p > self . threshold ) + 0 self . y , self . p = y , p return (( self . y == self . p ) . sum () / len ( self . y )) . round ( 4 )","title":"compute"},{"location":"reference/#section-7-database-management","text":"This module provides sqlalchemy orm tables and utility objects DefaultTable - generic table template Architecture - Architecture table Layer - layer table Neurons - neurons table Cost - cost table Weight - weight table","title":"Section 7. database management"},{"location":"reference/#neural_net.db.DBmanager","text":"Manages database connections and sessions using SQLAlchemy. Attributes: session ( Session ) \u2013 SQLAlchemy session for database operations. Methods: Name Description __start str=None)-> None: Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. Source code in neural_net\\db.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class DBmanager : \"\"\" Manages database connections and sessions using SQLAlchemy. Attributes: session (Session): SQLAlchemy session for database operations. Methods: __start(db : str=None)-> None: Starts a session add_table(table: DefaultTable) -> None: Adds a table instance to the current session. commit() -> None: Commits changes to the session. \"\"\" engines = {} status = False def __start ( db : str = None ) -> None : \"\"\" Starts a session Args: db (str, optional): Path to database server or SQLite database file. Defaults to None. \"\"\" db_path = db or f 'sqlite:/// { get_module_path ([ \"run\" , f \"model { now () } .db\" ]) } ' DBmanager . db_path = db_path DBmanager . engines [ DBmanager . db_path ] = create_engine ( DBmanager . db_path ) Base . metadata . create_all ( DBmanager . engines [ DBmanager . db_path ]) Session = sessionmaker ( bind = DBmanager . engines [ DBmanager . db_path ]) DBmanager . session = Session () def add_table ( self , table : DefaultTable ) -> None : \"\"\" Adds a table instance to the current session. Args: table (DefaultTable): An instance of table object \"\"\" if not DBmanager . status : DBmanager . _DBmanager__start () DBmanager . status = True DBmanager . session . add ( table )","title":"DBmanager"},{"location":"reference/#neural_net.db.DBmanager.__start","text":"Starts a session Parameters: db ( str , default: None ) \u2013 Path to database server or SQLite database file. Defaults to None. Source code in neural_net\\db.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __start ( db : str = None ) -> None : \"\"\" Starts a session Args: db (str, optional): Path to database server or SQLite database file. Defaults to None. \"\"\" db_path = db or f 'sqlite:/// { get_module_path ([ \"run\" , f \"model { now () } .db\" ]) } ' DBmanager . db_path = db_path DBmanager . engines [ DBmanager . db_path ] = create_engine ( DBmanager . db_path ) Base . metadata . create_all ( DBmanager . engines [ DBmanager . db_path ]) Session = sessionmaker ( bind = DBmanager . engines [ DBmanager . db_path ]) DBmanager . session = Session ()","title":"__start"},{"location":"reference/#neural_net.db.DBmanager.add_table","text":"Adds a table instance to the current session. Parameters: table ( DefaultTable ) \u2013 An instance of table object Source code in neural_net\\db.py 124 125 126 127 128 129 130 131 132 133 134 135 def add_table ( self , table : DefaultTable ) -> None : \"\"\" Adds a table instance to the current session. Args: table (DefaultTable): An instance of table object \"\"\" if not DBmanager . status : DBmanager . _DBmanager__start () DBmanager . status = True DBmanager . session . add ( table )","title":"add_table"},{"location":"reference/#neural_net.db.get_instance","text":"Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table ( DefaultTable ) \u2013 SQLAlchemy Table object corresponding to the specified table name. Source code in neural_net\\db.py 70 71 72 73 74 75 76 77 78 79 def get_instance ( self ) -> DefaultTable : \"\"\" Returns an SQLAlchemy Table object corresponding to the given table name. Returns: Table: SQLAlchemy Table object corresponding to the specified table name. \"\"\" table , cols = tables [ str ( self )] values = { k : v for k , v in self . id . items () if k in cols } return table ( ** values )","title":"get_instance"},{"location":"reference/#neural_net.db.update_instance","text":"Updates the given instance with the provided keyword arguments. Source code in neural_net\\db.py 81 82 83 84 85 86 87 88 89 def update_instance ( self ) -> None : \"\"\" Updates the given instance with the provided keyword arguments. \"\"\" _ , cols = tables [ str ( self )] for k , v in self . id . items (): if k in cols : setattr ( self . table , k , v )","title":"update_instance"},{"location":"reference/#section-8-data-preparation-functions","text":"This module provides functions for data preparation Batch - feed data in chunks shuffle - shuffles train sets onehot - onehot encodes target variables scaler - scales input features","title":"Section 8. data preparation functions"},{"location":"reference/#neural_net.pipeline.Batch","text":"Source code in neural_net\\pipeline.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class Batch : def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.ndarray): function providing access to Numpy array containing features. y (numpy.ndarray): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0 def __iter__ ( self ): return self def __next__ ( self ): if self . c < len ( self . ix ): self . c += 1 return self . getters ( self . ix [ self . c - 1 ]) self . c = 0 raise StopIteration","title":"Batch"},{"location":"reference/#neural_net.pipeline.Batch.__init__","text":"Initialize a Batch object. Parameters: size ( int ) \u2013 Size of each batch. obs ( int ) \u2013 Total sample size. X ( ndarray ) \u2013 function providing access to Numpy array containing features. y ( ndarray ) \u2013 function providing access to Numpy array containing target variable. Returns: None \u2013 None Example: >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] Source code in neural_net\\pipeline.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , size : int , obs : int , X : callable , y : callable ) -> None : \"\"\" Initialize a Batch object. Args: size (int): Size of each batch. obs (int): Total sample size. X (numpy.ndarray): function providing access to Numpy array containing features. y (numpy.ndarray): function providing access to Numpy array containing target variable. Returns: None Example: ```python >>> def get_X(): ... return numpy.array([[1, 2], [3, 4], [5, 6]]) >>> def get_y(): ... return numpy.array([[0], [1], [0]]) >>> batch_size = 2 >>> total_samples = len(X) >>> batch = Batch(size=batch_size, obs=total_samples, X=get_X, y=get_y) >>> for X_batch, y_batch in batch: ... print(f\"Features: {X_batch}, Target: {y_batch}\") Features: [[1 2] [3 4]], Target: [[0] [1]] Features: [[5 6]], Target: [[0]] ``` \"\"\" self . size = size self . obs = obs self . X = X self . y = y self . getters = lambda ix : ( X ()[ ix ,:], y ()[ ix ,:]) self . i = self . getters ( slice ( 0 , 10 )) self . ix = get_ix ( size , obs ) self . c = 0","title":"__init__"},{"location":"reference/#neural_net.pipeline.get_ix","text":"Create batch slices for a given sample size and batch size. Parameters: obs ( int ) \u2013 Total number of samples in the dataset. size ( int ) \u2013 Size of each batch. Returns: list [ slice ] \u2013 list[slice]: A list of slice objects representing batch indices. Example: >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] Source code in neural_net\\pipeline.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def get_ix ( size : int , obs : int ) -> list [ slice ]: \"\"\" Create batch slices for a given sample size and batch size. Args: obs (int): Total number of samples in the dataset. size (int): Size of each batch. Returns: list[slice]: A list of slice objects representing batch indices. Example: ```python >>> obs = 70 # Total samples >>> batch_size = 20 >>> batch_slices = get_ix(obs, batch_size) >>> batch_slices [slice(0, 20, None), slice(20, 40, None), slice(40, 60, None), slice(60, 70, None)] ``` \"\"\" batchix = list ( range ( 0 , obs , size )) if batchix [ - 1 ] < obs : batchix . append ( obs ) batchix = [ slice ( low , high ) for low , high in zip ( batchix , batchix [ 1 :])] return batchix","title":"get_ix"},{"location":"reference/#neural_net.pipeline.onehot","text":"One-hot encodes a categorical target variable. Parameters: y ( ndarray ) \u2013 Numpy array containing the categorical target variable. Returns: ndarray \u2013 numpy.ndarray: One-hot encoded representation of the target variable. Example: >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] Source code in neural_net\\pipeline.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def onehot ( y : numpy . ndarray ) -> numpy . ndarray : \"\"\" One-hot encodes a categorical target variable. Args: y (numpy.ndarray): Numpy array containing the categorical target variable. Returns: numpy.ndarray: One-hot encoded representation of the target variable. Example: ```python >>> y = numpy.array([[0],[ 1], [2], [1], [0]]) >>> onehot_encoded = onehot(y) >>> print(onehot_encoded) [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.] [0. 1. 0.] [1. 0. 0.]] ``` \"\"\" return ( y == numpy . unique ( y )) + 0","title":"onehot"},{"location":"reference/#neural_net.pipeline.scaler","text":"Custom scaler function for centering and standardizing features. Parameters: X ( ndarray ) \u2013 Input numpy array containing features. Returns: ndarray \u2013 numpy.ndarray: Scaled version of the input array. Example: >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] Source code in neural_net\\pipeline.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def scaler ( X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Custom scaler function for centering and standardizing features. Args: X (numpy.ndarray): Input numpy array containing features. Returns: numpy.ndarray: Scaled version of the input array. Example: ```python >>> X = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) >>> scaled_X = scaler(X) >>> print(scaled_X) [[-1.22474487 -1.22474487] [ 0. 0. ] [ 1.22474487 1.22474487]] ``` \"\"\" return ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 )","title":"scaler"},{"location":"reference/#neural_net.pipeline.shuffle","text":"shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Parameters: X ( ndarray ) \u2013 Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y ( ndarray ) \u2013 Target variable with shape (n, 1). Returns: tuple [ ndarray , ndarray ] \u2013 Tuple[numpy.ndarray, numpy.ndarray]: Shuffled X and y arrays. Example: >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. Source code in neural_net\\pipeline.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def shuffle ( X : numpy . ndarray , y : numpy . ndarray ) -> tuple [ numpy . ndarray , numpy . ndarray ]: \"\"\" shuffle features and tagert variable numpy arrays X and y using pandas.sample method. Args: X (numpy.ndarray): Matrix of training features with shape (n, k), where n is the number of samples and k is the number of features. y (numpy.ndarray): Target variable with shape (n, 1). Returns: Tuple[numpy.ndarray, numpy.ndarray]: Shuffled X and y arrays. Example: ```python >>> n,k = 5000,2 >>> X_train = numpy.random.uniform(-100,100,size=(n,k)) >>> y_train =( (X_train[:, 0]**2 + X_train[:, 1]**2)/numpy.pi < 1000).reshape(-1,1)+0 >>> shuffled_X, shuffled_y = shuffle(X_train, y_train) # Now shuffled_X and shuffled_y contain randomly shuffled samples. ``` \"\"\" X = pandas . DataFrame ( X ) . sample ( frac = 1 ) y = pandas . DataFrame ( y ) . loc [ X . index ] return X . values , y . values","title":"shuffle"},{"location":"reference/#9-class-models-used-to-build-other-classes","text":"This modules provides genric templates for other modules Define - generic object Architecture - Architecture super object Layer - layer super object Neurons - neurons super object Cost - cost super object Metrics - weight super object","title":"9. Class Models used to build other classes"},{"location":"reference/#neural_net.model.Architecture","text":"Bases: Define Model for Architecture functions see :func: ~architecture.Sequential Source code in neural_net\\model.py 326 327 328 329 330 331 332 333 class Architecture ( Define ): \"\"\" Model for Architecture functions see :func:`~architecture.Sequential` \"\"\" def __str__ ( self ) -> str : return 'Architecture'","title":"Architecture"},{"location":"reference/#neural_net.model.Cost","text":"Bases: Define Model for Cost functions see :func: ~cost.binaryCrossEntropy Source code in neural_net\\model.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 class Cost ( Define ): \"\"\" Model for Cost functions see :func:`~cost.binaryCrossEntropy` \"\"\" def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 ) def __str__ ( self ) -> str : return 'Cost'","title":"Cost"},{"location":"reference/#neural_net.model.Cost.clip","text":"Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than np.minimum(a_max, np.maximum(a, a_min)) . No check is performed to ensure a_min < a_max .","title":"clip"},{"location":"reference/#neural_net.model.Cost.clip--parameters","text":"a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If None , clipping is not performed on the corresponding edge. Only one of a_min and a_max may be None . Both are broadcast against a . out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. out must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref: ufunc docs <ufuncs.kwargs> . .. versionadded:: 1.17.0","title":"Parameters"},{"location":"reference/#neural_net.model.Cost.clip--returns","text":"clipped_array : ndarray An array with the elements of a , but where values < a_min are replaced with a_min , and those > a_max with a_max .","title":"Returns"},{"location":"reference/#neural_net.model.Cost.clip--see-also","text":":ref: ufuncs-output-type","title":"See Also"},{"location":"reference/#neural_net.model.Cost.clip--notes","text":"When a_min is greater than a_max , clip returns an array in which all values are equal to a_max , as shown in the second example.","title":"Notes"},{"location":"reference/#neural_net.model.Cost.clip--examples","text":">>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) Source code in neural_net\\model.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def clip ( self ): \"\"\" Applies numpy.clip function described bellow to the predicted probabilities It constrains values between [\u03b5,1-\u03b5] where \u03b5=1e-7 clip(a, a_min, a_max, out=None, **kwargs) Clip (limit) the values in an array. Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of ``[0, 1]`` is specified, values smaller than 0 become 0, and values larger than 1 become 1. Equivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``. No check is performed to ensure ``a_min < a_max``. Parameters ---------- a : array_like Array containing elements to clip. a_min, a_max : array_like or None Minimum and maximum value. If ``None``, clipping is not performed on the corresponding edge. Only one of `a_min` and `a_max` may be ``None``. Both are broadcast against `a`. out : ndarray, optional The results will be placed in this array. It may be the input array for in-place clipping. `out` must be of the right shape to hold the output. Its type is preserved. **kwargs For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`. .. versionadded:: 1.17.0 Returns ------- clipped_array : ndarray An array with the elements of `a`, but where values < `a_min` are replaced with `a_min`, and those > `a_max` with `a_max`. See Also -------- :ref:`ufuncs-output-type` Notes ----- When `a_min` is greater than `a_max`, `clip` returns an array in which all values are equal to `a_max`, as shown in the second example. Examples -------- ```python >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, 1, 8) array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8]) >>> np.clip(a, 8, 1) array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) >>> np.clip(a, 3, 6, out=a) array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6]) >>> a = np.arange(10) >>> a array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8) array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8]) ``` \"\"\" \u03b5 = 1e-7 self . p = self . p . clip ( \u03b5 , 1 - \u03b5 )","title":"Examples"},{"location":"reference/#neural_net.model.Define","text":"Bases: DBmanager Source code in neural_net\\model.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class Define ( DBmanager ): __store = True def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__ @property def id ( self ) -> dict : return self . _id @id . setter def id ( self , loc ) -> None : \"\"\" Sets id property for the class Instantiate and stores sqlalchemy table of self \"\"\" loc = unfold ( loc ) self . _id = { 'id' : id ( self ), f ' { str ( self ) } _id' : id ( self ), 'name' : repr ( self ), ** loc } if Define . _Define__store : if not hasattr ( self , 'table' ): self . table = get_instance ( self ) self . add_table ( self . table ) else : update_instance ( self ) self . commit () def __getitem__ ( self , ix ) -> any : return self . id [ ix ] def __setitem__ ( self , ix , val ) -> None : self . id [ ix ] = val @property def get ( self ) -> dict . get : return self . id . get def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent } def __iter__ ( self ) -> object : return self def __len__ ( self ) -> int : return len ( self [ 'steps' ]) def __next__ ( self ) -> any : if self . c < len ( self ): self . c += 1 return self [ 'steps' ][ self . c - 1 ] self . c = 0 raise StopIteration def commit ( self ) -> None : if Define . _Define__store : DBmanager . session . commit () def predict ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.ndarray) : input features matrix Returns: numpy.ndarray of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out def update ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.ndarray) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) if Define . _Define__store : self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ])","title":"Define"},{"location":"reference/#neural_net.model.Define.__add__","text":"Triggers the id property Parameters: loc ( dict) ) \u2013 dictionary of properties Source code in neural_net\\model.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __add__ ( self , loc : dict ) -> None : \"\"\" Triggers the id property Args: loc (dict) : dictionary of properties \"\"\" self . id = loc self . c = 0 class func : def __init__ ( self , _ ): ... self . init_method = self . get ( 'Layer_init_method' , func ) self . func = self . get ( 'func' , func ) self . func = self . func ( self . id ) self [ 'steps' ] = self . get ( 'steps' ,[]) parent = { f ' { str ( self ) } _id' : self [ 'id' ]} for step in self : step . id = { ** step . id , ** parent }","title":"__add__"},{"location":"reference/#neural_net.model.Define.__repr__","text":"Returns the name of the class. Returns: str ( str ) \u2013 The name of the class. Source code in neural_net\\model.py 19 20 21 22 23 24 25 26 def __repr__ ( self ) -> str : \"\"\" Returns the name of the class. Returns: str: The name of the class. \"\"\" return self . __class__ . __name__","title":"__repr__"},{"location":"reference/#neural_net.model.Define.compute_store","text":"Generic method that computes item and stores it to sqlalchemy session Source code in neural_net\\model.py 131 132 133 134 135 136 137 138 139 140 141 def compute_store ( self ) -> None : \"\"\" Generic method that computes item and stores it to sqlalchemy session \"\"\" value = self . compute ( self . y , self . p ) if Define . _Define__store : self . commit () del ( self . table ) self + { ** self . id , ** locals ()} return value","title":"compute_store"},{"location":"reference/#neural_net.model.Define.predict","text":"Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Parameters: X ( numpy.ndarray) ) \u2013 input features matrix Returns: ndarray \u2013 numpy.ndarray of output layer predictions Source code in neural_net\\model.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def predict ( self , X : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implements forward prediction of input feature matrix X of size n,k Passes outputs from input layer to output layer Args: X (numpy.ndarray) : input features matrix Returns: numpy.ndarray of output layer predictions \"\"\" self . out = X for step in self : self . out = step . func . compute ( self . out ) return self . out","title":"predict"},{"location":"reference/#neural_net.model.Define.update","text":"Implement backpropagation of cost gradient to all layers Passes gradients backward Parameters: \u0394 ( numpy.ndarray) ) \u2013 array of gradient from next step Returns: ndarray \u2013 numpy array of input layer gradient Source code in neural_net\\model.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def update ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Implement backpropagation of cost gradient to all layers Passes gradients backward Args: \u0394 (numpy.ndarray) : array of gradient from next step Returns: numpy array of input layer gradient \"\"\" for step in self [ 'steps' ][:: - 1 ]: \u0394 = step . func . grad ( \u0394 ) return \u0394","title":"update"},{"location":"reference/#neural_net.model.Define.updateW","text":"Updates sqlalchemy tables containing weights Source code in neural_net\\model.py 143 144 145 146 147 148 149 150 151 def updateW ( self ) -> None : \"\"\" Updates sqlalchemy tables containing weights \"\"\" for obj in Neurons . with_weights : for i , r in enumerate ( obj . Wtables ): for j , table in enumerate ( r ): setattr ( table , 'value' , obj . W [ i , j ])","title":"updateW"},{"location":"reference/#neural_net.model.Layer","text":"Bases: Define Model for layer functions see :func: ~layer.fullyconnected Source code in neural_net\\model.py 153 154 155 156 157 158 159 160 class Layer ( Define ): \"\"\" Model for layer functions see :func:`~layer.fullyconnected` \"\"\" def __str__ ( self ) -> str : return 'Layer'","title":"Layer"},{"location":"reference/#neural_net.model.Metrics","text":"Bases: Define Model for Metrics functions see :func: ~metrics.accuracy Source code in neural_net\\model.py 317 318 319 320 321 322 323 324 class Metrics ( Define ): \"\"\" Model for Metrics functions see :func:`~metrics.accuracy` \"\"\" def __str__ ( self ) -> str : return 'Metrics'","title":"Metrics"},{"location":"reference/#neural_net.model.Neurons","text":"Bases: Define Model for activation functions see :func: ~activation.Softmax Source code in neural_net\\model.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class Neurons ( Define ): \"\"\" Model for activation functions see :func:`~activation.Softmax` \"\"\" with_weights = [] def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" if Define . _Define__store : table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ] def storeW ( self ): \"\"\" Stores weights tables \"\"\" if Define . _Define__store : for row in self . Wtables : for table in row : self . add_table ( table ) def __str__ ( self ) -> str : return 'Neurons' def __sub__ ( self , \u0394 : numpy . ndarray ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ] def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.ndarray) : gradient from next step Returns: numpy.ndarray of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394","title":"Neurons"},{"location":"reference/#neural_net.model.Neurons.__sub__","text":"Substracts Gradient to Weights Source code in neural_net\\model.py 204 205 206 207 208 209 def __sub__ ( self , \u0394 : numpy . ndarray ) -> None : \"\"\" Substracts Gradient to Weights \"\"\" self . W -= \u0394","title":"__sub__"},{"location":"reference/#neural_net.model.Neurons.grad","text":"Computes gradient for previous step Parameters: \u0394 ( numpy.ndarray) ) \u2013 gradient from next step Returns: ndarray \u2013 numpy.ndarray of gradient for previous step Source code in neural_net\\model.py 218 219 220 221 222 223 224 225 226 227 228 229 230 def grad ( self , \u0394 : numpy . ndarray ) -> numpy . ndarray : \"\"\" Computes gradient for previous step Args: \u0394 (numpy.ndarray) : gradient from next step Returns: numpy.ndarray of gradient for previous step \"\"\" self . \u0394 = self . pr () * \u0394 return self . \u0394","title":"grad"},{"location":"reference/#neural_net.model.Neurons.instantiateW","text":"Instantiate weight tables Source code in neural_net\\model.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def instantiateW ( self ) -> None : \"\"\" Instantiate weight tables \"\"\" if Define . _Define__store : table , cols = tables [ 'Weight' ] self . Wtables = [] for i , r in enumerate ( self . W ): instances = [] for j , e in enumerate ( r ): instances += [ table ( Weight_id = f ' { i } _ { j } ' , value = e , Neurons_id = self [ 'id' ] ) ] self . Wtables += [ instances ] instances = [] Neurons . with_weights += [ self ]","title":"instantiateW"},{"location":"reference/#neural_net.model.Neurons.n","text":"Returns sample size for current features matrix Source code in neural_net\\model.py 211 212 213 214 215 def n ( self ) -> int : \"\"\" Returns sample size for current features matrix \"\"\" return self . X . shape [ 0 ]","title":"n"},{"location":"reference/#neural_net.model.Neurons.storeW","text":"Stores weights tables Source code in neural_net\\model.py 192 193 194 195 196 197 198 199 def storeW ( self ): \"\"\" Stores weights tables \"\"\" if Define . _Define__store : for row in self . Wtables : for table in row : self . add_table ( table )","title":"storeW"},{"location":"reference/#10-utility-functions","text":"Provides utility functions get_module_path - Returns the path to a subdirectory named 'dir' relative to the currently executed script. now - current timestamp unfold - Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary.","title":"10. Utility functions"},{"location":"reference/#neural_net.utils.HouseDatasetDownloader","text":"Downloads boston housing dataset. Parameters: src \u2013 str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: columns \u2013 list of str ordered list of columns in the dataset. data \u2013 array data array of features and target variable csv \u2013 str Raw CSV textfile description \u2013 str Full description of housing database Example >>> houseloader = HouseDatasetDownloader() >>> houseloader.load_dataset() >>> print(houseloader.columns) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] >>> print(houseloader.data) [[6.3200e-03 1.8000e+01 2.3100e+00 ... 3.9690e+02 4.9800e+00 2.4000e+01] [2.7310e-02 0.0000e+00 7.0700e+00 ... 3.9690e+02 9.1400e+00 2.1600e+01] [2.7290e-02 0.0000e+00 7.0700e+00 ... 3.9283e+02 4.0300e+00 3.4700e+01] ... [6.0760e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 5.6400e+00 2.3900e+01] [1.0959e-01 0.0000e+00 1.1930e+01 ... 3.9345e+02 6.4800e+00 2.2000e+01] [4.7410e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 7.8800e+00 1.1900e+01]] Source code in neural_net\\utils.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 class HouseDatasetDownloader : \"\"\" Downloads boston housing dataset. Parameters: src : str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: columns : list of str ordered list of columns in the dataset. data : array data array of features and target variable csv : str Raw CSV textfile description : str Full description of housing database Example: ```python >>> houseloader = HouseDatasetDownloader() >>> houseloader.load_dataset() >>> print(houseloader.columns) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] >>> print(houseloader.data) [[6.3200e-03 1.8000e+01 2.3100e+00 ... 3.9690e+02 4.9800e+00 2.4000e+01] [2.7310e-02 0.0000e+00 7.0700e+00 ... 3.9690e+02 9.1400e+00 2.1600e+01] [2.7290e-02 0.0000e+00 7.0700e+00 ... 3.9283e+02 4.0300e+00 3.4700e+01] ... [6.0760e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 5.6400e+00 2.3900e+01] [1.0959e-01 0.0000e+00 1.1930e+01 ... 3.9345e+02 6.4800e+00 2.2000e+01] [4.7410e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 7.8800e+00 1.1900e+01]] ``` \"\"\" def __init__ ( self , src = \"http://lib.stat.cmu.edu/datasets/boston\" ): self . src = src self . csv = None def load_dataset ( self ): \"\"\" Load the House dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text self . description , data = self . csv [: self . csv . index ( \"0.00632\" )], self . csv [ self . csv . index ( \"0.00632\" ):] data = data . replace ( ' \\n ' , ' ' ) . split ( ' \\n ' )[: - 1 ] data = [ re . findall ( r '(\\d+\\.*\\d*)' , r ) for r in data ] self . data = numpy . array ( data ) . astype ( float ) self . columns = columns = [ c . split ()[ 0 ] for c in self . description . split ( ' \\n ' )[ - 16 : - 2 ]]","title":"HouseDatasetDownloader"},{"location":"reference/#neural_net.utils.HouseDatasetDownloader.load_dataset","text":"Load the House dataset from the specified online CSV source. Source code in neural_net\\utils.py 408 409 410 411 412 413 414 415 416 417 418 def load_dataset ( self ): \"\"\" Load the House dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text self . description , data = self . csv [: self . csv . index ( \"0.00632\" )], self . csv [ self . csv . index ( \"0.00632\" ):] data = data . replace ( ' \\n ' , ' ' ) . split ( ' \\n ' )[: - 1 ] data = [ re . findall ( r '(\\d+\\.*\\d*)' , r ) for r in data ] self . data = numpy . array ( data ) . astype ( float ) self . columns = columns = [ c . split ()[ 0 ] for c in self . description . split ( ' \\n ' )[ - 16 : - 2 ]]","title":"load_dataset"},{"location":"reference/#neural_net.utils.IrisDatasetDownloader","text":"Downloads the Iris dataset from an online CSV source. Parameters: src \u2013 str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: target_names \u2013 list of str Names of each target label (species) in the dataset. feature_names \u2013 list of str Names of all features (attributes) in the dataset. csv \u2013 str raw CSV textfile description \u2013 str Full description of iris database data \u2013 numpy.ndarray array of all features target \u2013 numpy.ndarray array of target variable Example >>> iris = IrisDatasetDownloader() >>> iris.load_dataset() >>> print(iris.data.shape,iris.target.shape) (150, 4) (150, 1) >>> print(iris.target_names) ['setosa', 'versicolor', 'virginica'] >>> print(iris.feature_names) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] >>> print(iris.data[:5,:]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] >>> print(iris.target[:5,:]) [[0] [0] [0] [0] [0]] Source code in neural_net\\utils.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 class IrisDatasetDownloader : \"\"\" Downloads the Iris dataset from an online CSV source. Parameters: src : str URL to the online CSV file containing the Iris dataset default at https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv Attributes: target_names : list of str Names of each target label (species) in the dataset. feature_names : list of str Names of all features (attributes) in the dataset. csv : str raw CSV textfile description : str Full description of iris database data : numpy.ndarray array of all features target : numpy.ndarray array of target variable Example: ```python >>> iris = IrisDatasetDownloader() >>> iris.load_dataset() >>> print(iris.data.shape,iris.target.shape) (150, 4) (150, 1) >>> print(iris.target_names) ['setosa', 'versicolor', 'virginica'] >>> print(iris.feature_names) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] >>> print(iris.data[:5,:]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] >>> print(iris.target[:5,:]) [[0] [0] [0] [0] [0]] ``` \"\"\" description = r \"\"\" 1. Title: Iris Plants Database Updated Sept 21 by C.Blake - Added discrepency information 2. Sources: (a) Creator: R.A. Fisher (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) (c) Date: July, 1988 3. Past Usage: - Publications: too many to mention!!! Here are a few. 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. -- Results: -- very low misclassification rates (0 % f or the setosa class) 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. -- Results: -- very low misclassification rates again 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. 4. Relevant Information: --- This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. --- Predicted attribute: class of iris plant. --- This is an exceedingly simple domain. --- This data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick@espeedaz.net ) The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features. 5. Number of Instances: 150 (50 in each of three classes) 6. Number of Attributes: 4 numeric, predictive attributes and the class 7. Attribute Information: 1. sepal length in cm 2. sepal width in cm 3. petal length in cm 4. petal width in cm 5. class: -- Iris Setosa -- Iris Versicolour -- Iris Virginica 8. Missing Attribute Values: None Summary Statistics: Min Max Mean SD Class Correlation sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) 9. Class Distribution: 33.3 % f or each of 3 classes. \"\"\" def __init__ ( self , src = \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\" ): self . src = src self . feature_names = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ] self . target_names = [] self . csv = None def load_dataset ( self ): \"\"\" Load the Iris dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text columns , * data , _ = self . csv . split ( ' \\n ' ) data , target = [ r . split ( ',' )[: - 1 ] for r in data ],[ r . split ( ',' )[ - 1 ] for r in data ] self . target_names = [] for l in target : if l not in self . target_names : self . target_names += [ l ] self . target = numpy . array ([[ self . target_names . index ( l )] for l in target ]) self . feature_names = columns . split ( ',' )[: - 1 ] for i in range ( len ( self . feature_names )): self . feature_names [ i ] = self . feature_names [ i ] . replace ( '_' , ' ' ) self . feature_names [ i ] += ' (cm)' self . data = numpy . array ( data ) . astype ( float )","title":"IrisDatasetDownloader"},{"location":"reference/#neural_net.utils.IrisDatasetDownloader.load_dataset","text":"Load the Iris dataset from the specified online CSV source. Source code in neural_net\\utils.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 def load_dataset ( self ): \"\"\" Load the Iris dataset from the specified online CSV source. \"\"\" self . csv = requests . get ( self . src ) . text columns , * data , _ = self . csv . split ( ' \\n ' ) data , target = [ r . split ( ',' )[: - 1 ] for r in data ],[ r . split ( ',' )[ - 1 ] for r in data ] self . target_names = [] for l in target : if l not in self . target_names : self . target_names += [ l ] self . target = numpy . array ([[ self . target_names . index ( l )] for l in target ]) self . feature_names = columns . split ( ',' )[: - 1 ] for i in range ( len ( self . feature_names )): self . feature_names [ i ] = self . feature_names [ i ] . replace ( '_' , ' ' ) self . feature_names [ i ] += ' (cm)' self . data = numpy . array ( data ) . astype ( float )","title":"load_dataset"},{"location":"reference/#neural_net.utils.Pearson","text":"Computes the Pearson correlation matrix and generates a heatmap. Attributes: X \u2013 numpy.ndarray The input data containing features. n \u2013 int number of observations k \u2013 int number of features cols \u2013 list list of fed columns cov \u2013 numpy.ndarray covariance matrix var \u2013 numpy.ndarray variance matrix corr ( ndarray ) \u2013 numpy.ndarray correlation matrix Methods: Name Description __init__ numpy.ndarray,cols:list=None)->None: Initialize Pearson object corr computes Pearson correlation matrix heatmap int=6,digits:int=1, xrotation:Union[int,str]=45,yrotation:Union[int,str]='horizontal') -> None: plots correlation heatmap Example >>> import pandas as pd >>> from matplotlib import pyplot as plt >>> >>> # Create a sample dataset >>> data = pd.DataFrame({ ... 'feature1': [1, 2, 3, 4, 5], ... 'feature2': [5, 4, 3, 2, 1], ... 'feature3': [3, 3, 3, 3, 3] ... }) >>> >>> # Initialize the Pearson correlation analyzer >>> pearson_analyzer = Pearson(X=data.values,cols=data.columns) >>> # Compute the Pearson correlation matrix >>> correlation_matrix = pearson_analyzer.corr() >>> # Generate the heatmap >>> pearson_analyzer.heatmap() Source code in neural_net\\utils.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class Pearson : \"\"\" Computes the Pearson correlation matrix and generates a heatmap. Attributes: X : numpy.ndarray The input data containing features. n : int number of observations k: int number of features cols: list list of fed columns cov: numpy.ndarray covariance matrix var: numpy.ndarray variance matrix corr: numpy.ndarray correlation matrix Methods: __init__(X:numpy.ndarray,cols:list=None)->None: Initialize Pearson object corr()->numpy.ndarray: computes Pearson correlation matrix heatmap(ax=None,fontsize:int=6,digits:int=1, xrotation:Union[int,str]=45,yrotation:Union[int,str]='horizontal') -> None: plots correlation heatmap Example: ```python >>> import pandas as pd >>> from matplotlib import pyplot as plt >>> >>> # Create a sample dataset >>> data = pd.DataFrame({ ... 'feature1': [1, 2, 3, 4, 5], ... 'feature2': [5, 4, 3, 2, 1], ... 'feature3': [3, 3, 3, 3, 3] ... }) >>> >>> # Initialize the Pearson correlation analyzer >>> pearson_analyzer = Pearson(X=data.values,cols=data.columns) >>> # Compute the Pearson correlation matrix >>> correlation_matrix = pearson_analyzer.corr() >>> # Generate the heatmap >>> pearson_analyzer.heatmap() ``` \"\"\" def __init__ ( self , X : numpy . ndarray , cols : list = None ) -> None : \"\"\" Initialize Pearson object Args: X:numpy.ndarray array for which you'd like to get correlations from cols: list list of labels for columns \"\"\" self . X = X self . n , self . k = X . shape self . cols = cols or tuple ( range ( self . k )) def corr ( self ) -> numpy . ndarray : self . cov = ( v := ( self . X - self . X . mean ( axis = 0 ))) . T . dot ( v ) / self . n self . var = ( std := self . X . std ( axis = 0 )) . reshape ( - 1 , 1 ) * std self . corr = self . cov / self . var return self . corr def heatmap ( self , ax = None , fontsize : int = 6 , digits : int = 1 , xrotation : Union [ int , str ] = 45 , yrotation : Union [ int , str ] = 'horizontal' ): ax = ax or plt . subplots ()[ 1 ] im = ax . imshow ( self . corr ) im . set_clim ( - 1 , 1 ) ax . grid ( False ) ax . xaxis . set_ticks ( ticks = tuple ( range ( self . k )), labels = self . cols , rotation = xrotation ) ax . yaxis . set_ticks ( ticks = tuple ( range ( self . k )), labels = self . cols , rotation = yrotation ) for i in range ( self . k ): for j in range ( self . k ): ax . text ( j , i , self . corr . round ( digits )[ i , j ], ha = 'center' , va = 'center' , fontsize = fontsize , color = 'r' ) cbar = ax . figure . colorbar ( im , ax = ax , format = ' % .2f ' ) plt . show ()","title":"Pearson"},{"location":"reference/#neural_net.utils.Pearson.__init__","text":"Initialize Pearson object Parameters: X ( ndarray ) \u2013 numpy.ndarray array for which you'd like to get correlations from cols ( list , default: None ) \u2013 list list of labels for columns Source code in neural_net\\utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , X : numpy . ndarray , cols : list = None ) -> None : \"\"\" Initialize Pearson object Args: X:numpy.ndarray array for which you'd like to get correlations from cols: list list of labels for columns \"\"\" self . X = X self . n , self . k = X . shape self . cols = cols or tuple ( range ( self . k ))","title":"__init__"},{"location":"reference/#neural_net.utils.get_module_path","text":"Returns the path to a subdirectory named 'dir' relative to the currently executed script. Parameters: dir ( str ) \u2013 path to the subdirectory. Returns: str ( str ) \u2013 Absolute path to the specified subdirectory. Source code in neural_net\\utils.py 323 324 325 326 327 328 329 330 331 332 333 def get_module_path ( dir : list [ str ]) -> str : \"\"\" Returns the path to a subdirectory named 'dir' relative to the currently executed script. Args: dir (str): path to the subdirectory. Returns: str: Absolute path to the specified subdirectory. \"\"\" return os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), * dir )","title":"get_module_path"},{"location":"reference/#neural_net.utils.make_circle_data","text":"Generates random data points distributed within circles. Parameters: centers \u2013 list of tuples List of (x, y) coordinates representing the centers of circles. radii \u2013 list of floats List of radii for each circle. p \u2013 float, optional (default=0.2) Percentage of randomly picked data points outside the circles. n_grid \u2013 int, optional (default=100) Meshgrid parameter for creating the grid of points. xmin \u2013 int, optional (default=-100) Minimum x limit for the data points. xmax \u2013 int, optional (default=100) Maximum x limit for the data points. ymin \u2013 int, optional (default=-100) Minimum y limit for the data points. ymax \u2013 int, optional (default=100) Maximum y limit for the data points. Returns: X ( tuple ) \u2013 numpy.ndarray, shape (n_samples, 2) 2D matrix of features (coordinates of data points). y ( tuple ) \u2013 numpy.ndarray, shape (n_samples,1) Labels corresponding to the data points (1 if inside a circle, 0 otherwise). Example >>> centers = [(0, 0), (20, 30)] >>> radii = [10, 15] >>> X, y = make_circle(centers, radii, p=0.1) >>> print(X.shape, y.shape) (200, 2) (200,1) Source code in neural_net\\utils.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def make_circle_data ( centers : list , radii : list , p : float = .2 , n_grid : int = 100 , xmin : int =- 100 , xmax : int = 100 , ymin : int =- 100 , ymax : int = 100 ) -> tuple : \"\"\" Generates random data points distributed within circles. Parameters: centers : list of tuples List of (x, y) coordinates representing the centers of circles. radii : list of floats List of radii for each circle. p : float, optional (default=0.2) Percentage of randomly picked data points outside the circles. n_grid : int, optional (default=100) Meshgrid parameter for creating the grid of points. xmin : int, optional (default=-100) Minimum x limit for the data points. xmax : int, optional (default=100) Maximum x limit for the data points. ymin : int, optional (default=-100) Minimum y limit for the data points. ymax : int, optional (default=100) Maximum y limit for the data points. Returns: X : numpy.ndarray, shape (n_samples, 2) 2D matrix of features (coordinates of data points). y : numpy.ndarray, shape (n_samples,1) Labels corresponding to the data points (1 if inside a circle, 0 otherwise). Example: ```python >>> centers = [(0, 0), (20, 30)] >>> radii = [10, 15] >>> X, y = make_circle(centers, radii, p=0.1) >>> print(X.shape, y.shape) (200, 2) (200,1) ``` \"\"\" x , y = numpy . linspace ( xmin , xmax , n_grid ), numpy . linspace ( ymin , ymax , n_grid ) xm , ym = numpy . meshgrid ( x , y ) x_news , y_news = [],[] labels = [] n_centers = len ( centers ) j = 0 for i in range ( n_centers ): c = ( xm - centers [ i ][ 0 ]) ** 2 + ( ym - centers [ i ][ 1 ]) ** 2 <= radii [ i ] ** 2 x_new , y_new = numpy . where ( c ) n = len ( x_new ) n_sample = int ( numpy . ceil ( n * p )) ix = numpy . random . randint ( 0 , n , n_sample ) x_news += [ x [ x_new ][ ix ]] y_news += [ y [ y_new ][ ix ]] labels += [ j ] * n_sample j += 1 x_news = numpy . concatenate ( x_news ) y_news = numpy . concatenate ( y_news ) labels = numpy . array ( labels ) . reshape ( - 1 , 1 ) return numpy . c_ [ x_news , y_news ], labels","title":"make_circle_data"},{"location":"reference/#neural_net.utils.now","text":"Returns the current timestamp as an integer. Returns: int ( int ) \u2013 Current timestamp (number of seconds since the epoch). Source code in neural_net\\utils.py 335 336 337 338 339 340 341 342 def now () -> int : \"\"\" Returns the current timestamp as an integer. Returns: int: Current timestamp (number of seconds since the epoch). \"\"\" return int ( datetime . datetime . now () . timestamp ())","title":"now"},{"location":"reference/#neural_net.utils.unfold","text":"Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Parameters: d ( dict ) \u2013 Input dictionary with nested dictionaries. Returns: dict ( dict ) \u2013 Unfolded dictionary with concatenated keys. Example: >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} Source code in neural_net\\utils.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def unfold ( d : dict ) -> dict : \"\"\" Unfolds a nested dictionary by appending the values of inner dictionaries to the outer dictionary. Args: d (dict): Input dictionary with nested dictionaries. Returns: dict: Unfolded dictionary with concatenated keys. Example: ```python >>> d = {'a':1,'b':{'c':2,'d':4}} >>> unfold(d) {'a': 1, 'b_c': 2, 'b_d': 4} ``` \"\"\" new_d = {} for k in d : if hasattr ( d [ k ], 'keys' ): for j in d [ k ]: new_d [ f ' { k } _ { j } ' ] = d [ k ][ j ] else : new_d [ k ] = d [ k ] return new_d","title":"unfold"},{"location":"tutorials/","text":"Tutorials Install NNN library from TestPypi repo link here Check python version (requirement python >=3.8) $ python --version Python 3.11.7 $ pip install --upgrade pip $ pip install -i https://test.pypi.org/simple/ neural-net-numpy Requirement already satisfied: pip in /home/analyst/dlenv/lib/python3.11/site-packages (24.0) Looking in indexes: https://test.pypi.org/simple/ Collecting neural-net-numpy==0.1.4 Downloading https://test-files.pythonhosted.org/packages/48/77/4d5e4d9de3f9bd758dd510a2d9a3dfb0566f3c90dcd8e40d81e3af815ef4/neural_net_numpy-0.1.4-py3-none-any.whl.metadata (1.8 kB) Requirement already satisfied: tqdm>=4.66.2 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (4.66.2) Requirement already satisfied: numpy>=1.26.4 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (1.26.4) Requirement already satisfied: SQLAlchemy>=2.0.27 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (2.0.27) Requirement already satisfied: pandas>=2.2.0 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (2.2.0) Requirement already satisfied: python-dateutil>=2.8.2 in /home/analyst/dlenv/lib/python3.11/site-packages (from pandas>=2.2.0->neural-net-numpy==0.1.4) (2.8.2) Requirement already satisfied: pytz>=2020.1 in /home/analyst/dlenv/lib/python3.11/site-packages (from pandas>=2.2.0->neural-net-numpy==0.1.4) (2024.1) Requirement already satisfied: tzdata>=2022.7 in /home/analyst/dlenv/lib/python3.11/site-packages (from pandas>=2.2.0->neural-net-numpy==0.1.4) (2024.1) Requirement already satisfied: typing-extensions>=4.6.0 in /home/analyst/dlenv/lib/python3.11/site-packages (from SQLAlchemy>=2.0.27->neural-net-numpy==0.1.4) (4.9.0) Requirement already satisfied: greenlet!=0.4.17 in /home/analyst/dlenv/lib/python3.11/site-packages (from SQLAlchemy>=2.0.27->neural-net-numpy==0.1.4) (3.0.3) Requirement already satisfied: six>=1.5 in /home/analyst/dlenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->neural-net-numpy==0.1.4) (1.16.0) Downloading https://test-files.pythonhosted.org/packages/48/77/4d5e4d9de3f9bd758dd510a2d9a3dfb0566f3c90dcd8e40d81e3af815ef4/neural_net_numpy-0.1.4-py3-none-any.whl (16 kB) Installing collected packages: neural-net-numpy Successfully installed neural-net-numpy-0.1.4 Check install $ python -c \"import neural_net;print(neural_net.__version__)\" 0.1.0 Import data science libraries import numpy,pandas import matplotlib.pyplot as plt Activation functions z = numpy.linspace(-6,6,1000+1) Sigmoid and Tanh Function values from neural_net.activation import \u03c3,Tanh sigmoid = \u03c3() tanh = Tanh() sigmoid.compute(z) array([0.00247262, 0.0025024 , 0.00253253, ..., 0.99746747, 0.9974976 , 0.99752738]) tanh.compute(z) array([-1.99997542, -1.99997483, -1.99997421, ..., 1.99997421, 1.99997483, 1.99997542]) sigmoid.preds array([0.00247262, 0.0025024 , 0.00253253, ..., 0.99746747, 0.9974976 , 0.99752738]) tanh.preds array([-1.99997542, -1.99997483, -1.99997421, ..., 1.99997421, 1.99997483, 1.99997542]) fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,sigmoid.preds,label=r'$\\sigma=\\frac{1}{1+e^{-z}}$') ax2.plot(z,tanh.preds,label='$2\\sigma(2z) - 1$') ax1.hlines(y=0.5,xmin=-5,xmax=5,color='green',label=r'$y=\\frac{1}{2}$',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',label=r'$y=0$',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Sigmoid') ax2.set_title('Hyperbolic Tangent Function') ax1.grid() ax2.grid() Derivatives sigmoidpr = sigmoid.pr() tanhpr = tanh.pr() fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,sigmoidpr,label=r'$\\sigma(1-\\sigma)$') ax2.plot(z,tanhpr,label='$1 - tanh^2$') ax1.hlines(y=0.25,xmin=65,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Sigmoid Derivative') ax2.set_title('Hyperbolic Tangent Function Derivative') ax1.grid() ax2.grid() Rectified Linear Unit (ReLU) Function values \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} from neural_net.activation import ReLU relu = ReLU() relupred = relu.compute(z) relupred array([0. , 0. , 0. , ..., 5.976, 5.988, 6. ]) fig,ax1 = plt.subplots(nrows=1,ncols=1,figsize=(7.5,6)) ax1.plot(z,relupred,label=r'ReLU') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax1.legend() ax1.set_title('Rectified Linear Unit') ax1.grid() Derivative \\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases} relupr = relu.pr() relupr array([0, 0, 0, ..., 1, 1, 1]) fig,ax1 = plt.subplots(nrows=1,ncols=1,figsize=(7.5,6)) ax1.plot(z,relupr,label=r'ReLU') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax1.legend() ax1.set_title('Rectified Linear Unit Derivative') ax1.grid() Non Saturating activations Function values Leaky Rectified Linear Unit(Leaky ReLU) \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ \\alpha z & \\text{if } z < 0 \\end{cases} Exponential Linear Unit(ELU) \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ \\alpha (e^{z} - 1) & \\text{if } z < 0 \\end{cases} from neural_net.activation import ELU,LeakyReLU elu = ELU(\u03b1=1) leakyrelu = LeakyReLU(\u03b1=.1) elupred,leakyrelupred = elu.compute(z),leakyrelu.compute(z) fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,elupred,label=r'$\\alpha=1$') ax2.plot(z,leakyrelupred,label=r'$\\alpha=0.1$') ax1.hlines(y=0,xmin=-5,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Exponential Linear Unit') ax2.set_title('Leaky Rectified Linear Unit') ax1.grid() ax2.grid() Derivatives ELU \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} Leaky ReLU \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} elupr,leakypr = elu.pr(),leakyrelu.pr() fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,elupr,label=r'$\\alpha=1$') ax2.plot(z,leakypr,label=r'$\\alpha=0.1$') ax1.hlines(y=0,xmin=-5,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Exponential Linear Unit Derivative') ax2.set_title('Leaky Rectified Linear Unit Derivative') ax1.grid() ax2.grid() All common activation function and their derivatives fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,elu.preds,label=r'$ELU(\\alpha=1)$') ax1.plot(z,leakyrelu.preds,label=r'$Leaky\\ ReLU(\\alpha=0.1)$') ax1.plot(z,relu.preds,label=r'ReLU') ax1.plot(z,sigmoid.preds,label=r'$\\sigma=\\frac{1}{1+e^{-z}}$') ax1.plot(z,tanh.preds,label='$tanh=2\\sigma(2z) - 1$') ax2.plot(z,elupr,label=r'$ELU(\\alpha=1)$') ax2.plot(z,leakypr,label=r'$Leaky\\ ReLU(\\alpha=0.1$)') ax2.plot(z,relupr,label=r'ReLU') ax2.plot(z,sigmoidpr,label=r\"$\\sigma'=\\sigma(1-\\sigma)$\") ax2.plot(z,tanhpr,label='$tanh=1 - tanh^2$') ax1.hlines(y=0,xmin=-5,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Common activation functions') ax2.set_title('Derivatives') ax1.grid() ax2.grid() Initialization from neural_net.init_funcs import XavierHe, zeros Weights + Bias n_cols = 2 Zeros W = zeros(n_cols,1) W array([[0.], [0.], [0.]]) W = zeros(n_cols,1,biais=False) W array([[0.], [0.]]) Xavier and He initializer = XavierHe(\"Normal\",\"Sigmoid\").init_func initializer(n_cols,1) array([[ 1.08529782], [ 0.55315106], [-1.20349346]]) initializer(n_cols,1,biais=False) array([[0.14914699], [0.68662846]]) Normal distribution n_cols = 1000 xe_norm_sigmoid = XavierHe(\"Normal\",\"Sigmoid\").init_func(n_cols,1,biais=False) xe_norm_tanh = XavierHe(\"Normal\",\"Tanh\").init_func(n_cols,1,biais=False) xe_norm_relu = XavierHe(\"Normal\",\"ReLU\").init_func(n_cols,1,biais=False) xe_norm_sigmoid.shape (1000, 1) std,m = xe_norm_sigmoid.std(),xe_norm_sigmoid.mean() std,m (0.04489498522857033, -0.0007231213204853488) (( xe_norm_sigmoid >= m-std) & (xe_norm_sigmoid<=m+std)).sum() 697 697/n_cols 0.697 (( xe_norm_sigmoid >= m-2*std) & (xe_norm_sigmoid<=m+2*std)).sum()/n_cols 0.95 (( xe_norm_sigmoid >= m-3*std) & (xe_norm_sigmoid<=m+3*std)).sum()/n_cols 0.997 xe_norm_tanh.std(),xe_norm_relu.std() (0.1711681672411125, 0.06410140383405528) plt.figure(figsize=(10,6)) plt.title('Xavier/He for normal distribution by activation type') plt.hist(xe_norm_tanh,label=r'Tanh $std=4\\sigma$',bins=30,alpha=.4) plt.hist(xe_norm_relu,label=r'ReLU $std=\\sqrt{2}\\sigma$',bins=30,alpha=.5) plt.hist(xe_norm_sigmoid,label=r'Sigmoid $std=\\sigma=\\sqrt{\\frac{2}{n_{in}+n_{out}}}$',bins=30,alpha=.6) plt.legend(loc='upper left') plt.axis('off') (-0.726385463544662, 0.6217518101350396, 0.0, 111.3) Uniform distribution xe_uni_sigmoid = XavierHe(\"Uniform\",\"Sigmoid\").init_func(n_cols,1,biais=False) xe_uni_tanh = XavierHe(\"Uniform\",\"Tanh\").init_func(n_cols,1,biais=False) xe_uni_relu = XavierHe(\"Uniform\",\"ReLU\").init_func(n_cols,1,biais=False) xe_uni_sigmoid.min() -0.07708092487365578 xe_uni_sigmoid.max() 0.0770798990273792 plt.figure(figsize=(10,6)) plt.title('Xavier/He for uniform distribution by activation type') plt.hist(xe_uni_tanh,label=r'Tanh $r=4m$',bins=25,alpha=.4) plt.hist(xe_uni_relu,label=r'ReLU $r=\\sqrt{2}m$',bins=25,alpha=.5) plt.hist(xe_uni_sigmoid,label=r'Sigmoid $r=m=\\sqrt{\\frac{2}{n_{in}+n_{out}}}$',bins=25,alpha=.6) plt.legend(loc='upper left') <matplotlib.legend.Legend at 0x7fac479fe350> Layers from neural_net.layers import Fullyconnected,Activation Linear Layer fc = Fullyconnected(n_in=2,n_out=1,init_method=zeros) repr(fc) 'Fullyconnected' str(fc) 'Layer' fc.id {'id': 140377909934032, 'Layer_id': 140377909934032, 'name': 'Fullyconnected', 'self': Fullyconnected, 'n_in': 2, 'n_out': 1, 'init_method': <function neural_net.init_funcs.zeros(n_in: int, n_out: int, biais: bool = True) -> <built-in function array>>, 'func': neural_net.activation.\u03a3, 'steps': []} fc['id'] 140377909934032 fc.id['id'] 140377909934032 fc.func \u03a3 Linear activation linear_activation = fc.func linear_activation \u03a3 str(linear_activation) 'Neurons' repr(linear_activation) '\u03a3' linear_activation.id {'id': 140377909866128, 'Neurons_id': 140377909866128, 'name': '\u03a3', 'self': \u03a3, 'Layer_id': 140377909934032, 'Layer_Layer_id': 140377909934032, 'Layer_name': 'Fullyconnected', 'Layer_self': Fullyconnected, 'Layer_n_in': 2, 'Layer_n_out': 1, 'Layer_init_method': <function neural_net.init_funcs.zeros(n_in: int, n_out: int, biais: bool = True) -> <built-in function array>>, 'Layer_func': neural_net.activation.\u03a3, 'steps': []} linear_activation.W array([[0.], [0.], [0.]]) linear_activation.W.shape (3, 1) activation layer sigmoid_activation = Activation(func=\u03c3) sigmoid_activation Activation str(sigmoid_activation) 'Layer' sigmoid_activation.id {'id': 140377909680656, 'Layer_id': 140377909680656, 'name': 'Activation', 'self': Activation, 'func': neural_net.activation.\u03c3, 'kargs': (), 'steps': []} Architecture from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3 from neural_net.cost import BinaryCrossEntropy network = Sequential( [ Fullyconnected(2,10,zeros), Activation(\u03c3), Fullyconnected(10,1,zeros), Activation(\u03c3) ] ,BinaryCrossEntropy) repr(network) 'Sequential' str(network) 'Architecture' network['steps'] [Fullyconnected, Activation, Fullyconnected, Activation] network.id.keys() dict_keys(['id', 'Architecture_id', 'name', 'self', 'steps', 'cost', 'store']) network['id'] 140379602988880 network.id['id'] 140379602988880 Adding Database network = Sequential( [ Fullyconnected(2,50,zeros), Activation(\u03c3), Fullyconnected(50,2,zeros), Activation(\u03c3) ] ,BinaryCrossEntropy,store=True) network.session <sqlalchemy.orm.session.Session at 0x7face42132d0> network.db_path 'sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db' utc_ts = network.db_path.split('/')[-1][5:-3] utc_ts '1709575905' import datetime datetime.datetime.fromtimestamp(int(utc_ts)).isoformat() '2024-03-04T18:11:45' db_folder = '/'.join(network.db_path.split('/')[3:-1]) db_folder '/home/analyst/notebooks/module/neural_net/run' %ls $db_folder/*db|tail -n 3 /home/analyst/notebooks/module/neural_net/run/model1709575470.db /home/analyst/notebooks/module/neural_net/run/model1709575647.db /home/analyst/notebooks/module/neural_net/run/model1709575905.db network.engines {'sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db': Engine(sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db)} network.engines.get(network.db_path) Engine(sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db) cursor = network.engines.get(network.db_path).connect() from sqlalchemy import text res = cursor.execute(text(''' SELECT * FROM sqlite_schema ''')) pandas.DataFrame(res.fetchall()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type name tbl_name rootpage sql 0 table Architecture Architecture 2 CREATE TABLE \"Architecture\" (\\n\\tid INTEGER NO... 1 table Layer Layer 3 CREATE TABLE \"Layer\" (\\n\\t\"Architecture_id\" IN... 2 table Cost Cost 4 CREATE TABLE \"Cost\" (\\n\\t\"Architecture_id\" INT... 3 table Neurons Neurons 5 CREATE TABLE \"Neurons\" (\\n\\t\"Layer_id\" INTEGER... 4 table Weight Weight 6 CREATE TABLE \"Weight\" (\\n\\tvalue INTEGER, \\n\\t... Forward Feeding data to network Generating Linearly seperable data n,k = 300,2 X = numpy.random.uniform(-100,100,size=(n,k)) y = (X.sum(axis=1) < numpy.random.uniform(.3,.37,(len(X),))).reshape(-1,1)+0 plt.scatter(x=X[:,0],y=X[:,1],c=y) <matplotlib.collections.PathCollection at 0x7faca51e37d0> Looping over layers for layer in network: print(repr(layer)) Fullyconnected Activation Fullyconnected Activation layer.func \u03c3 out = X for layer in network: out = layer.func.compute(out) out array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) out.shape (300, 2) Using predict method network.predict(X) network.out.shape (300, 2) Exploring database %load_ext sql %sql $network.db_path Architecture %%sql SELECT * FROM Architecture * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. id created_at updated_at name Architecture_id 1 2024-03-04 18:11:45.895299 2024-03-04 18:11:45.895302 Sequential 140379602670032 2 2024-03-04 18:17:26.316307 2024-03-04 18:17:26.316315 Sequential 140379603024400 network['id'] 140379603024400 Costs %%sql SELECT * FROM cost WHERE Architecture_id = 140379603024400 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. Architecture_id value id created_at updated_at name Cost_id 140379603024400 None 2 2024-03-04 18:17:26.591265 2024-03-04 18:17:26.591271 BinaryCrossEntropy 140379601731216 Layers %%sql SELECT * FROM layer WHERE Architecture_id=140379603024400 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. Architecture_id n_in n_out id created_at updated_at name Layer_id 140379603024400 2 50 5 2024-03-04 18:17:26.327160 2024-03-04 18:17:26.327167 Fullyconnected 140379603033872 140379603024400 None None 6 2024-03-04 18:17:26.327168 2024-03-04 18:17:26.513963 Activation 140379602907280 140379603024400 50 2 7 2024-03-04 18:17:26.327169 2024-03-04 18:17:26.547988 Fullyconnected 140379601709072 140379603024400 None None 8 2024-03-04 18:17:26.327170 2024-03-04 18:17:26.572231 Activation 140379601815760 Neurons %%sql SELECT * FROM neurons WHERE layer_id=140379603033872 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. Layer_id id created_at updated_at name Neurons_id 140379603033872 1 2024-03-04 18:17:26.330924 2024-03-04 18:17:26.330930 \u03a3 140379601518480 Weights %%sql SELECT * FROM weight WHERE neurons_id = 140379601518480 LIMIT 10 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. value Neurons_id id created_at updated_at name Weight_id 0 140379601518480 1 2024-03-04 18:17:26.341093 2024-03-04 18:17:26.341100 None 0_0 0 140379601518480 2 2024-03-04 18:17:26.341101 2024-03-04 18:17:26.341102 None 0_1 0 140379601518480 3 2024-03-04 18:17:26.341102 2024-03-04 18:17:26.341103 None 0_2 0 140379601518480 4 2024-03-04 18:17:26.341104 2024-03-04 18:17:26.341104 None 0_3 0 140379601518480 5 2024-03-04 18:17:26.341105 2024-03-04 18:17:26.341105 None 0_4 0 140379601518480 6 2024-03-04 18:17:26.341106 2024-03-04 18:17:26.341106 None 0_5 0 140379601518480 7 2024-03-04 18:17:26.341107 2024-03-04 18:17:26.341107 None 0_6 0 140379601518480 8 2024-03-04 18:17:26.341108 2024-03-04 18:17:26.341108 None 0_7 0 140379601518480 9 2024-03-04 18:17:26.341109 2024-03-04 18:17:26.341109 None 0_8 0 140379601518480 10 2024-03-04 18:17:26.341110 2024-03-04 18:17:26.341110 None 0_9 %%sql SELECT count(*) n_neurons, AVG(value) mean_value FROM weight WHERE neurons_id = 140379601518480 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. n_neurons mean_value 150 0.0 Predict Method network.predict(X) array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) Cost functions from neural_net.cost import BinaryCrossEntropy, CrossEntropy, MSE from neural_net.utils import make_circle_data,IrisDatasetDownloader,HouseDatasetDownloader,Pearson,Boostrap import numpy import matplotlib.pyplot as plt Binary Crossentropy \\mathrm{\\mathit{Binary\\ Cross\\ Entropy}}(p, y) = \\begin{cases} -\\log(p) & \\text{if } y = 1, \\\\ -\\log(1-p) & \\text{otherwise.} \\end{cases} bcost = BinaryCrossEntropy() Circles dataset centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 1) (-100.0, 100.0) dum_classifier = numpy.random.random(len(y)) dum_classifier.shape (328,) bcost.compute(y,dum_classifier) 0.9439193222155523 round(bcost.compute(y,y)) 0 Properties With clipped values( default clip=True) ps = numpy.linspace(0,1,1000).reshape(-1,1) y1 = numpy.array([ [[bcost.compute(numpy.array([1]),p)],bcost.pr()] for p in ps ]) y0 = numpy.array([ [[bcost.compute(numpy.array([0]),p)],bcost.pr()] for p in ps ]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,0],label='estimated probabilities') ax2.set_title('y=0') ax2.plot(ps,y0[:,0],label='estimated probabilities') ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d6a6e3d90> Derivaties fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps,y0[:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d68199a90> fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps[10:-10,:],y1[10:-10,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps[10:-10,:],y0[10:-10:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d680e5f10> Without clippping ps = numpy.linspace(1e-9,1-1e-9,1000).reshape(-1,1) y1 = numpy.array([ [[bcost.compute(numpy.array([1]),p,clip=False)],bcost.pr()] for p in ps ]) y0 = numpy.array([ [[bcost.compute(numpy.array([0]),p,clip=False)],bcost.pr()] for p in ps ]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,0],label='estimated probabilities') ax2.set_title('y=0') ax2.plot(ps,y0[:,0],label='estimated probabilities') ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d67d6a490> Derivaties fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps,y0[:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d680bd010> fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps[10:-10,:],y1[10:-10,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps[10:-10,:],y0[10:-10:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d67b1f6d0> Cross Entropy ce = CrossEntropy() Iris dataset iris = IrisDatasetDownloader() iris.load_dataset() print(iris.description) 1. Title: Iris Plants Database Updated Sept 21 by C.Blake - Added discrepency information 2. Sources: (a) Creator: R.A. Fisher (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) (c) Date: July, 1988 3. Past Usage: - Publications: too many to mention!!! Here are a few. 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. -- Results: -- very low misclassification rates (0% for the setosa class) 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. -- Results: -- very low misclassification rates again 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. 4. Relevant Information: --- This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. --- Predicted attribute: class of iris plant. --- This is an exceedingly simple domain. --- This data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick@espeedaz.net ) The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features. 5. Number of Instances: 150 (50 in each of three classes) 6. Number of Attributes: 4 numeric, predictive attributes and the class 7. Attribute Information: 1. sepal length in cm 2. sepal width in cm 3. petal length in cm 4. petal width in cm 5. class: -- Iris Setosa -- Iris Versicolour -- Iris Virginica 8. Missing Attribute Values: None Summary Statistics: Min Max Mean SD Class Correlation sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) 9. Class Distribution: 33.3% for each of 3 classes. print(iris.data.shape,iris.target.shape) (150, 4) (150, 1) print(iris.target_names) ['setosa', 'versicolor', 'virginica'] print(iris.feature_names) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] print(iris.data[:5,:]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] print(iris.target[:5,:]) [[0] [0] [0] [0] [0]] _, ax = plt.subplots() scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target) ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1]) _ = ax.legend( scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\" ) dum_classifier = numpy.random.random((len(iris.target),3)) dum_classifier/=dum_classifier.sum(axis=1,keepdims=True) print(dum_classifier.shape) dum_classifier[:5,:] (150, 3) array([[0.30771342, 0.63291054, 0.05937604], [0.19948573, 0.3398386 , 0.46067567], [0.42137261, 0.06943003, 0.50919737], [0.08200258, 0.60102031, 0.31697711], [0.27441097, 0.40437459, 0.32121444]]) from neural_net.pipeline import onehot y = onehot(iris.target) print(y.shape) y[:5,:] (150, 3) array([[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]) ce.compute(y,dum_classifier) 1.2920221019539604 round(ce.compute(y,y)) 0 Properties binary case : 2 labels ps = numpy.linspace(1e-9,1-1e-9,1000).reshape(-1,1) y1,pr = [],[] for p in ps: y1.append(ce.compute(numpy.array([[1,0]]),numpy.c_[p,1-p])) pr.append(ce.pr()[0]) fig,ax1 = plt.subplots() plt.title('y=[1,0]') ax1.plot(ps,y1,label='[p0,p1=1-p0]') ax1.legend() <matplotlib.legend.Legend at 0x7f00792e5310> pr=numpy.array(pr) pr.shape (1000, 2) fig,ax1 = plt.subplots() ax1.set_title('derivatives at y=[1,0]') ax1.plot(ps[10:-10,:],pr[10:-10,0],label=r\"$CE'_{p0}$\") ax1.plot(ps[10:-10,:],pr[10:-10,1],label=r\"$CE'_{p1}$\") plt.legend() <matplotlib.legend.Legend at 0x7f0079144dd0> Multimodal case : 3+ labels ps = numpy.linspace(1e-9,1-1e-9,1000).reshape(-1,1) y1,pr = [],[] for p in ps: y1.append(ce.compute(numpy.array([[0,1,0]]),numpy.c_[(1-p)*2/3,p,(1-p)/3])) pr.append(ce.pr()[0]) fig,ax1 = plt.subplots() plt.title('y=[0,1,0]') ax1.plot(ps,y1,label='[p0,p1,p2]') ax1.legend() <matplotlib.legend.Legend at 0x7f0079016210> pr=numpy.array(pr) pr.shape (1000, 3) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,7)) ax1.set_title('derivatives at y=[0,1,0]') ax1.plot(ps[10:-10,:],pr[10:-10,0],label=r\"$CE'_{p0}$\") ax1.plot(ps[10:-10,:],pr[10:-10,1],label=r\"$CE'_{p1}$\") ax1.plot(ps[10:-10,:],pr[10:-10,2],label=r\"$CE'_{p2}$\") ax2.set_title('derivatives at y=[0,1,0]') ax2.plot(ps[10:-10,:],pr[10:-10,0],label=r\"$CE'_{p0}$\") ax2.plot(ps[10:-10,:],pr[10:-10,2],label=r\"$CE'_{p2}$\") plt.legend() <matplotlib.legend.Legend at 0x7f0078ff72d0> Mean Squared Error mse = MSE() Boston Housing housing = HouseDatasetDownloader() housing.load_dataset() print(housing.description) The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, 'Regression diagnostics ...', Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. Variables in order: CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000's print(housing.columns) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] print(housing.data) [[6.3200e-03 1.8000e+01 2.3100e+00 ... 3.9690e+02 4.9800e+00 2.4000e+01] [2.7310e-02 0.0000e+00 7.0700e+00 ... 3.9690e+02 9.1400e+00 2.1600e+01] [2.7290e-02 0.0000e+00 7.0700e+00 ... 3.9283e+02 4.0300e+00 3.4700e+01] ... [6.0760e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 5.6400e+00 2.3900e+01] [1.0959e-01 0.0000e+00 1.1930e+01 ... 3.9345e+02 6.4800e+00 2.2000e+01] [4.7410e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 7.8800e+00 1.1900e+01]] print(housing.data.shape) (506, 14) Correlations fig = plt.figure(figsize=(22,14)) gs = fig.add_gridspec(2,3) ax1 = fig.add_subplot(gs[0, 0]) ax2 = fig.add_subplot(gs[0, 1]) ax3 = fig.add_subplot(gs[0, 2]) ax4 = fig.add_subplot(gs[1, :]) ax1.hist(housing.data[:,-1],label=\"Median value of owner-occupied homes in $1,000's\",bins=10) ax1.legend() ax2.hist(((lp:=numpy.log(housing.data[:,-1]))-lp.mean())/lp.std(),label='Normalized Log Median House Values',bins=15) ax2.legend() corr = Pearson(housing.data,cols=housing.columns) corr.corr() ax4.scatter(x=housing.data[:,-2],y=housing.data[:,-1],label='MEDV by % lower status of the population' ) ax4.set_xlabel(\"LSTAT\") ax4.set_ylabel(\"MEDV\") ax4.legend() corr.heatmap(ax=ax3,digits=2) Ordinary Least Squares X = numpy.c_[housing.data[:,[-2]],numpy.ones((len(housing.data),1))] y = housing.data[:,[-1]] XtX = X.T.dot(X) XTXinv = numpy.linalg.inv(XtX) \u03b2hat = XTXinv.dot(X.T.dot(y)) pred = X.dot(\u03b2hat) \u03b5 = y-pred sigma\u03b5,mean\u03b5 = \u03b5.std(),\u03b5.mean() print(sigma\u03b5,mean\u03b5) Var\u03b2 = sigma\u03b5**2 * XTXinv \u03b2hat 6.20346413142642 -1.842355868215912e-14 array([[-0.95004935], [34.55384088]]) t-test student = \u03b2hat/Var\u03b2.diagonal().reshape(-1,1)**.5 student array([[-24.57651813], [ 61.53688032]]) import scipy.stats p_value = scipy.stats.norm.sf(abs(student)) p_value array([[1.12616355e-133], [0.00000000e+000]]) R2 score SSE = (\u03b5**2).sum() SStot = ((y-y.mean())**2).sum() R2 = 1 - SSE/SStot R2 0.5441462975864797 plt.scatter(x=housing.data[:,-2],y=housing.data[:,-1],label='MEDV by % lower status of the population' ) plt.plot(housing.data[:,-2],pred,label=\"y=$34,000 - \\$950xLSTAT;\\ R^{2}=$\"+f'{R2:.2}') plt.xlabel(\"LSTAT\") plt.ylabel(\"MEDV\") plt.legend() <matplotlib.legend.Legend at 0x7f4d64160450> Computing mse mse = MSE() mse.compute(y,pred) 38.48296722989415 mse.compute(y,y) 0.0 Residual Analysis probs = numpy.linspace(0,1,100) \u03b5quantiles = numpy.quantile(\u03b5,probs) theoratical = numpy.random.normal(loc=mean\u03b5,scale=sigma\u03b5,size=10000) normal_quantiles = numpy.quantile(theoratical,probs) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(22,7)) ax1.scatter(normal_quantiles,\u03b5quantiles,label='Residual quantiles') ax1.plot(normal_quantiles,normal_quantiles,label='theoratical normal') ax1.set_title(\"Normal Q-Q\") ax1.set_xlabel(\"normal quantiles\") ax1.set_ylabel(\"\u03b5 quantiles\") ax2.scatter(pred,\u03b5,label=r\"Residuals\") ax2.set_title(r\"Residuals vs fitted\") ax2.set_xlabel(\"predictions\") ax2.set_ylabel(\"\u03b5\") ax2.legend() <matplotlib.legend.Legend at 0x7f4d54953290> Biais analysis coeffs = [] for (x_new,y_new) in Boostrap((X,y),n_sample=1000): \u03b2_new = numpy.linalg.inv(x_new.T.dot(x_new)).dot(x_new.T.dot(y_new)) coeffs.append(\u03b2_new) coeffs = numpy.concatenate(coeffs,axis=1).T coeffs.mean(axis=0) array([-0.95607794, 34.59180802]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(22,7)) ax1.hist(coeffs[:,0],label=r\"$\\beta_{LSTAT}$\",bins=15) ax1.set_title(r\"$E(\\beta_{LSTAT})=-\\$957$\") ax1.legend() ax2.hist(coeffs[:,1],label=r\"$\\beta_{Intercept}$\",bins=15) ax2.set_title(r\"$E(\\beta_{Intercept})=\\$34,625$\") ax2.legend() <matplotlib.legend.Legend at 0x7f4d545e6610> Properties ps = numpy.linspace(0,200,1000).reshape(-1,1) mses = numpy.array([ [mse.compute(numpy.array([100]),p),mse.pr()[0]] for p in ps ]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,7)) ax1.plot(ps,mses[:,0],label='mse') ax1.set_title('mse for true=100') ax1.legend() ax2.plot(ps,mses[:,1],label=r'$-2(y_{n,1}-p_{n,1})$') ax2.set_title('mse derivative at true=100') ax2.legend() <matplotlib.legend.Legend at 0x7f4d54150950> Backpropagation from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3 from neural_net.cost import BinaryCrossEntropy from neural_net.utils import make_circle_data import numpy import matplotlib.pyplot as plt network = Sequential( [ Fullyconnected(2,10,zeros), Activation(\u03c3), Fullyconnected(10,1,zeros), Activation(\u03c3) ] ,BinaryCrossEntropy,store=True) network['steps'] [Fullyconnected, Activation, Fullyconnected, Activation] n,k = 1000,2 X = numpy.random.uniform(-100,100,size=(n,k)) y = (X.sum(axis=1) < numpy.random.uniform(30,90,(len(X),))).reshape(-1,1)+0 plt.scatter(x=X[:,0],y=X[:,1],c=y) <matplotlib.collections.PathCollection at 0x7f792bffff10> network.predict(X) network.out.shape (1000, 1) network['cost'] BinaryCrossEntropy network['cost'].compute(y,network.out) 0.6931471805599454 \u03940 = network['cost'].pr() \u03940.shape (1000, 1) Output Layer network['steps'][-1] Activation \u03941 = network['steps'][-1].func.grad(\u03940) \u03941.shape (1000, 1) Last Linear Layer network['steps'][-2] Fullyconnected \u03942 = network['steps'][-2].func.grad(\u03941) \u03942.shape (1000, 10) %load_ext sql %sql $network.db_path Update method network.update(\u03940) array([[ 0.34141679, 0.29918978], [ 0.34141679, 0.29918978], [-0.34141679, -0.29918978], ..., [ 0.34141679, 0.29918978], [ 0.34141679, 0.29918978], [ 0.34141679, 0.29918978]]) View weights changes on db network.updateW() network.commit() %%sql SELECT * from weight * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709931963.db Done. value Neurons_id id created_at updated_at name Weight_id -0.9932124687481962 140158405172944 1 2024-03-08 21:06:03.650976 2024-03-08 21:07:12.085321 None 0_0 -0.9932124687481962 140158405172944 2 2024-03-08 21:06:03.650981 2024-03-08 21:07:12.085329 None 0_1 -0.9932124687481962 140158405172944 3 2024-03-08 21:06:03.650983 2024-03-08 21:07:12.085330 None 0_2 -0.9932124687481962 140158405172944 4 2024-03-08 21:06:03.650984 2024-03-08 21:07:12.085332 None 0_3 -0.9932124687481962 140158405172944 5 2024-03-08 21:06:03.650985 2024-03-08 21:07:12.085332 None 0_4 -0.9932124687481962 140158405172944 6 2024-03-08 21:06:03.650985 2024-03-08 21:07:12.085333 None 0_5 -0.9932124687481962 140158405172944 7 2024-03-08 21:06:03.650987 2024-03-08 21:07:12.085333 None 0_6 -0.9932124687481962 140158405172944 8 2024-03-08 21:06:03.650987 2024-03-08 21:07:12.085334 None 0_7 -0.9932124687481961 140158405172944 9 2024-03-08 21:06:03.650988 2024-03-08 21:07:12.085334 None 0_8 -0.9932124687481961 140158405172944 10 2024-03-08 21:06:03.650989 2024-03-08 21:07:12.085335 None 0_9 -0.8703702558942491 140158405172944 11 2024-03-08 21:06:03.650990 2024-03-08 21:07:12.085336 None 1_0 -0.8703702558942491 140158405172944 12 2024-03-08 21:06:03.650991 2024-03-08 21:07:12.085336 None 1_1 -0.8703702558942491 140158405172944 13 2024-03-08 21:06:03.650992 2024-03-08 21:07:12.085337 None 1_2 -0.8703702558942491 140158405172944 14 2024-03-08 21:06:03.650993 2024-03-08 21:07:12.085337 None 1_3 -0.8703702558942491 140158405172944 15 2024-03-08 21:06:03.650994 2024-03-08 21:07:12.085338 None 1_4 -0.8703702558942491 140158405172944 16 2024-03-08 21:06:03.650995 2024-03-08 21:07:12.085339 None 1_5 -0.8703702558942491 140158405172944 17 2024-03-08 21:06:03.650996 2024-03-08 21:07:12.085339 None 1_6 -0.8703702558942491 140158405172944 18 2024-03-08 21:06:03.650997 2024-03-08 21:07:12.085340 None 1_7 -0.8703702558942492 140158405172944 19 2024-03-08 21:06:03.650998 2024-03-08 21:07:12.085340 None 1_8 -0.8703702558942492 140158405172944 20 2024-03-08 21:06:03.650999 2024-03-08 21:07:12.085341 None 1_9 0.01890625000000001 140158405172944 21 2024-03-08 21:06:03.651000 2024-03-08 21:07:12.085341 None 2_0 0.01890625000000001 140158405172944 22 2024-03-08 21:06:03.651001 2024-03-08 21:07:12.085342 None 2_1 0.01890625000000001 140158405172944 23 2024-03-08 21:06:03.651002 2024-03-08 21:07:12.085343 None 2_2 0.01890625000000001 140158405172944 24 2024-03-08 21:06:03.651003 2024-03-08 21:07:12.085343 None 2_3 0.01890625000000001 140158405172944 25 2024-03-08 21:06:03.651004 2024-03-08 21:07:12.085344 None 2_4 0.01890625000000001 140158405172944 26 2024-03-08 21:06:03.651005 2024-03-08 21:07:12.085344 None 2_5 0.01890625000000001 140158405172944 27 2024-03-08 21:06:03.651006 2024-03-08 21:07:12.085345 None 2_6 0.01890625000000001 140158405172944 28 2024-03-08 21:06:03.651006 2024-03-08 21:07:12.085346 None 2_7 0.01890625 140158405172944 29 2024-03-08 21:06:03.651007 2024-03-08 21:07:12.085346 None 2_8 0.01890625 140158405172944 30 2024-03-08 21:06:03.651008 2024-03-08 21:07:12.085347 None 2_9 0.275 140158405279056 31 2024-03-08 21:06:03.651009 2024-03-08 21:07:12.085347 None 0_0 0.275 140158405279056 32 2024-03-08 21:06:03.651010 2024-03-08 21:07:12.085348 None 1_0 0.275 140158405279056 33 2024-03-08 21:06:03.651011 2024-03-08 21:07:12.085348 None 2_0 0.275 140158405279056 34 2024-03-08 21:06:03.651012 2024-03-08 21:07:12.085349 None 3_0 0.275 140158405279056 35 2024-03-08 21:06:03.651013 2024-03-08 21:07:12.085349 None 4_0 0.275 140158405279056 36 2024-03-08 21:06:03.651014 2024-03-08 21:07:12.085350 None 5_0 0.275 140158405279056 37 2024-03-08 21:06:03.651015 2024-03-08 21:07:12.085351 None 6_0 0.275 140158405279056 38 2024-03-08 21:06:03.651016 2024-03-08 21:07:12.085351 None 7_0 0.275 140158405279056 39 2024-03-08 21:06:03.651017 2024-03-08 21:07:12.085352 None 8_0 0.275 140158405279056 40 2024-03-08 21:06:03.651018 2024-03-08 21:07:12.085354 None 9_0 0.55 140158405279056 41 2024-03-08 21:06:03.651019 2024-03-08 21:07:12.085354 None 10_0 Logistic Regression from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3,Softmax from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.utils import make_circle_data import numpy import matplotlib.pyplot as plt centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 1) (-100.0, 100.0) Pure numpy definition H_\u03b8 = lambda X,\u03b8 : 1/(1+numpy.exp(-X.dot(\u03b8))) \u03b8 = numpy.zeros((3,1)) X_const = numpy.c_[X,numpy.ones((len(X),1))] H_\u03b8(X_const,\u03b8).shape (328, 1) Using layers LogReg = Sequential( [ Fullyconnected(2,1,zeros), Activation(\u03c3) ], BinaryCrossEntropy ) LogReg.predict(X).shape (328, 1) LogReg['steps'][-2].func.W array([[0.], [0.], [0.]]) Computing Gradients Analytic gradient J = lambda \u03b8,X : 1/len(X)*X.T.dot(H_\u03b8(X,\u03b8)-y) J(\u03b8,X_const) array([[ -8.2471052 ], [-15.8967726 ], [ -0.07317073]]) Chain rule logistic Regression LogReg[\"cost\"].compute(y,LogReg.out) p0 = LogReg[\"cost\"].pr() p1 = LogReg[\"steps\"][-1].func.grad(p0) p2 = LogReg[\"steps\"][-2].func.grad(p1) -LogReg[\"steps\"][-2].func.W array([[ -8.2471052 ], [-15.8967726 ], [ -0.07317073]]) Softmax Regression( j target class in {1,..,k} and m=1,..,n instances) {\\displaystyle \\sigma (\\mathbf {z} )_{j}={\\frac {\\mathrm {e} ^{z_{j}}}{\\sum _{i=1}^{K}\\mathrm {e} ^{z_{i}}}}} Analytical gradient from neural_net.pipeline import onehot y_one_hot = onehot(y) y_one_hot[:5,:] array([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]) W = numpy.zeros((X.shape[1]+1,2)) W array([[0., 0.], [0., 0.], [0., 0.]]) W = numpy.zeros((X.shape[1],2)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) Sm(W,X)[:5,:] array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) J = 1/len(X)*X.T.dot(Sm(W,X)-y_one_hot) J array([[ 8.2471052, -8.2471052], [ 15.8967726, -15.8967726]]) Chain rule softmax = Sequential( [ Fullyconnected(2,2,zeros), Activation(Softmax) ], CrossEntropy ) softmax.predict(X)[:5,:] array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) softmax[\"cost\"].compute(y_one_hot,softmax.out) 0.6931471805599453 softmax[\"cost\"].compute(y_one_hot,softmax.out) _ = softmax.update(softmax[\"cost\"].pr()) -softmax[\"steps\"][-2].func.W array([[ -8.2471052 , 8.2471052 ], [-15.8967726 , 15.8967726 ], [ -0.07317073, 0.07317073]]) Training from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3,Softmax from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.utils import make_circle_data,IrisDatasetDownloader from neural_net.metrics import accuracy from neural_net.pipeline import onehot import numpy import matplotlib.pyplot as plt centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 1) (-100.0, 100.0) Logistic Regression n_epoch = 1000 \u03b1 = 0.1 Using analytical gradient H_\u03b8 = lambda X,\u03b8 : 1/(1+numpy.exp(-X.dot(\u03b8))) \u03b8 = numpy.zeros((3,1)) X_const = numpy.c_[X,numpy.ones((len(X),1))] J = lambda \u03b8,X : 1/len(X)*X.T.dot(H_\u03b8(X,\u03b8)-y) for _ in range(n_epoch): \u03b8 -= \u03b1*J(\u03b8,X_const) \u03b8 array([[1.19524694], [1.47756754], [0.05197856]]) pred = (H_\u03b8(X_const,\u03b8) > .5 )+0 plt.scatter(x=X[:,0],y=X[:,1],c=pred) <matplotlib.collections.PathCollection at 0x7f2c4820df90> Using chain rule LogReg = Sequential( [ Fullyconnected(2,1,zeros), Activation(\u03c3) ], BinaryCrossEntropy ) for _ in range(n_epoch): LogReg.predict(X) LogReg[\"cost\"].compute(y,LogReg.out) LogReg[\"cost\"].compute(y,LogReg.out) _ = LogReg.update(\u03b1*LogReg[\"cost\"].pr()) LogReg['steps'][-2].func.W array([[1.19524694], [1.47756754], [0.05197856]]) p = (LogReg.out>.5)+0 plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f2c481c2b90> Using train method LogReg = Sequential( [ Fullyconnected(2,1,zeros), Activation(\u03c3) ], BinaryCrossEntropy ) LogReg.train(X,y,epochs=n_epoch,\u03b1=\u03b1,metrics=accuracy) BinaryCrossEntropy 0.0002 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 298.57it/s] LogReg['steps'][-2].func.W array([[1.19524694], [1.47756754], [0.05197856]]) Softmax 2 labels from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3,Softmax from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.utils import make_circle_data,IrisDatasetDownloader from neural_net.metrics import accuracy from neural_net.pipeline import onehot import numpy import matplotlib.pyplot as plt centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) y = onehot(y) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y.argmax(axis=1)) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 2) (-100.0, 100.0) n_epoch = 1000 \u03b1 = 0.1 Analytically n,k=X.shape j=y.shape[1] W = numpy.zeros((k+1,j)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) J_W = lambda W,X : 1/n*X.T.dot(Sm(W,X)-y) for _ in range(n_epoch): W -= \u03b1*J_W(W,numpy.c_[X,numpy.ones((n,1))] ) W array([[-1.07789341, 1.07789341], [-1.56804297, 1.56804297], [-0.02426586, 0.02426586]]) p = Sm(W,numpy.c_[X,numpy.ones((n,1))]).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f0083e3af50> using chain rule softmax = Sequential( [ Fullyconnected(k,j,zeros), Activation(Softmax) ], CrossEntropy ) softmax.train(X,y,epochs=n_epoch,\u03b1=\u03b1,metrics=accuracy) CrossEntropy 0.0001 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 282.21it/s] softmax['steps'][-2].func.W array([[-1.07789341, 1.07789341], [-1.56804297, 1.56804297], [-0.02426586, 0.02426586]]) p = softmax.predict(X).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f0083de2e90> 3 labels iris = IrisDatasetDownloader() iris.load_dataset() _, ax = plt.subplots() scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target) ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1]) _ = ax.legend( scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\" ) X,y = iris.data,onehot(iris.target) n,k = X.shape j = y.shape[1] X.shape,y.shape ((150, 4), (150, 3)) Using Softmax analytical solution n_epoch=1000 W = numpy.zeros((k+1,j)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) J_W = lambda W,X : 1/n*X.T.dot(Sm(W,X)-y) for _ in range(n_epoch): W -= \u03b1*J_W(W,numpy.c_[X,numpy.ones((n,1))]) W array([[ 0.88907029, 0.7230017 , -1.612072 ], [ 2.05504822, -0.20042131, -1.85462692], [-2.81867225, -0.13349788, 2.95217013], [-1.32033636, -1.14715458, 2.46749093], [ 0.42390714, 0.65830856, -1.0822157 ]]) p = Sm(W,numpy.c_[X,numpy.ones((n,1))]).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f008403af50> accuracy().compute(y,Sm(W,numpy.c_[X,numpy.ones((n,1))])) 0.9867 Chain rule softmax = Sequential( [ Fullyconnected(k,j,zeros), Activation(Softmax) ], CrossEntropy ) softmax.train(X,y,epochs=n_epoch,\u03b1=\u03b1,metrics=accuracy) CrossEntropy 0.126 accuracy 0.9867: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 318.95it/s] p = softmax.predict(X).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f0080252e90> Non Linear Problems import numpy n,k = 1500,2 X = numpy.random.uniform(-100,100,size=(n,k)) y =( (X[:, 0]**2 + X[:, 1]**2) < 3000).reshape(-1,1)+0 y_one_hot = onehot(y) plt.scatter(x=X[:,0],y=X[:,1],c=y) <matplotlib.collections.PathCollection at 0x7f0d3d72b510> Beyond linear architecture n_epoch = 1000 \u03b1 = 0.2 Logistic regression H_\u03b8 = lambda X,\u03b8 : 1/(1+numpy.exp(-X.dot(\u03b8))) \u03b8 = numpy.zeros((3,1)) X_const = numpy.c_[X,numpy.ones((len(X),1))] J = lambda \u03b8,X : 1/len(X)*X.T.dot(H_\u03b8(X,\u03b8)-y) for _ in range(n_epoch): \u03b8 -= \u03b1*J(\u03b8,X_const) pred = (H_\u03b8(X_const,\u03b8) > .5 )+0 (pred==y).sum()/len(y) 0.49066666666666664 plt.scatter(x=X[:,0],y=X[:,1],c=pred) <matplotlib.collections.PathCollection at 0x7f00797c7e10> Softmax y_one_hot = onehot(y) n,j= y_one_hot.shape (n,j) (1500, 2) W = numpy.zeros((k+1,j)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) J_W = lambda W,X : 1/n*X.T.dot(Sm(W,X)-y_one_hot) for _ in range(n_epoch): W -= \u03b1*J_W(W,X_const) p = Sm(W,X_const).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f007949a8d0> Neural network from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros,XavierHe from neural_net.activation import \u03c3,Softmax,LeakyReLU,Tanh,ELU,ReLU from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.metrics import accuracy from neural_net.pipeline import onehot,scaler,shuffle,Batch from neural_net.utils import IrisDatasetDownloader import numpy import matplotlib.pyplot as plt Xavier and He Initialization methods We don\u2019t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs NN = Sequential( [ Fullyconnected(2,50,XavierHe(\"Uniform\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(50,1,XavierHe(\"Uniform\",\"Sigmoid\").init_func), Activation(\u03c3) ], BinaryCrossEntropy ) NN.train(scaler(X),y,\u03b1=\u03b1,epochs=n_epoch,metrics=accuracy) BinaryCrossEntropy 0.0721 accuracy 0.994: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:17<00:00, 56.29it/s] pred = (NN.predict(scaler(X))>.5)+0 pred array([[0], [1], [0], ..., [0], [0], [0]]) plt.scatter(x=X[:,0],y=X[:,1],c=pred) <matplotlib.collections.PathCollection at 0x7f0d3b510590> NN.train(scaler(X),y,\u03b1=\u03b1,epochs=n_epoch,metrics=accuracy) BinaryCrossEntropy 0.0463 accuracy 0.996: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:19<00:00, 51.83it/s] Iris Problem iris = IrisDatasetDownloader() iris.load_dataset() y = onehot(iris.target) X = iris.data X,y = shuffle(X,y) n,k = X.shape j = y.shape[1] NN = Sequential( [ Fullyconnected(k,100,XavierHe(\"Uniform\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(100,50,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(ELU), Fullyconnected(50,j,zeros), Activation(Softmax) ], CrossEntropy ) batch = Batch(60,len(X),lambda : scaler(X), lambda : y) NN.train(batch=batch,\u03b1=0.2,epochs=1000,metrics=accuracy) CrossEntropy 0.001 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:32<00:00, 30.43it/s] pred = NN.predict(scaler(X)).argmax(axis=1,keepdims=True) true_y = y.argmax(axis=1,keepdims=True) (pred == true_y).sum()/len(pred) 1.0 Storing weights from neural_net.model import Define Define._Define__store = True NN.updateW() NN.commit() NN.db_path 'sqlite:////home/analyst/notebooks/module/neural_net/run/model1709981118.db' from sqlalchemy import text import pandas cursor = NN.engines.get(NN.db_path).connect() res = cursor.execute(text(''' SELECT * FROM Weight ''')) pandas.DataFrame(res.fetchall()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value Neurons_id id created_at updated_at name Weight_id 0 -0.003729 139694848570384 1 2024-03-09 11:08:51.902537 2024-03-09 11:08:51.902543 None 0_0 1 -0.384036 139694848570384 2 2024-03-09 11:08:51.902544 2024-03-09 11:08:51.902545 None 0_1 2 0.033767 139694848570384 3 2024-03-09 11:08:51.902545 2024-03-09 11:08:51.902546 None 0_2 3 0.265035 139694848570384 4 2024-03-09 11:08:51.902546 2024-03-09 11:08:51.902547 None 0_3 4 0.261001 139694848570384 5 2024-03-09 11:08:51.902547 2024-03-09 11:08:51.902548 None 0_4 ... ... ... ... ... ... ... ... 196 -0.207245 139694847460432 197 2024-03-09 11:08:51.902896 2024-03-09 11:08:51.902896 None 46_0 197 -0.108507 139694847460432 198 2024-03-09 11:08:51.902896 2024-03-09 11:08:51.902897 None 47_0 198 -0.229139 139694847460432 199 2024-03-09 11:08:51.902897 2024-03-09 11:08:51.902898 None 48_0 199 0.260771 139694847460432 200 2024-03-09 11:08:51.902899 2024-03-09 11:08:51.902899 None 49_0 200 0.265368 139694847460432 201 2024-03-09 11:08:51.902900 2024-03-09 11:08:51.902900 None 50_0 201 rows \u00d7 7 columns Optical character recognition(OCR) Hand written dataset from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report,ConfusionMatrixDisplay digits = datasets.load_digits() _, axes = plt.subplots(nrows=1, ncols=10, figsize=(13, 3)) for ax, image, label in zip(axes, digits.images, digits.target): ax.set_axis_off() ax.imshow(image) ax.set_title(\"Training: %i\" % label) n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) ## Split data into 50% train and 50% test subsets X_train, X_test, y_train, y_test = train_test_split( data, onehot(digits.target.reshape(-1,1)), test_size=0.5, shuffle=False ) X_train.shape,y_train.shape ((898, 64), (898, 10)) Softmax softmax = Sequential( [ Fullyconnected(64,10,zeros), Activation(Softmax) ], CrossEntropy ) softmax.train(X_train,y_train,epochs=1000,\u03b1=0.001,metrics=accuracy) CrossEntropy 0.1366 accuracy 0.9811: 100%|==========| 1000/1000 [00:25<00:00, 39.08it/s] predicted = softmax.predict(X_test).argmax(axis=1) _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3)) for ax, image, prediction in zip(axes, X_test, predicted): ax.set_axis_off() image = image.reshape(8, 8) ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\") ax.set_title(f\"Prediction: {prediction}\") print( f\"Classification report for classifier :\\n\" f\"{classification_report(y_test.argmax(axis=1), predicted)}\\n\" ) Classification report for classifier : precision recall f1-score support 0 1.00 0.97 0.98 88 1 0.93 0.84 0.88 91 2 1.00 0.97 0.98 86 3 0.94 0.85 0.89 91 4 0.97 0.91 0.94 92 5 0.89 0.93 0.91 91 6 0.93 0.99 0.96 91 7 0.96 0.99 0.97 89 8 0.89 0.90 0.89 88 9 0.83 0.96 0.89 92 accuracy 0.93 899 macro avg 0.93 0.93 0.93 899 weighted avg 0.93 0.93 0.93 899 disp = ConfusionMatrixDisplay.from_predictions(y_test.argmax(axis=1), predicted) disp.figure_.suptitle(\"Confusion Matrix\") print(f\"Confusion matrix:\\n{disp.confusion_matrix}\") plt.show() Confusion matrix: [[85 0 0 0 1 1 1 0 0 0] [ 0 76 0 1 2 1 1 0 0 10] [ 0 0 83 3 0 0 0 0 0 0] [ 0 2 0 77 0 3 0 4 5 0] [ 0 0 0 0 84 0 4 0 3 1] [ 0 0 0 0 0 85 1 0 0 5] [ 0 1 0 0 0 0 90 0 0 0] [ 0 0 0 0 0 0 0 88 1 0] [ 0 3 0 0 0 4 0 0 79 2] [ 0 0 0 1 0 2 0 0 1 88]] Deep Learning NN = Sequential( [ Fullyconnected(64,1000,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(ELU), Fullyconnected(1000,100,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(ELU), Fullyconnected(100,10,XavierHe(\"Normal\",\"Sigmoid\").init_func), Activation(Softmax) ], CrossEntropy ) X_train,y_train = shuffle(X_train,y_train) batch = Batch(10,len(X_train),lambda : X_train/16, lambda : y_train) NN.train(batch=batch,\u03b1=0.014,epochs=100,metrics=accuracy) CrossEntropy 0.0117 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:42<00:00, 2.35it/s] predicted = NN.predict(X_test/16).argmax(axis=1) _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3)) for ax, image, prediction in zip(axes, X_test, predicted): ax.set_axis_off() image = image.reshape(8, 8) ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\") ax.set_title(f\"Prediction: {prediction}\") print( f\"Classification report for classifier :\\n\" f\"{classification_report(y_test.argmax(axis=1), predicted)}\\n\" ) Classification report for classifier : precision recall f1-score support 0 1.00 0.98 0.99 88 1 0.95 0.91 0.93 91 2 0.99 1.00 0.99 86 3 0.97 0.86 0.91 91 4 0.98 0.92 0.95 92 5 0.91 0.95 0.92 91 6 0.95 0.99 0.97 91 7 0.96 0.96 0.96 89 8 0.92 0.92 0.92 88 9 0.85 0.97 0.90 92 accuracy 0.94 899 macro avg 0.95 0.94 0.94 899 weighted avg 0.95 0.94 0.94 899 Mnist from keras.datasets import mnist 2024-03-09 13:59:33.278357: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2024-03-09 13:59:33.373835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. (X_train, Y_train), (X_test, Y_test) = mnist.load_data() num = 10 images = X_train[:num] labels = Y_train[:num] num_row = 2 num_col = 5 ## plot images fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row)) for i in range(num): ax = axes[i//num_col, i%num_col] ax.imshow(images[i], cmap='gray') ax.set_title('Label: {}'.format(labels[i])) plt.tight_layout() plt.show() X_train = X_train.reshape((X_train.shape[0], 28*28)).astype('float32') X_test = X_test.reshape((X_test.shape[0], 28*28)).astype('float32') X_train /= 255 X_test /= 255 X_train.shape (60000, 784) Y_train = onehot(Y_train.reshape(-1,1)) Y_train.shape, Y_test.shape ((60000, 10), (10000,)) n_inputs = 28 * 28 n_hidden1 = 300 n_hidden2 = 100 n_outputs = 10 NN = Sequential( [ Fullyconnected(n_inputs,n_hidden1,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(n_hidden1,n_hidden2,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(n_hidden2,n_outputs,XavierHe(\"Normal\",\"Sigmoid\").init_func), Activation(Softmax) ], CrossEntropy ) batch = Batch(500,len(X_train),lambda : X_train, lambda : Y_train) NN.train(batch=batch,\u03b1=0.014,epochs=100,metrics=accuracy) CrossEntropy 0.129 accuracy 0.976: 100%|==========| 100/100 [10:28<00:00, 6.28s/it] pred = NN.predict(X_test).argmax(axis=1) pred array([7, 2, 1, ..., 4, 5, 6]) print( f\"Classification report for classifier :\\n\" f\"{classification_report(Y_test, pred)}\\n\" ) Classification report for classifier : precision recall f1-score support 0 0.98 0.99 0.98 980 1 0.98 0.99 0.98 1135 2 0.97 0.97 0.97 1032 3 0.96 0.97 0.96 1010 4 0.97 0.97 0.97 982 5 0.95 0.96 0.96 892 6 0.97 0.97 0.97 958 7 0.97 0.96 0.96 1028 8 0.96 0.96 0.96 974 9 0.97 0.95 0.96 1009 accuracy 0.97 10000 macro avg 0.97 0.97 0.97 10000 weighted avg 0.97 0.97 0.97 10000","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"tutorials/#install-nnn-library-from-testpypi-repo-link-here","text":"","title":"Install  NNN library from TestPypi repo link here"},{"location":"tutorials/#check-python-version-requirement-python-38","text":"$ python --version Python 3.11.7 $ pip install --upgrade pip $ pip install -i https://test.pypi.org/simple/ neural-net-numpy Requirement already satisfied: pip in /home/analyst/dlenv/lib/python3.11/site-packages (24.0) Looking in indexes: https://test.pypi.org/simple/ Collecting neural-net-numpy==0.1.4 Downloading https://test-files.pythonhosted.org/packages/48/77/4d5e4d9de3f9bd758dd510a2d9a3dfb0566f3c90dcd8e40d81e3af815ef4/neural_net_numpy-0.1.4-py3-none-any.whl.metadata (1.8 kB) Requirement already satisfied: tqdm>=4.66.2 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (4.66.2) Requirement already satisfied: numpy>=1.26.4 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (1.26.4) Requirement already satisfied: SQLAlchemy>=2.0.27 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (2.0.27) Requirement already satisfied: pandas>=2.2.0 in /home/analyst/dlenv/lib/python3.11/site-packages (from neural-net-numpy==0.1.4) (2.2.0) Requirement already satisfied: python-dateutil>=2.8.2 in /home/analyst/dlenv/lib/python3.11/site-packages (from pandas>=2.2.0->neural-net-numpy==0.1.4) (2.8.2) Requirement already satisfied: pytz>=2020.1 in /home/analyst/dlenv/lib/python3.11/site-packages (from pandas>=2.2.0->neural-net-numpy==0.1.4) (2024.1) Requirement already satisfied: tzdata>=2022.7 in /home/analyst/dlenv/lib/python3.11/site-packages (from pandas>=2.2.0->neural-net-numpy==0.1.4) (2024.1) Requirement already satisfied: typing-extensions>=4.6.0 in /home/analyst/dlenv/lib/python3.11/site-packages (from SQLAlchemy>=2.0.27->neural-net-numpy==0.1.4) (4.9.0) Requirement already satisfied: greenlet!=0.4.17 in /home/analyst/dlenv/lib/python3.11/site-packages (from SQLAlchemy>=2.0.27->neural-net-numpy==0.1.4) (3.0.3) Requirement already satisfied: six>=1.5 in /home/analyst/dlenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->neural-net-numpy==0.1.4) (1.16.0) Downloading https://test-files.pythonhosted.org/packages/48/77/4d5e4d9de3f9bd758dd510a2d9a3dfb0566f3c90dcd8e40d81e3af815ef4/neural_net_numpy-0.1.4-py3-none-any.whl (16 kB) Installing collected packages: neural-net-numpy Successfully installed neural-net-numpy-0.1.4","title":"Check python version (requirement python &gt;=3.8)"},{"location":"tutorials/#check-install","text":"$ python -c \"import neural_net;print(neural_net.__version__)\" 0.1.0","title":"Check install"},{"location":"tutorials/#import-data-science-libraries","text":"import numpy,pandas import matplotlib.pyplot as plt","title":"Import data science libraries"},{"location":"tutorials/#activation-functions","text":"z = numpy.linspace(-6,6,1000+1)","title":"Activation functions"},{"location":"tutorials/#sigmoid-and-tanh","text":"","title":"Sigmoid and Tanh"},{"location":"tutorials/#function-values","text":"from neural_net.activation import \u03c3,Tanh sigmoid = \u03c3() tanh = Tanh() sigmoid.compute(z) array([0.00247262, 0.0025024 , 0.00253253, ..., 0.99746747, 0.9974976 , 0.99752738]) tanh.compute(z) array([-1.99997542, -1.99997483, -1.99997421, ..., 1.99997421, 1.99997483, 1.99997542]) sigmoid.preds array([0.00247262, 0.0025024 , 0.00253253, ..., 0.99746747, 0.9974976 , 0.99752738]) tanh.preds array([-1.99997542, -1.99997483, -1.99997421, ..., 1.99997421, 1.99997483, 1.99997542]) fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,sigmoid.preds,label=r'$\\sigma=\\frac{1}{1+e^{-z}}$') ax2.plot(z,tanh.preds,label='$2\\sigma(2z) - 1$') ax1.hlines(y=0.5,xmin=-5,xmax=5,color='green',label=r'$y=\\frac{1}{2}$',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',label=r'$y=0$',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Sigmoid') ax2.set_title('Hyperbolic Tangent Function') ax1.grid() ax2.grid()","title":"Function values"},{"location":"tutorials/#derivatives","text":"sigmoidpr = sigmoid.pr() tanhpr = tanh.pr() fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,sigmoidpr,label=r'$\\sigma(1-\\sigma)$') ax2.plot(z,tanhpr,label='$1 - tanh^2$') ax1.hlines(y=0.25,xmin=65,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Sigmoid Derivative') ax2.set_title('Hyperbolic Tangent Function Derivative') ax1.grid() ax2.grid()","title":"Derivatives"},{"location":"tutorials/#rectified-linear-unit-relu","text":"","title":"Rectified Linear Unit (ReLU)"},{"location":"tutorials/#function-values_1","text":"\\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ % & is your \"\\tab\" 0 & \\text{if } z < 0 \\end{cases} from neural_net.activation import ReLU relu = ReLU() relupred = relu.compute(z) relupred array([0. , 0. , 0. , ..., 5.976, 5.988, 6. ]) fig,ax1 = plt.subplots(nrows=1,ncols=1,figsize=(7.5,6)) ax1.plot(z,relupred,label=r'ReLU') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax1.legend() ax1.set_title('Rectified Linear Unit') ax1.grid()","title":"Function values"},{"location":"tutorials/#derivative","text":"\\mathrm{\\mathit{H}}(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases} relupr = relu.pr() relupr array([0, 0, 0, ..., 1, 1, 1]) fig,ax1 = plt.subplots(nrows=1,ncols=1,figsize=(7.5,6)) ax1.plot(z,relupr,label=r'ReLU') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax1.legend() ax1.set_title('Rectified Linear Unit Derivative') ax1.grid()","title":"Derivative"},{"location":"tutorials/#non-saturating-activations","text":"","title":"Non Saturating activations"},{"location":"tutorials/#function-values_2","text":"Leaky Rectified Linear Unit(Leaky ReLU) \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ \\alpha z & \\text{if } z < 0 \\end{cases} Exponential Linear Unit(ELU) \\mathrm{\\mathit{H}}(z) = \\begin{cases} z & \\text{if } z \\geq 0 \\\\ \\alpha (e^{z} - 1) & \\text{if } z < 0 \\end{cases} from neural_net.activation import ELU,LeakyReLU elu = ELU(\u03b1=1) leakyrelu = LeakyReLU(\u03b1=.1) elupred,leakyrelupred = elu.compute(z),leakyrelu.compute(z) fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,elupred,label=r'$\\alpha=1$') ax2.plot(z,leakyrelupred,label=r'$\\alpha=0.1$') ax1.hlines(y=0,xmin=-5,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Exponential Linear Unit') ax2.set_title('Leaky Rectified Linear Unit') ax1.grid() ax2.grid()","title":"Function values"},{"location":"tutorials/#derivatives_1","text":"ELU \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\mathrm{\\mathit{H}}(z) + \\alpha & \\text{if } z < 0 \\end{cases} Leaky ReLU \\mathrm{\\mathit{H}}'(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ % & \\alpha & \\text{if } z < 0 \\end{cases} elupr,leakypr = elu.pr(),leakyrelu.pr() fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,elupr,label=r'$\\alpha=1$') ax2.plot(z,leakypr,label=r'$\\alpha=0.1$') ax1.hlines(y=0,xmin=-5,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Exponential Linear Unit Derivative') ax2.set_title('Leaky Rectified Linear Unit Derivative') ax1.grid() ax2.grid()","title":"Derivatives"},{"location":"tutorials/#all-common-activation-function-and-their-derivatives","text":"fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,6)) ax1.plot(z,elu.preds,label=r'$ELU(\\alpha=1)$') ax1.plot(z,leakyrelu.preds,label=r'$Leaky\\ ReLU(\\alpha=0.1)$') ax1.plot(z,relu.preds,label=r'ReLU') ax1.plot(z,sigmoid.preds,label=r'$\\sigma=\\frac{1}{1+e^{-z}}$') ax1.plot(z,tanh.preds,label='$tanh=2\\sigma(2z) - 1$') ax2.plot(z,elupr,label=r'$ELU(\\alpha=1)$') ax2.plot(z,leakypr,label=r'$Leaky\\ ReLU(\\alpha=0.1$)') ax2.plot(z,relupr,label=r'ReLU') ax2.plot(z,sigmoidpr,label=r\"$\\sigma'=\\sigma(1-\\sigma)$\") ax2.plot(z,tanhpr,label='$tanh=1 - tanh^2$') ax1.hlines(y=0,xmin=-5,xmax=5,color='green',linestyle='--') ax1.vlines(x=0,ymin=-5,ymax=5,color='brown',linestyle='--') ax2.hlines(y=0,xmin=-5,xmax=5,color='red',linestyle='--') ax1.set_ylim(-5,5) ax1.set_xlim(-5,5) ax2.set_ylim(-5,5) ax2.set_xlim(-5,5) ax1.legend() ax2.legend() ax1.set_title('Common activation functions') ax2.set_title('Derivatives') ax1.grid() ax2.grid()","title":"All common activation function and their derivatives"},{"location":"tutorials/#initialization","text":"from neural_net.init_funcs import XavierHe, zeros","title":"Initialization"},{"location":"tutorials/#weights-bias","text":"n_cols = 2","title":"Weights + Bias"},{"location":"tutorials/#zeros","text":"W = zeros(n_cols,1) W array([[0.], [0.], [0.]]) W = zeros(n_cols,1,biais=False) W array([[0.], [0.]])","title":"Zeros"},{"location":"tutorials/#xavier-and-he","text":"initializer = XavierHe(\"Normal\",\"Sigmoid\").init_func initializer(n_cols,1) array([[ 1.08529782], [ 0.55315106], [-1.20349346]]) initializer(n_cols,1,biais=False) array([[0.14914699], [0.68662846]])","title":"Xavier and He"},{"location":"tutorials/#normal-distribution","text":"n_cols = 1000 xe_norm_sigmoid = XavierHe(\"Normal\",\"Sigmoid\").init_func(n_cols,1,biais=False) xe_norm_tanh = XavierHe(\"Normal\",\"Tanh\").init_func(n_cols,1,biais=False) xe_norm_relu = XavierHe(\"Normal\",\"ReLU\").init_func(n_cols,1,biais=False) xe_norm_sigmoid.shape (1000, 1) std,m = xe_norm_sigmoid.std(),xe_norm_sigmoid.mean() std,m (0.04489498522857033, -0.0007231213204853488) (( xe_norm_sigmoid >= m-std) & (xe_norm_sigmoid<=m+std)).sum() 697 697/n_cols 0.697 (( xe_norm_sigmoid >= m-2*std) & (xe_norm_sigmoid<=m+2*std)).sum()/n_cols 0.95 (( xe_norm_sigmoid >= m-3*std) & (xe_norm_sigmoid<=m+3*std)).sum()/n_cols 0.997 xe_norm_tanh.std(),xe_norm_relu.std() (0.1711681672411125, 0.06410140383405528) plt.figure(figsize=(10,6)) plt.title('Xavier/He for normal distribution by activation type') plt.hist(xe_norm_tanh,label=r'Tanh $std=4\\sigma$',bins=30,alpha=.4) plt.hist(xe_norm_relu,label=r'ReLU $std=\\sqrt{2}\\sigma$',bins=30,alpha=.5) plt.hist(xe_norm_sigmoid,label=r'Sigmoid $std=\\sigma=\\sqrt{\\frac{2}{n_{in}+n_{out}}}$',bins=30,alpha=.6) plt.legend(loc='upper left') plt.axis('off') (-0.726385463544662, 0.6217518101350396, 0.0, 111.3)","title":"Normal distribution"},{"location":"tutorials/#uniform-distribution","text":"xe_uni_sigmoid = XavierHe(\"Uniform\",\"Sigmoid\").init_func(n_cols,1,biais=False) xe_uni_tanh = XavierHe(\"Uniform\",\"Tanh\").init_func(n_cols,1,biais=False) xe_uni_relu = XavierHe(\"Uniform\",\"ReLU\").init_func(n_cols,1,biais=False) xe_uni_sigmoid.min() -0.07708092487365578 xe_uni_sigmoid.max() 0.0770798990273792 plt.figure(figsize=(10,6)) plt.title('Xavier/He for uniform distribution by activation type') plt.hist(xe_uni_tanh,label=r'Tanh $r=4m$',bins=25,alpha=.4) plt.hist(xe_uni_relu,label=r'ReLU $r=\\sqrt{2}m$',bins=25,alpha=.5) plt.hist(xe_uni_sigmoid,label=r'Sigmoid $r=m=\\sqrt{\\frac{2}{n_{in}+n_{out}}}$',bins=25,alpha=.6) plt.legend(loc='upper left') <matplotlib.legend.Legend at 0x7fac479fe350>","title":"Uniform distribution"},{"location":"tutorials/#layers","text":"from neural_net.layers import Fullyconnected,Activation","title":"Layers"},{"location":"tutorials/#linear-layer","text":"fc = Fullyconnected(n_in=2,n_out=1,init_method=zeros) repr(fc) 'Fullyconnected' str(fc) 'Layer' fc.id {'id': 140377909934032, 'Layer_id': 140377909934032, 'name': 'Fullyconnected', 'self': Fullyconnected, 'n_in': 2, 'n_out': 1, 'init_method': <function neural_net.init_funcs.zeros(n_in: int, n_out: int, biais: bool = True) -> <built-in function array>>, 'func': neural_net.activation.\u03a3, 'steps': []} fc['id'] 140377909934032 fc.id['id'] 140377909934032 fc.func \u03a3","title":"Linear Layer"},{"location":"tutorials/#linear-activation","text":"linear_activation = fc.func linear_activation \u03a3 str(linear_activation) 'Neurons' repr(linear_activation) '\u03a3' linear_activation.id {'id': 140377909866128, 'Neurons_id': 140377909866128, 'name': '\u03a3', 'self': \u03a3, 'Layer_id': 140377909934032, 'Layer_Layer_id': 140377909934032, 'Layer_name': 'Fullyconnected', 'Layer_self': Fullyconnected, 'Layer_n_in': 2, 'Layer_n_out': 1, 'Layer_init_method': <function neural_net.init_funcs.zeros(n_in: int, n_out: int, biais: bool = True) -> <built-in function array>>, 'Layer_func': neural_net.activation.\u03a3, 'steps': []} linear_activation.W array([[0.], [0.], [0.]]) linear_activation.W.shape (3, 1)","title":"Linear activation"},{"location":"tutorials/#activation-layer","text":"sigmoid_activation = Activation(func=\u03c3) sigmoid_activation Activation str(sigmoid_activation) 'Layer' sigmoid_activation.id {'id': 140377909680656, 'Layer_id': 140377909680656, 'name': 'Activation', 'self': Activation, 'func': neural_net.activation.\u03c3, 'kargs': (), 'steps': []}","title":"activation layer"},{"location":"tutorials/#architecture","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3 from neural_net.cost import BinaryCrossEntropy network = Sequential( [ Fullyconnected(2,10,zeros), Activation(\u03c3), Fullyconnected(10,1,zeros), Activation(\u03c3) ] ,BinaryCrossEntropy) repr(network) 'Sequential' str(network) 'Architecture' network['steps'] [Fullyconnected, Activation, Fullyconnected, Activation] network.id.keys() dict_keys(['id', 'Architecture_id', 'name', 'self', 'steps', 'cost', 'store']) network['id'] 140379602988880 network.id['id'] 140379602988880","title":"Architecture"},{"location":"tutorials/#adding-database","text":"network = Sequential( [ Fullyconnected(2,50,zeros), Activation(\u03c3), Fullyconnected(50,2,zeros), Activation(\u03c3) ] ,BinaryCrossEntropy,store=True) network.session <sqlalchemy.orm.session.Session at 0x7face42132d0> network.db_path 'sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db' utc_ts = network.db_path.split('/')[-1][5:-3] utc_ts '1709575905' import datetime datetime.datetime.fromtimestamp(int(utc_ts)).isoformat() '2024-03-04T18:11:45' db_folder = '/'.join(network.db_path.split('/')[3:-1]) db_folder '/home/analyst/notebooks/module/neural_net/run' %ls $db_folder/*db|tail -n 3 /home/analyst/notebooks/module/neural_net/run/model1709575470.db /home/analyst/notebooks/module/neural_net/run/model1709575647.db /home/analyst/notebooks/module/neural_net/run/model1709575905.db network.engines {'sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db': Engine(sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db)} network.engines.get(network.db_path) Engine(sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db) cursor = network.engines.get(network.db_path).connect() from sqlalchemy import text res = cursor.execute(text(''' SELECT * FROM sqlite_schema ''')) pandas.DataFrame(res.fetchall()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type name tbl_name rootpage sql 0 table Architecture Architecture 2 CREATE TABLE \"Architecture\" (\\n\\tid INTEGER NO... 1 table Layer Layer 3 CREATE TABLE \"Layer\" (\\n\\t\"Architecture_id\" IN... 2 table Cost Cost 4 CREATE TABLE \"Cost\" (\\n\\t\"Architecture_id\" INT... 3 table Neurons Neurons 5 CREATE TABLE \"Neurons\" (\\n\\t\"Layer_id\" INTEGER... 4 table Weight Weight 6 CREATE TABLE \"Weight\" (\\n\\tvalue INTEGER, \\n\\t...","title":"Adding Database"},{"location":"tutorials/#forward-feeding-data-to-network","text":"","title":"Forward Feeding data to network"},{"location":"tutorials/#generating-linearly-seperable-data","text":"n,k = 300,2 X = numpy.random.uniform(-100,100,size=(n,k)) y = (X.sum(axis=1) < numpy.random.uniform(.3,.37,(len(X),))).reshape(-1,1)+0 plt.scatter(x=X[:,0],y=X[:,1],c=y) <matplotlib.collections.PathCollection at 0x7faca51e37d0>","title":"Generating Linearly seperable data"},{"location":"tutorials/#looping-over-layers","text":"for layer in network: print(repr(layer)) Fullyconnected Activation Fullyconnected Activation layer.func \u03c3 out = X for layer in network: out = layer.func.compute(out) out array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) out.shape (300, 2)","title":"Looping over layers"},{"location":"tutorials/#using-predict-method","text":"network.predict(X) network.out.shape (300, 2)","title":"Using predict method"},{"location":"tutorials/#exploring-database","text":"%load_ext sql %sql $network.db_path","title":"Exploring database"},{"location":"tutorials/#architecture_1","text":"%%sql SELECT * FROM Architecture * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. id created_at updated_at name Architecture_id 1 2024-03-04 18:11:45.895299 2024-03-04 18:11:45.895302 Sequential 140379602670032 2 2024-03-04 18:17:26.316307 2024-03-04 18:17:26.316315 Sequential 140379603024400 network['id'] 140379603024400","title":"Architecture"},{"location":"tutorials/#costs","text":"%%sql SELECT * FROM cost WHERE Architecture_id = 140379603024400 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. Architecture_id value id created_at updated_at name Cost_id 140379603024400 None 2 2024-03-04 18:17:26.591265 2024-03-04 18:17:26.591271 BinaryCrossEntropy 140379601731216","title":"Costs"},{"location":"tutorials/#layers_1","text":"%%sql SELECT * FROM layer WHERE Architecture_id=140379603024400 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. Architecture_id n_in n_out id created_at updated_at name Layer_id 140379603024400 2 50 5 2024-03-04 18:17:26.327160 2024-03-04 18:17:26.327167 Fullyconnected 140379603033872 140379603024400 None None 6 2024-03-04 18:17:26.327168 2024-03-04 18:17:26.513963 Activation 140379602907280 140379603024400 50 2 7 2024-03-04 18:17:26.327169 2024-03-04 18:17:26.547988 Fullyconnected 140379601709072 140379603024400 None None 8 2024-03-04 18:17:26.327170 2024-03-04 18:17:26.572231 Activation 140379601815760","title":"Layers"},{"location":"tutorials/#neurons","text":"%%sql SELECT * FROM neurons WHERE layer_id=140379603033872 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. Layer_id id created_at updated_at name Neurons_id 140379603033872 1 2024-03-04 18:17:26.330924 2024-03-04 18:17:26.330930 \u03a3 140379601518480","title":"Neurons"},{"location":"tutorials/#weights","text":"%%sql SELECT * FROM weight WHERE neurons_id = 140379601518480 LIMIT 10 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. value Neurons_id id created_at updated_at name Weight_id 0 140379601518480 1 2024-03-04 18:17:26.341093 2024-03-04 18:17:26.341100 None 0_0 0 140379601518480 2 2024-03-04 18:17:26.341101 2024-03-04 18:17:26.341102 None 0_1 0 140379601518480 3 2024-03-04 18:17:26.341102 2024-03-04 18:17:26.341103 None 0_2 0 140379601518480 4 2024-03-04 18:17:26.341104 2024-03-04 18:17:26.341104 None 0_3 0 140379601518480 5 2024-03-04 18:17:26.341105 2024-03-04 18:17:26.341105 None 0_4 0 140379601518480 6 2024-03-04 18:17:26.341106 2024-03-04 18:17:26.341106 None 0_5 0 140379601518480 7 2024-03-04 18:17:26.341107 2024-03-04 18:17:26.341107 None 0_6 0 140379601518480 8 2024-03-04 18:17:26.341108 2024-03-04 18:17:26.341108 None 0_7 0 140379601518480 9 2024-03-04 18:17:26.341109 2024-03-04 18:17:26.341109 None 0_8 0 140379601518480 10 2024-03-04 18:17:26.341110 2024-03-04 18:17:26.341110 None 0_9 %%sql SELECT count(*) n_neurons, AVG(value) mean_value FROM weight WHERE neurons_id = 140379601518480 * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709575905.db Done. n_neurons mean_value 150 0.0","title":"Weights"},{"location":"tutorials/#predict-method","text":"network.predict(X) array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]])","title":"Predict Method"},{"location":"tutorials/#cost-functions","text":"from neural_net.cost import BinaryCrossEntropy, CrossEntropy, MSE from neural_net.utils import make_circle_data,IrisDatasetDownloader,HouseDatasetDownloader,Pearson,Boostrap import numpy import matplotlib.pyplot as plt","title":"Cost functions"},{"location":"tutorials/#binary-crossentropy","text":"\\mathrm{\\mathit{Binary\\ Cross\\ Entropy}}(p, y) = \\begin{cases} -\\log(p) & \\text{if } y = 1, \\\\ -\\log(1-p) & \\text{otherwise.} \\end{cases} bcost = BinaryCrossEntropy()","title":"Binary Crossentropy"},{"location":"tutorials/#circles-dataset","text":"centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 1) (-100.0, 100.0) dum_classifier = numpy.random.random(len(y)) dum_classifier.shape (328,) bcost.compute(y,dum_classifier) 0.9439193222155523 round(bcost.compute(y,y)) 0","title":"Circles dataset"},{"location":"tutorials/#properties","text":"","title":"Properties"},{"location":"tutorials/#with-clipped-values-default-cliptrue","text":"ps = numpy.linspace(0,1,1000).reshape(-1,1) y1 = numpy.array([ [[bcost.compute(numpy.array([1]),p)],bcost.pr()] for p in ps ]) y0 = numpy.array([ [[bcost.compute(numpy.array([0]),p)],bcost.pr()] for p in ps ]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,0],label='estimated probabilities') ax2.set_title('y=0') ax2.plot(ps,y0[:,0],label='estimated probabilities') ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d6a6e3d90>","title":"With clipped values( default clip=True)"},{"location":"tutorials/#derivaties","text":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps,y0[:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d68199a90> fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps[10:-10,:],y1[10:-10,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps[10:-10,:],y0[10:-10:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d680e5f10>","title":"Derivaties"},{"location":"tutorials/#without-clippping","text":"ps = numpy.linspace(1e-9,1-1e-9,1000).reshape(-1,1) y1 = numpy.array([ [[bcost.compute(numpy.array([1]),p,clip=False)],bcost.pr()] for p in ps ]) y0 = numpy.array([ [[bcost.compute(numpy.array([0]),p,clip=False)],bcost.pr()] for p in ps ]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,0],label='estimated probabilities') ax2.set_title('y=0') ax2.plot(ps,y0[:,0],label='estimated probabilities') ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d67d6a490>","title":"Without clippping"},{"location":"tutorials/#derivaties_1","text":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps,y1[:,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps,y0[:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d680bd010> fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5)) ax1.set_title('y=1') ax1.plot(ps[10:-10,:],y1[10:-10,1],label=\"$BCE'_{p}$\") ax2.set_title('y=0') ax2.plot(ps[10:-10,:],y0[10:-10:,1],label=\"$BCE'_{p}$\") ax1.legend() ax2.legend() <matplotlib.legend.Legend at 0x7f4d67b1f6d0>","title":"Derivaties"},{"location":"tutorials/#cross-entropy","text":"ce = CrossEntropy()","title":"Cross Entropy"},{"location":"tutorials/#iris-dataset","text":"iris = IrisDatasetDownloader() iris.load_dataset() print(iris.description) 1. Title: Iris Plants Database Updated Sept 21 by C.Blake - Added discrepency information 2. Sources: (a) Creator: R.A. Fisher (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) (c) Date: July, 1988 3. Past Usage: - Publications: too many to mention!!! Here are a few. 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. -- Results: -- very low misclassification rates (0% for the setosa class) 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. -- Results: -- very low misclassification rates again 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. 4. Relevant Information: --- This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. --- Predicted attribute: class of iris plant. --- This is an exceedingly simple domain. --- This data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick@espeedaz.net ) The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features. 5. Number of Instances: 150 (50 in each of three classes) 6. Number of Attributes: 4 numeric, predictive attributes and the class 7. Attribute Information: 1. sepal length in cm 2. sepal width in cm 3. petal length in cm 4. petal width in cm 5. class: -- Iris Setosa -- Iris Versicolour -- Iris Virginica 8. Missing Attribute Values: None Summary Statistics: Min Max Mean SD Class Correlation sepal length: 4.3 7.9 5.84 0.83 0.7826 sepal width: 2.0 4.4 3.05 0.43 -0.4194 petal length: 1.0 6.9 3.76 1.76 0.9490 (high!) petal width: 0.1 2.5 1.20 0.76 0.9565 (high!) 9. Class Distribution: 33.3% for each of 3 classes. print(iris.data.shape,iris.target.shape) (150, 4) (150, 1) print(iris.target_names) ['setosa', 'versicolor', 'virginica'] print(iris.feature_names) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] print(iris.data[:5,:]) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2]] print(iris.target[:5,:]) [[0] [0] [0] [0] [0]] _, ax = plt.subplots() scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target) ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1]) _ = ax.legend( scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\" ) dum_classifier = numpy.random.random((len(iris.target),3)) dum_classifier/=dum_classifier.sum(axis=1,keepdims=True) print(dum_classifier.shape) dum_classifier[:5,:] (150, 3) array([[0.30771342, 0.63291054, 0.05937604], [0.19948573, 0.3398386 , 0.46067567], [0.42137261, 0.06943003, 0.50919737], [0.08200258, 0.60102031, 0.31697711], [0.27441097, 0.40437459, 0.32121444]]) from neural_net.pipeline import onehot y = onehot(iris.target) print(y.shape) y[:5,:] (150, 3) array([[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]) ce.compute(y,dum_classifier) 1.2920221019539604 round(ce.compute(y,y)) 0","title":"Iris dataset"},{"location":"tutorials/#properties_1","text":"","title":"Properties"},{"location":"tutorials/#binary-case-2-labels","text":"ps = numpy.linspace(1e-9,1-1e-9,1000).reshape(-1,1) y1,pr = [],[] for p in ps: y1.append(ce.compute(numpy.array([[1,0]]),numpy.c_[p,1-p])) pr.append(ce.pr()[0]) fig,ax1 = plt.subplots() plt.title('y=[1,0]') ax1.plot(ps,y1,label='[p0,p1=1-p0]') ax1.legend() <matplotlib.legend.Legend at 0x7f00792e5310> pr=numpy.array(pr) pr.shape (1000, 2) fig,ax1 = plt.subplots() ax1.set_title('derivatives at y=[1,0]') ax1.plot(ps[10:-10,:],pr[10:-10,0],label=r\"$CE'_{p0}$\") ax1.plot(ps[10:-10,:],pr[10:-10,1],label=r\"$CE'_{p1}$\") plt.legend() <matplotlib.legend.Legend at 0x7f0079144dd0>","title":"binary case : 2 labels"},{"location":"tutorials/#multimodal-case-3-labels","text":"ps = numpy.linspace(1e-9,1-1e-9,1000).reshape(-1,1) y1,pr = [],[] for p in ps: y1.append(ce.compute(numpy.array([[0,1,0]]),numpy.c_[(1-p)*2/3,p,(1-p)/3])) pr.append(ce.pr()[0]) fig,ax1 = plt.subplots() plt.title('y=[0,1,0]') ax1.plot(ps,y1,label='[p0,p1,p2]') ax1.legend() <matplotlib.legend.Legend at 0x7f0079016210> pr=numpy.array(pr) pr.shape (1000, 3) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,7)) ax1.set_title('derivatives at y=[0,1,0]') ax1.plot(ps[10:-10,:],pr[10:-10,0],label=r\"$CE'_{p0}$\") ax1.plot(ps[10:-10,:],pr[10:-10,1],label=r\"$CE'_{p1}$\") ax1.plot(ps[10:-10,:],pr[10:-10,2],label=r\"$CE'_{p2}$\") ax2.set_title('derivatives at y=[0,1,0]') ax2.plot(ps[10:-10,:],pr[10:-10,0],label=r\"$CE'_{p0}$\") ax2.plot(ps[10:-10,:],pr[10:-10,2],label=r\"$CE'_{p2}$\") plt.legend() <matplotlib.legend.Legend at 0x7f0078ff72d0>","title":"Multimodal case : 3+ labels"},{"location":"tutorials/#mean-squared-error","text":"mse = MSE()","title":"Mean Squared Error"},{"location":"tutorials/#boston-housing","text":"housing = HouseDatasetDownloader() housing.load_dataset() print(housing.description) The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, 'Regression diagnostics ...', Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. Variables in order: CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per $10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT % lower status of the population MEDV Median value of owner-occupied homes in $1000's print(housing.columns) ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] print(housing.data) [[6.3200e-03 1.8000e+01 2.3100e+00 ... 3.9690e+02 4.9800e+00 2.4000e+01] [2.7310e-02 0.0000e+00 7.0700e+00 ... 3.9690e+02 9.1400e+00 2.1600e+01] [2.7290e-02 0.0000e+00 7.0700e+00 ... 3.9283e+02 4.0300e+00 3.4700e+01] ... [6.0760e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 5.6400e+00 2.3900e+01] [1.0959e-01 0.0000e+00 1.1930e+01 ... 3.9345e+02 6.4800e+00 2.2000e+01] [4.7410e-02 0.0000e+00 1.1930e+01 ... 3.9690e+02 7.8800e+00 1.1900e+01]] print(housing.data.shape) (506, 14)","title":"Boston Housing"},{"location":"tutorials/#correlations","text":"fig = plt.figure(figsize=(22,14)) gs = fig.add_gridspec(2,3) ax1 = fig.add_subplot(gs[0, 0]) ax2 = fig.add_subplot(gs[0, 1]) ax3 = fig.add_subplot(gs[0, 2]) ax4 = fig.add_subplot(gs[1, :]) ax1.hist(housing.data[:,-1],label=\"Median value of owner-occupied homes in $1,000's\",bins=10) ax1.legend() ax2.hist(((lp:=numpy.log(housing.data[:,-1]))-lp.mean())/lp.std(),label='Normalized Log Median House Values',bins=15) ax2.legend() corr = Pearson(housing.data,cols=housing.columns) corr.corr() ax4.scatter(x=housing.data[:,-2],y=housing.data[:,-1],label='MEDV by % lower status of the population' ) ax4.set_xlabel(\"LSTAT\") ax4.set_ylabel(\"MEDV\") ax4.legend() corr.heatmap(ax=ax3,digits=2)","title":"Correlations"},{"location":"tutorials/#ordinary-least-squares","text":"X = numpy.c_[housing.data[:,[-2]],numpy.ones((len(housing.data),1))] y = housing.data[:,[-1]] XtX = X.T.dot(X) XTXinv = numpy.linalg.inv(XtX) \u03b2hat = XTXinv.dot(X.T.dot(y)) pred = X.dot(\u03b2hat) \u03b5 = y-pred sigma\u03b5,mean\u03b5 = \u03b5.std(),\u03b5.mean() print(sigma\u03b5,mean\u03b5) Var\u03b2 = sigma\u03b5**2 * XTXinv \u03b2hat 6.20346413142642 -1.842355868215912e-14 array([[-0.95004935], [34.55384088]])","title":"Ordinary Least Squares"},{"location":"tutorials/#t-test","text":"student = \u03b2hat/Var\u03b2.diagonal().reshape(-1,1)**.5 student array([[-24.57651813], [ 61.53688032]]) import scipy.stats p_value = scipy.stats.norm.sf(abs(student)) p_value array([[1.12616355e-133], [0.00000000e+000]])","title":"t-test"},{"location":"tutorials/#r2-score","text":"SSE = (\u03b5**2).sum() SStot = ((y-y.mean())**2).sum() R2 = 1 - SSE/SStot R2 0.5441462975864797 plt.scatter(x=housing.data[:,-2],y=housing.data[:,-1],label='MEDV by % lower status of the population' ) plt.plot(housing.data[:,-2],pred,label=\"y=$34,000 - \\$950xLSTAT;\\ R^{2}=$\"+f'{R2:.2}') plt.xlabel(\"LSTAT\") plt.ylabel(\"MEDV\") plt.legend() <matplotlib.legend.Legend at 0x7f4d64160450>","title":"R2 score"},{"location":"tutorials/#computing-mse","text":"mse = MSE() mse.compute(y,pred) 38.48296722989415 mse.compute(y,y) 0.0","title":"Computing mse"},{"location":"tutorials/#residual-analysis","text":"probs = numpy.linspace(0,1,100) \u03b5quantiles = numpy.quantile(\u03b5,probs) theoratical = numpy.random.normal(loc=mean\u03b5,scale=sigma\u03b5,size=10000) normal_quantiles = numpy.quantile(theoratical,probs) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(22,7)) ax1.scatter(normal_quantiles,\u03b5quantiles,label='Residual quantiles') ax1.plot(normal_quantiles,normal_quantiles,label='theoratical normal') ax1.set_title(\"Normal Q-Q\") ax1.set_xlabel(\"normal quantiles\") ax1.set_ylabel(\"\u03b5 quantiles\") ax2.scatter(pred,\u03b5,label=r\"Residuals\") ax2.set_title(r\"Residuals vs fitted\") ax2.set_xlabel(\"predictions\") ax2.set_ylabel(\"\u03b5\") ax2.legend() <matplotlib.legend.Legend at 0x7f4d54953290>","title":"Residual Analysis"},{"location":"tutorials/#biais-analysis","text":"coeffs = [] for (x_new,y_new) in Boostrap((X,y),n_sample=1000): \u03b2_new = numpy.linalg.inv(x_new.T.dot(x_new)).dot(x_new.T.dot(y_new)) coeffs.append(\u03b2_new) coeffs = numpy.concatenate(coeffs,axis=1).T coeffs.mean(axis=0) array([-0.95607794, 34.59180802]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(22,7)) ax1.hist(coeffs[:,0],label=r\"$\\beta_{LSTAT}$\",bins=15) ax1.set_title(r\"$E(\\beta_{LSTAT})=-\\$957$\") ax1.legend() ax2.hist(coeffs[:,1],label=r\"$\\beta_{Intercept}$\",bins=15) ax2.set_title(r\"$E(\\beta_{Intercept})=\\$34,625$\") ax2.legend() <matplotlib.legend.Legend at 0x7f4d545e6610>","title":"Biais analysis"},{"location":"tutorials/#properties_2","text":"ps = numpy.linspace(0,200,1000).reshape(-1,1) mses = numpy.array([ [mse.compute(numpy.array([100]),p),mse.pr()[0]] for p in ps ]) fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,7)) ax1.plot(ps,mses[:,0],label='mse') ax1.set_title('mse for true=100') ax1.legend() ax2.plot(ps,mses[:,1],label=r'$-2(y_{n,1}-p_{n,1})$') ax2.set_title('mse derivative at true=100') ax2.legend() <matplotlib.legend.Legend at 0x7f4d54150950>","title":"Properties"},{"location":"tutorials/#backpropagation","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3 from neural_net.cost import BinaryCrossEntropy from neural_net.utils import make_circle_data import numpy import matplotlib.pyplot as plt network = Sequential( [ Fullyconnected(2,10,zeros), Activation(\u03c3), Fullyconnected(10,1,zeros), Activation(\u03c3) ] ,BinaryCrossEntropy,store=True) network['steps'] [Fullyconnected, Activation, Fullyconnected, Activation] n,k = 1000,2 X = numpy.random.uniform(-100,100,size=(n,k)) y = (X.sum(axis=1) < numpy.random.uniform(30,90,(len(X),))).reshape(-1,1)+0 plt.scatter(x=X[:,0],y=X[:,1],c=y) <matplotlib.collections.PathCollection at 0x7f792bffff10> network.predict(X) network.out.shape (1000, 1) network['cost'] BinaryCrossEntropy network['cost'].compute(y,network.out) 0.6931471805599454 \u03940 = network['cost'].pr() \u03940.shape (1000, 1)","title":"Backpropagation"},{"location":"tutorials/#output-layer","text":"network['steps'][-1] Activation \u03941 = network['steps'][-1].func.grad(\u03940) \u03941.shape (1000, 1)","title":"Output Layer"},{"location":"tutorials/#last-linear-layer","text":"network['steps'][-2] Fullyconnected \u03942 = network['steps'][-2].func.grad(\u03941) \u03942.shape (1000, 10) %load_ext sql %sql $network.db_path","title":"Last Linear Layer"},{"location":"tutorials/#update-method","text":"network.update(\u03940) array([[ 0.34141679, 0.29918978], [ 0.34141679, 0.29918978], [-0.34141679, -0.29918978], ..., [ 0.34141679, 0.29918978], [ 0.34141679, 0.29918978], [ 0.34141679, 0.29918978]])","title":"Update method"},{"location":"tutorials/#view-weights-changes-on-db","text":"network.updateW() network.commit() %%sql SELECT * from weight * sqlite:////home/analyst/notebooks/module/neural_net/run/model1709931963.db Done. value Neurons_id id created_at updated_at name Weight_id -0.9932124687481962 140158405172944 1 2024-03-08 21:06:03.650976 2024-03-08 21:07:12.085321 None 0_0 -0.9932124687481962 140158405172944 2 2024-03-08 21:06:03.650981 2024-03-08 21:07:12.085329 None 0_1 -0.9932124687481962 140158405172944 3 2024-03-08 21:06:03.650983 2024-03-08 21:07:12.085330 None 0_2 -0.9932124687481962 140158405172944 4 2024-03-08 21:06:03.650984 2024-03-08 21:07:12.085332 None 0_3 -0.9932124687481962 140158405172944 5 2024-03-08 21:06:03.650985 2024-03-08 21:07:12.085332 None 0_4 -0.9932124687481962 140158405172944 6 2024-03-08 21:06:03.650985 2024-03-08 21:07:12.085333 None 0_5 -0.9932124687481962 140158405172944 7 2024-03-08 21:06:03.650987 2024-03-08 21:07:12.085333 None 0_6 -0.9932124687481962 140158405172944 8 2024-03-08 21:06:03.650987 2024-03-08 21:07:12.085334 None 0_7 -0.9932124687481961 140158405172944 9 2024-03-08 21:06:03.650988 2024-03-08 21:07:12.085334 None 0_8 -0.9932124687481961 140158405172944 10 2024-03-08 21:06:03.650989 2024-03-08 21:07:12.085335 None 0_9 -0.8703702558942491 140158405172944 11 2024-03-08 21:06:03.650990 2024-03-08 21:07:12.085336 None 1_0 -0.8703702558942491 140158405172944 12 2024-03-08 21:06:03.650991 2024-03-08 21:07:12.085336 None 1_1 -0.8703702558942491 140158405172944 13 2024-03-08 21:06:03.650992 2024-03-08 21:07:12.085337 None 1_2 -0.8703702558942491 140158405172944 14 2024-03-08 21:06:03.650993 2024-03-08 21:07:12.085337 None 1_3 -0.8703702558942491 140158405172944 15 2024-03-08 21:06:03.650994 2024-03-08 21:07:12.085338 None 1_4 -0.8703702558942491 140158405172944 16 2024-03-08 21:06:03.650995 2024-03-08 21:07:12.085339 None 1_5 -0.8703702558942491 140158405172944 17 2024-03-08 21:06:03.650996 2024-03-08 21:07:12.085339 None 1_6 -0.8703702558942491 140158405172944 18 2024-03-08 21:06:03.650997 2024-03-08 21:07:12.085340 None 1_7 -0.8703702558942492 140158405172944 19 2024-03-08 21:06:03.650998 2024-03-08 21:07:12.085340 None 1_8 -0.8703702558942492 140158405172944 20 2024-03-08 21:06:03.650999 2024-03-08 21:07:12.085341 None 1_9 0.01890625000000001 140158405172944 21 2024-03-08 21:06:03.651000 2024-03-08 21:07:12.085341 None 2_0 0.01890625000000001 140158405172944 22 2024-03-08 21:06:03.651001 2024-03-08 21:07:12.085342 None 2_1 0.01890625000000001 140158405172944 23 2024-03-08 21:06:03.651002 2024-03-08 21:07:12.085343 None 2_2 0.01890625000000001 140158405172944 24 2024-03-08 21:06:03.651003 2024-03-08 21:07:12.085343 None 2_3 0.01890625000000001 140158405172944 25 2024-03-08 21:06:03.651004 2024-03-08 21:07:12.085344 None 2_4 0.01890625000000001 140158405172944 26 2024-03-08 21:06:03.651005 2024-03-08 21:07:12.085344 None 2_5 0.01890625000000001 140158405172944 27 2024-03-08 21:06:03.651006 2024-03-08 21:07:12.085345 None 2_6 0.01890625000000001 140158405172944 28 2024-03-08 21:06:03.651006 2024-03-08 21:07:12.085346 None 2_7 0.01890625 140158405172944 29 2024-03-08 21:06:03.651007 2024-03-08 21:07:12.085346 None 2_8 0.01890625 140158405172944 30 2024-03-08 21:06:03.651008 2024-03-08 21:07:12.085347 None 2_9 0.275 140158405279056 31 2024-03-08 21:06:03.651009 2024-03-08 21:07:12.085347 None 0_0 0.275 140158405279056 32 2024-03-08 21:06:03.651010 2024-03-08 21:07:12.085348 None 1_0 0.275 140158405279056 33 2024-03-08 21:06:03.651011 2024-03-08 21:07:12.085348 None 2_0 0.275 140158405279056 34 2024-03-08 21:06:03.651012 2024-03-08 21:07:12.085349 None 3_0 0.275 140158405279056 35 2024-03-08 21:06:03.651013 2024-03-08 21:07:12.085349 None 4_0 0.275 140158405279056 36 2024-03-08 21:06:03.651014 2024-03-08 21:07:12.085350 None 5_0 0.275 140158405279056 37 2024-03-08 21:06:03.651015 2024-03-08 21:07:12.085351 None 6_0 0.275 140158405279056 38 2024-03-08 21:06:03.651016 2024-03-08 21:07:12.085351 None 7_0 0.275 140158405279056 39 2024-03-08 21:06:03.651017 2024-03-08 21:07:12.085352 None 8_0 0.275 140158405279056 40 2024-03-08 21:06:03.651018 2024-03-08 21:07:12.085354 None 9_0 0.55 140158405279056 41 2024-03-08 21:06:03.651019 2024-03-08 21:07:12.085354 None 10_0","title":"View weights changes on db"},{"location":"tutorials/#logistic-regression","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3,Softmax from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.utils import make_circle_data import numpy import matplotlib.pyplot as plt centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 1) (-100.0, 100.0)","title":"Logistic Regression"},{"location":"tutorials/#pure-numpy-definition","text":"H_\u03b8 = lambda X,\u03b8 : 1/(1+numpy.exp(-X.dot(\u03b8))) \u03b8 = numpy.zeros((3,1)) X_const = numpy.c_[X,numpy.ones((len(X),1))] H_\u03b8(X_const,\u03b8).shape (328, 1)","title":"Pure numpy definition"},{"location":"tutorials/#using-layers","text":"LogReg = Sequential( [ Fullyconnected(2,1,zeros), Activation(\u03c3) ], BinaryCrossEntropy ) LogReg.predict(X).shape (328, 1) LogReg['steps'][-2].func.W array([[0.], [0.], [0.]])","title":"Using layers"},{"location":"tutorials/#computing-gradients","text":"","title":"Computing Gradients"},{"location":"tutorials/#analytic-gradient","text":"J = lambda \u03b8,X : 1/len(X)*X.T.dot(H_\u03b8(X,\u03b8)-y) J(\u03b8,X_const) array([[ -8.2471052 ], [-15.8967726 ], [ -0.07317073]])","title":"Analytic gradient"},{"location":"tutorials/#chain-rule-logistic-regression","text":"LogReg[\"cost\"].compute(y,LogReg.out) p0 = LogReg[\"cost\"].pr() p1 = LogReg[\"steps\"][-1].func.grad(p0) p2 = LogReg[\"steps\"][-2].func.grad(p1) -LogReg[\"steps\"][-2].func.W array([[ -8.2471052 ], [-15.8967726 ], [ -0.07317073]])","title":"Chain rule logistic Regression"},{"location":"tutorials/#softmax-regression-j-target-class-in-1k-and-m1n-instances","text":"{\\displaystyle \\sigma (\\mathbf {z} )_{j}={\\frac {\\mathrm {e} ^{z_{j}}}{\\sum _{i=1}^{K}\\mathrm {e} ^{z_{i}}}}}","title":"Softmax Regression( j target class in {1,..,k} and m=1,..,n instances)"},{"location":"tutorials/#analytical-gradient","text":"from neural_net.pipeline import onehot y_one_hot = onehot(y) y_one_hot[:5,:] array([[1, 0], [1, 0], [1, 0], [1, 0], [1, 0]]) W = numpy.zeros((X.shape[1]+1,2)) W array([[0., 0.], [0., 0.], [0., 0.]]) W = numpy.zeros((X.shape[1],2)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) Sm(W,X)[:5,:] array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) J = 1/len(X)*X.T.dot(Sm(W,X)-y_one_hot) J array([[ 8.2471052, -8.2471052], [ 15.8967726, -15.8967726]])","title":"Analytical gradient"},{"location":"tutorials/#chain-rule","text":"softmax = Sequential( [ Fullyconnected(2,2,zeros), Activation(Softmax) ], CrossEntropy ) softmax.predict(X)[:5,:] array([[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]) softmax[\"cost\"].compute(y_one_hot,softmax.out) 0.6931471805599453 softmax[\"cost\"].compute(y_one_hot,softmax.out) _ = softmax.update(softmax[\"cost\"].pr()) -softmax[\"steps\"][-2].func.W array([[ -8.2471052 , 8.2471052 ], [-15.8967726 , 15.8967726 ], [ -0.07317073, 0.07317073]])","title":"Chain rule"},{"location":"tutorials/#training","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3,Softmax from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.utils import make_circle_data,IrisDatasetDownloader from neural_net.metrics import accuracy from neural_net.pipeline import onehot import numpy import matplotlib.pyplot as plt centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 1) (-100.0, 100.0)","title":"Training"},{"location":"tutorials/#logistic-regression_1","text":"n_epoch = 1000 \u03b1 = 0.1","title":"Logistic Regression"},{"location":"tutorials/#using-analytical-gradient","text":"H_\u03b8 = lambda X,\u03b8 : 1/(1+numpy.exp(-X.dot(\u03b8))) \u03b8 = numpy.zeros((3,1)) X_const = numpy.c_[X,numpy.ones((len(X),1))] J = lambda \u03b8,X : 1/len(X)*X.T.dot(H_\u03b8(X,\u03b8)-y) for _ in range(n_epoch): \u03b8 -= \u03b1*J(\u03b8,X_const) \u03b8 array([[1.19524694], [1.47756754], [0.05197856]]) pred = (H_\u03b8(X_const,\u03b8) > .5 )+0 plt.scatter(x=X[:,0],y=X[:,1],c=pred) <matplotlib.collections.PathCollection at 0x7f2c4820df90>","title":"Using analytical gradient"},{"location":"tutorials/#using-chain-rule","text":"LogReg = Sequential( [ Fullyconnected(2,1,zeros), Activation(\u03c3) ], BinaryCrossEntropy ) for _ in range(n_epoch): LogReg.predict(X) LogReg[\"cost\"].compute(y,LogReg.out) LogReg[\"cost\"].compute(y,LogReg.out) _ = LogReg.update(\u03b1*LogReg[\"cost\"].pr()) LogReg['steps'][-2].func.W array([[1.19524694], [1.47756754], [0.05197856]]) p = (LogReg.out>.5)+0 plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f2c481c2b90>","title":"Using chain rule"},{"location":"tutorials/#using-train-method","text":"LogReg = Sequential( [ Fullyconnected(2,1,zeros), Activation(\u03c3) ], BinaryCrossEntropy ) LogReg.train(X,y,epochs=n_epoch,\u03b1=\u03b1,metrics=accuracy) BinaryCrossEntropy 0.0002 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 298.57it/s] LogReg['steps'][-2].func.W array([[1.19524694], [1.47756754], [0.05197856]])","title":"Using train method"},{"location":"tutorials/#softmax","text":"","title":"Softmax"},{"location":"tutorials/#2-labels","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros from neural_net.activation import \u03c3,Softmax from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.utils import make_circle_data,IrisDatasetDownloader from neural_net.metrics import accuracy from neural_net.pipeline import onehot import numpy import matplotlib.pyplot as plt centers = [(-50, 0), (20, 30)] radii = [30, 35] X, y = make_circle_data(centers, radii) y = onehot(y) print(X.shape, y.shape) ax = plt.subplot() ax.scatter(X[:,0],X[:,1],c=y.argmax(axis=1)) ax.set_xlim(-100,100) ax.set_ylim(-100,100) (328, 2) (328, 2) (-100.0, 100.0) n_epoch = 1000 \u03b1 = 0.1","title":"2 labels"},{"location":"tutorials/#analytically","text":"n,k=X.shape j=y.shape[1] W = numpy.zeros((k+1,j)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) J_W = lambda W,X : 1/n*X.T.dot(Sm(W,X)-y) for _ in range(n_epoch): W -= \u03b1*J_W(W,numpy.c_[X,numpy.ones((n,1))] ) W array([[-1.07789341, 1.07789341], [-1.56804297, 1.56804297], [-0.02426586, 0.02426586]]) p = Sm(W,numpy.c_[X,numpy.ones((n,1))]).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f0083e3af50>","title":"Analytically"},{"location":"tutorials/#using-chain-rule_1","text":"softmax = Sequential( [ Fullyconnected(k,j,zeros), Activation(Softmax) ], CrossEntropy ) softmax.train(X,y,epochs=n_epoch,\u03b1=\u03b1,metrics=accuracy) CrossEntropy 0.0001 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 282.21it/s] softmax['steps'][-2].func.W array([[-1.07789341, 1.07789341], [-1.56804297, 1.56804297], [-0.02426586, 0.02426586]]) p = softmax.predict(X).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f0083de2e90>","title":"using chain rule"},{"location":"tutorials/#3-labels","text":"iris = IrisDatasetDownloader() iris.load_dataset() _, ax = plt.subplots() scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target) ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1]) _ = ax.legend( scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\" ) X,y = iris.data,onehot(iris.target) n,k = X.shape j = y.shape[1] X.shape,y.shape ((150, 4), (150, 3))","title":"3 labels"},{"location":"tutorials/#using-softmax-analytical-solution","text":"n_epoch=1000 W = numpy.zeros((k+1,j)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) J_W = lambda W,X : 1/n*X.T.dot(Sm(W,X)-y) for _ in range(n_epoch): W -= \u03b1*J_W(W,numpy.c_[X,numpy.ones((n,1))]) W array([[ 0.88907029, 0.7230017 , -1.612072 ], [ 2.05504822, -0.20042131, -1.85462692], [-2.81867225, -0.13349788, 2.95217013], [-1.32033636, -1.14715458, 2.46749093], [ 0.42390714, 0.65830856, -1.0822157 ]]) p = Sm(W,numpy.c_[X,numpy.ones((n,1))]).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f008403af50> accuracy().compute(y,Sm(W,numpy.c_[X,numpy.ones((n,1))])) 0.9867","title":"Using Softmax analytical solution"},{"location":"tutorials/#chain-rule_1","text":"softmax = Sequential( [ Fullyconnected(k,j,zeros), Activation(Softmax) ], CrossEntropy ) softmax.train(X,y,epochs=n_epoch,\u03b1=\u03b1,metrics=accuracy) CrossEntropy 0.126 accuracy 0.9867: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 318.95it/s] p = softmax.predict(X).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f0080252e90>","title":"Chain rule"},{"location":"tutorials/#non-linear-problems","text":"import numpy n,k = 1500,2 X = numpy.random.uniform(-100,100,size=(n,k)) y =( (X[:, 0]**2 + X[:, 1]**2) < 3000).reshape(-1,1)+0 y_one_hot = onehot(y) plt.scatter(x=X[:,0],y=X[:,1],c=y) <matplotlib.collections.PathCollection at 0x7f0d3d72b510>","title":"Non Linear Problems"},{"location":"tutorials/#beyond-linear-architecture","text":"n_epoch = 1000 \u03b1 = 0.2","title":"Beyond linear architecture"},{"location":"tutorials/#logistic-regression_2","text":"H_\u03b8 = lambda X,\u03b8 : 1/(1+numpy.exp(-X.dot(\u03b8))) \u03b8 = numpy.zeros((3,1)) X_const = numpy.c_[X,numpy.ones((len(X),1))] J = lambda \u03b8,X : 1/len(X)*X.T.dot(H_\u03b8(X,\u03b8)-y) for _ in range(n_epoch): \u03b8 -= \u03b1*J(\u03b8,X_const) pred = (H_\u03b8(X_const,\u03b8) > .5 )+0 (pred==y).sum()/len(y) 0.49066666666666664 plt.scatter(x=X[:,0],y=X[:,1],c=pred) <matplotlib.collections.PathCollection at 0x7f00797c7e10>","title":"Logistic regression"},{"location":"tutorials/#softmax_1","text":"y_one_hot = onehot(y) n,j= y_one_hot.shape (n,j) (1500, 2) W = numpy.zeros((k+1,j)) Sm = lambda W,X : numpy.exp(X.dot(W))/numpy.exp(X.dot(W)).sum(axis=1).reshape(-1,1) J_W = lambda W,X : 1/n*X.T.dot(Sm(W,X)-y_one_hot) for _ in range(n_epoch): W -= \u03b1*J_W(W,X_const) p = Sm(W,X_const).argmax(axis=1) plt.scatter(x=X[:,0],y=X[:,1],c=p) <matplotlib.collections.PathCollection at 0x7f007949a8d0>","title":"Softmax"},{"location":"tutorials/#neural-network","text":"from neural_net.architecture import Sequential from neural_net.layers import Fullyconnected,Activation from neural_net.init_funcs import zeros,XavierHe from neural_net.activation import \u03c3,Softmax,LeakyReLU,Tanh,ELU,ReLU from neural_net.cost import BinaryCrossEntropy,CrossEntropy from neural_net.metrics import accuracy from neural_net.pipeline import onehot,scaler,shuffle,Batch from neural_net.utils import IrisDatasetDownloader import numpy import matplotlib.pyplot as plt","title":"Neural network"},{"location":"tutorials/#xavier-and-he-initialization-methods","text":"We don\u2019t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs NN = Sequential( [ Fullyconnected(2,50,XavierHe(\"Uniform\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(50,1,XavierHe(\"Uniform\",\"Sigmoid\").init_func), Activation(\u03c3) ], BinaryCrossEntropy ) NN.train(scaler(X),y,\u03b1=\u03b1,epochs=n_epoch,metrics=accuracy) BinaryCrossEntropy 0.0721 accuracy 0.994: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:17<00:00, 56.29it/s] pred = (NN.predict(scaler(X))>.5)+0 pred array([[0], [1], [0], ..., [0], [0], [0]]) plt.scatter(x=X[:,0],y=X[:,1],c=pred) <matplotlib.collections.PathCollection at 0x7f0d3b510590> NN.train(scaler(X),y,\u03b1=\u03b1,epochs=n_epoch,metrics=accuracy) BinaryCrossEntropy 0.0463 accuracy 0.996: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:19<00:00, 51.83it/s]","title":"Xavier and He Initialization methods"},{"location":"tutorials/#iris-problem","text":"iris = IrisDatasetDownloader() iris.load_dataset() y = onehot(iris.target) X = iris.data X,y = shuffle(X,y) n,k = X.shape j = y.shape[1] NN = Sequential( [ Fullyconnected(k,100,XavierHe(\"Uniform\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(100,50,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(ELU), Fullyconnected(50,j,zeros), Activation(Softmax) ], CrossEntropy ) batch = Batch(60,len(X),lambda : scaler(X), lambda : y) NN.train(batch=batch,\u03b1=0.2,epochs=1000,metrics=accuracy) CrossEntropy 0.001 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:32<00:00, 30.43it/s] pred = NN.predict(scaler(X)).argmax(axis=1,keepdims=True) true_y = y.argmax(axis=1,keepdims=True) (pred == true_y).sum()/len(pred) 1.0","title":"Iris Problem"},{"location":"tutorials/#storing-weights","text":"from neural_net.model import Define Define._Define__store = True NN.updateW() NN.commit() NN.db_path 'sqlite:////home/analyst/notebooks/module/neural_net/run/model1709981118.db' from sqlalchemy import text import pandas cursor = NN.engines.get(NN.db_path).connect() res = cursor.execute(text(''' SELECT * FROM Weight ''')) pandas.DataFrame(res.fetchall()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value Neurons_id id created_at updated_at name Weight_id 0 -0.003729 139694848570384 1 2024-03-09 11:08:51.902537 2024-03-09 11:08:51.902543 None 0_0 1 -0.384036 139694848570384 2 2024-03-09 11:08:51.902544 2024-03-09 11:08:51.902545 None 0_1 2 0.033767 139694848570384 3 2024-03-09 11:08:51.902545 2024-03-09 11:08:51.902546 None 0_2 3 0.265035 139694848570384 4 2024-03-09 11:08:51.902546 2024-03-09 11:08:51.902547 None 0_3 4 0.261001 139694848570384 5 2024-03-09 11:08:51.902547 2024-03-09 11:08:51.902548 None 0_4 ... ... ... ... ... ... ... ... 196 -0.207245 139694847460432 197 2024-03-09 11:08:51.902896 2024-03-09 11:08:51.902896 None 46_0 197 -0.108507 139694847460432 198 2024-03-09 11:08:51.902896 2024-03-09 11:08:51.902897 None 47_0 198 -0.229139 139694847460432 199 2024-03-09 11:08:51.902897 2024-03-09 11:08:51.902898 None 48_0 199 0.260771 139694847460432 200 2024-03-09 11:08:51.902899 2024-03-09 11:08:51.902899 None 49_0 200 0.265368 139694847460432 201 2024-03-09 11:08:51.902900 2024-03-09 11:08:51.902900 None 50_0 201 rows \u00d7 7 columns","title":"Storing weights"},{"location":"tutorials/#optical-character-recognitionocr","text":"","title":"Optical character recognition(OCR)"},{"location":"tutorials/#hand-written-dataset","text":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report,ConfusionMatrixDisplay digits = datasets.load_digits() _, axes = plt.subplots(nrows=1, ncols=10, figsize=(13, 3)) for ax, image, label in zip(axes, digits.images, digits.target): ax.set_axis_off() ax.imshow(image) ax.set_title(\"Training: %i\" % label) n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) ## Split data into 50% train and 50% test subsets X_train, X_test, y_train, y_test = train_test_split( data, onehot(digits.target.reshape(-1,1)), test_size=0.5, shuffle=False ) X_train.shape,y_train.shape ((898, 64), (898, 10))","title":"Hand written dataset"},{"location":"tutorials/#softmax_2","text":"softmax = Sequential( [ Fullyconnected(64,10,zeros), Activation(Softmax) ], CrossEntropy ) softmax.train(X_train,y_train,epochs=1000,\u03b1=0.001,metrics=accuracy) CrossEntropy 0.1366 accuracy 0.9811: 100%|==========| 1000/1000 [00:25<00:00, 39.08it/s] predicted = softmax.predict(X_test).argmax(axis=1) _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3)) for ax, image, prediction in zip(axes, X_test, predicted): ax.set_axis_off() image = image.reshape(8, 8) ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\") ax.set_title(f\"Prediction: {prediction}\") print( f\"Classification report for classifier :\\n\" f\"{classification_report(y_test.argmax(axis=1), predicted)}\\n\" ) Classification report for classifier : precision recall f1-score support 0 1.00 0.97 0.98 88 1 0.93 0.84 0.88 91 2 1.00 0.97 0.98 86 3 0.94 0.85 0.89 91 4 0.97 0.91 0.94 92 5 0.89 0.93 0.91 91 6 0.93 0.99 0.96 91 7 0.96 0.99 0.97 89 8 0.89 0.90 0.89 88 9 0.83 0.96 0.89 92 accuracy 0.93 899 macro avg 0.93 0.93 0.93 899 weighted avg 0.93 0.93 0.93 899 disp = ConfusionMatrixDisplay.from_predictions(y_test.argmax(axis=1), predicted) disp.figure_.suptitle(\"Confusion Matrix\") print(f\"Confusion matrix:\\n{disp.confusion_matrix}\") plt.show() Confusion matrix: [[85 0 0 0 1 1 1 0 0 0] [ 0 76 0 1 2 1 1 0 0 10] [ 0 0 83 3 0 0 0 0 0 0] [ 0 2 0 77 0 3 0 4 5 0] [ 0 0 0 0 84 0 4 0 3 1] [ 0 0 0 0 0 85 1 0 0 5] [ 0 1 0 0 0 0 90 0 0 0] [ 0 0 0 0 0 0 0 88 1 0] [ 0 3 0 0 0 4 0 0 79 2] [ 0 0 0 1 0 2 0 0 1 88]]","title":"Softmax"},{"location":"tutorials/#deep-learning","text":"NN = Sequential( [ Fullyconnected(64,1000,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(ELU), Fullyconnected(1000,100,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(ELU), Fullyconnected(100,10,XavierHe(\"Normal\",\"Sigmoid\").init_func), Activation(Softmax) ], CrossEntropy ) X_train,y_train = shuffle(X_train,y_train) batch = Batch(10,len(X_train),lambda : X_train/16, lambda : y_train) NN.train(batch=batch,\u03b1=0.014,epochs=100,metrics=accuracy) CrossEntropy 0.0117 accuracy 1.0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:42<00:00, 2.35it/s] predicted = NN.predict(X_test/16).argmax(axis=1) _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3)) for ax, image, prediction in zip(axes, X_test, predicted): ax.set_axis_off() image = image.reshape(8, 8) ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\") ax.set_title(f\"Prediction: {prediction}\") print( f\"Classification report for classifier :\\n\" f\"{classification_report(y_test.argmax(axis=1), predicted)}\\n\" ) Classification report for classifier : precision recall f1-score support 0 1.00 0.98 0.99 88 1 0.95 0.91 0.93 91 2 0.99 1.00 0.99 86 3 0.97 0.86 0.91 91 4 0.98 0.92 0.95 92 5 0.91 0.95 0.92 91 6 0.95 0.99 0.97 91 7 0.96 0.96 0.96 89 8 0.92 0.92 0.92 88 9 0.85 0.97 0.90 92 accuracy 0.94 899 macro avg 0.95 0.94 0.94 899 weighted avg 0.95 0.94 0.94 899","title":"Deep Learning"},{"location":"tutorials/#mnist","text":"from keras.datasets import mnist 2024-03-09 13:59:33.278357: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2024-03-09 13:59:33.373835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. (X_train, Y_train), (X_test, Y_test) = mnist.load_data() num = 10 images = X_train[:num] labels = Y_train[:num] num_row = 2 num_col = 5 ## plot images fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row)) for i in range(num): ax = axes[i//num_col, i%num_col] ax.imshow(images[i], cmap='gray') ax.set_title('Label: {}'.format(labels[i])) plt.tight_layout() plt.show() X_train = X_train.reshape((X_train.shape[0], 28*28)).astype('float32') X_test = X_test.reshape((X_test.shape[0], 28*28)).astype('float32') X_train /= 255 X_test /= 255 X_train.shape (60000, 784) Y_train = onehot(Y_train.reshape(-1,1)) Y_train.shape, Y_test.shape ((60000, 10), (10000,)) n_inputs = 28 * 28 n_hidden1 = 300 n_hidden2 = 100 n_outputs = 10 NN = Sequential( [ Fullyconnected(n_inputs,n_hidden1,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(n_hidden1,n_hidden2,XavierHe(\"Normal\",\"ReLU\").init_func), Activation(LeakyReLU), Fullyconnected(n_hidden2,n_outputs,XavierHe(\"Normal\",\"Sigmoid\").init_func), Activation(Softmax) ], CrossEntropy ) batch = Batch(500,len(X_train),lambda : X_train, lambda : Y_train) NN.train(batch=batch,\u03b1=0.014,epochs=100,metrics=accuracy) CrossEntropy 0.129 accuracy 0.976: 100%|==========| 100/100 [10:28<00:00, 6.28s/it] pred = NN.predict(X_test).argmax(axis=1) pred array([7, 2, 1, ..., 4, 5, 6]) print( f\"Classification report for classifier :\\n\" f\"{classification_report(Y_test, pred)}\\n\" ) Classification report for classifier : precision recall f1-score support 0 0.98 0.99 0.98 980 1 0.98 0.99 0.98 1135 2 0.97 0.97 0.97 1032 3 0.96 0.97 0.96 1010 4 0.97 0.97 0.97 982 5 0.95 0.96 0.96 892 6 0.97 0.97 0.97 958 7 0.97 0.96 0.96 1028 8 0.96 0.96 0.96 974 9 0.97 0.95 0.96 1009 accuracy 0.97 10000 macro avg 0.97 0.97 0.97 10000 weighted avg 0.97 0.97 0.97 10000","title":"Mnist"}]}